#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/Qwen2_5_VL/modular_Qwen2_5_VL.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_Qwen2_5_VL.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.
#
# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# and OPT implementations in this library. It has been modified from its
# original forms to accommodate minor architectural differences compared
# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# New Imports
import math
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple, Union

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.ops import MLP, generalized_box_iou_loss

# imports custom generators
from ...generation import LogitsProcessorList, StoppingCriteriaList, TopKLogitsWarper, TopPLogitsWarper, TemperatureLogitsWarper, RepetitionPenaltyLogitsProcessor

# Qwen general imports
from ...activations import ACT2FN
from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache
from ...generation import GenerationMixin
from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask
from ...modeling_attn_mask_utils import AttentionMaskConverter
from ...modeling_flash_attention_utils import FlashAttentionKwargs
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput
from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from ...processing_utils import Unpack
from ...modeling_utils import PreTrainedModel
from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging


from ...generation.configuration_utils import (
    NEED_SETUP_CACHE_CLASSES_MAPPING,
    QUANT_BACKEND_CLASSES_MAPPING,
    GenerationConfig,
    GenerationMode,
)
from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union
import warnings
from ...generation.utils import GenerateOutput, GenerateNonBeamOutput
from ...integrations.deepspeed import is_deepspeed_zero3_enabled
from ...integrations.fsdp import is_fsdp_managed_module
import inspect

# Qwen Imports modified for lel2
from .configuration_lel2 import Qwen2_5_VLConfig, Qwen2_5_VLTextConfig, Qwen2_5_VLVisionConfig

logger = logging.get_logger(__name__)


class Qwen2_5_VLMLP(nn.Module):
    def __init__(self, config, bias: bool = False):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=bias)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=bias)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=bias)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, hidden_state):
        return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))


class Qwen2_5_VisionPatchEmbed(nn.Module):
    def __init__(
        self,
        patch_size: int = 14,
        temporal_patch_size: int = 2,
        in_channels: int = 3,
        embed_dim: int = 1152,
    ) -> None:
        super().__init__()
        self.patch_size = patch_size
        self.temporal_patch_size = temporal_patch_size
        self.in_channels = in_channels
        self.embed_dim = embed_dim

        kernel_size = [temporal_patch_size, patch_size, patch_size]
        self.proj = nn.Conv3d(in_channels, embed_dim, kernel_size=kernel_size, stride=kernel_size, bias=False)

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        target_dtype = self.proj.weight.dtype
        hidden_states = hidden_states.view(
            -1, self.in_channels, self.temporal_patch_size, self.patch_size, self.patch_size
        )
        hidden_states = self.proj(hidden_states.to(dtype=target_dtype)).view(-1, self.embed_dim)
        return hidden_states


class Qwen2_5_VisionRotaryEmbedding(nn.Module):
    inv_freq: torch.Tensor  # fix linting for `register_buffer`

    def __init__(self, dim: int, theta: float = 10000.0) -> None:
        super().__init__()
        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))
        self.register_buffer("inv_freq", inv_freq, persistent=False)

    def forward(self, seqlen: int) -> torch.Tensor:
        seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)
        freqs = torch.outer(seq, self.inv_freq)
        return freqs


class Qwen2RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """
        Qwen2RMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


class Qwen2_5_VLPatchMerger(nn.Module):
    def __init__(self, dim: int, context_dim: int, spatial_merge_size: int = 2) -> None:
        super().__init__()
        self.hidden_size = context_dim * (spatial_merge_size**2)
        self.ln_q = Qwen2RMSNorm(context_dim, eps=1e-6)
        self.mlp = nn.Sequential(
            nn.Linear(self.hidden_size, self.hidden_size),
            nn.GELU(),
            nn.Linear(self.hidden_size, dim),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.mlp(self.ln_q(x).view(-1, self.hidden_size))
        return x


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb_vision(
    q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor
) -> tuple[torch.Tensor, torch.Tensor]:
    orig_q_dtype = q.dtype
    orig_k_dtype = k.dtype
    q, k = q.float(), k.float()
    cos, sin = cos.unsqueeze(-2).float(), sin.unsqueeze(-2).float()
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    q_embed = q_embed.to(orig_q_dtype)
    k_embed = k_embed.to(orig_k_dtype)
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs,
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if attention_mask is not None:
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights


class Qwen2_5_VLVisionAttention(nn.Module):
    def __init__(self, config: Qwen2_5_VLVisionConfig) -> None:
        super().__init__()
        self.dim = config.hidden_size
        self.num_heads = config.num_heads
        self.head_dim = self.dim // self.num_heads
        self.num_key_value_groups = 1  # needed for eager attention
        self.qkv = nn.Linear(self.dim, self.dim * 3, bias=True)
        self.proj = nn.Linear(self.dim, self.dim)
        self.scaling = self.head_dim**-0.5
        self.config = config
        self.attention_dropout = 0.0
        self.is_causal = False

    def forward(
        self,
        hidden_states: torch.Tensor,
        cu_seqlens: torch.Tensor,
        rotary_pos_emb: Optional[torch.Tensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,
        **kwargs,
    ) -> torch.Tensor:
        seq_length = hidden_states.shape[0]
        query_states, key_states, value_states = (
            self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)
        )
        if position_embeddings is None:
            logger.warning_once(
                "The attention layers in this model are transitioning from computing the RoPE embeddings internally "
                "through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed "
                "`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be "
                "removed and `position_embeddings` will be mandatory."
            )
            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)
            cos = emb.cos()
            sin = emb.sin()
        else:
            cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)

        query_states = query_states.transpose(0, 1).unsqueeze(0)
        key_states = key_states.transpose(0, 1).unsqueeze(0)
        value_states = value_states.transpose(0, 1).unsqueeze(0)

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        if self.config._attn_implementation == "flash_attention_2":
            # Flash Attention 2: Use cu_seqlens for variable length attention
            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max()
            attn_output, _ = attention_interface(
                self,
                query_states,
                key_states,
                value_states,
                attention_mask=None,
                scaling=self.scaling,
                dropout=0.0 if not self.training else self.attention_dropout,
                cu_seq_lens_q=cu_seqlens,
                cu_seq_lens_k=cu_seqlens,
                max_length_q=max_seqlen,
                max_length_k=max_seqlen,
                is_causal=False,
                **kwargs,
            )
        else:
            # Other implementations: Process each chunk separately
            lengths = cu_seqlens[1:] - cu_seqlens[:-1]
            splits = [
                torch.split(tensor, lengths.tolist(), dim=2) for tensor in (query_states, key_states, value_states)
            ]

            attn_outputs = [
                attention_interface(
                    self,
                    q,
                    k,
                    v,
                    attention_mask=None,
                    scaling=self.scaling,
                    dropout=0.0 if not self.training else self.attention_dropout,
                    is_causal=False,
                    **kwargs,
                )[0]
                for q, k, v in zip(*splits)
            ]
            attn_output = torch.cat(attn_outputs, dim=1)

        attn_output = attn_output.reshape(seq_length, -1).contiguous()
        attn_output = self.proj(attn_output)
        return attn_output


class Qwen2_5_VLVisionBlock(GradientCheckpointingLayer):
    def __init__(self, config, attn_implementation: str = "sdpa") -> None:
        super().__init__()
        self.norm1 = Qwen2RMSNorm(config.hidden_size, eps=1e-6)
        self.norm2 = Qwen2RMSNorm(config.hidden_size, eps=1e-6)
        self.attn = Qwen2_5_VLVisionAttention(config=config)
        self.mlp = Qwen2_5_VLMLP(config, bias=True)

    def forward(
        self,
        hidden_states: torch.Tensor,
        cu_seqlens: torch.Tensor,
        rotary_pos_emb: Optional[torch.Tensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,
        **kwargs,
    ) -> torch.Tensor:
        hidden_states = hidden_states + self.attn(
            self.norm1(hidden_states),
            cu_seqlens=cu_seqlens,
            rotary_pos_emb=rotary_pos_emb,
            position_embeddings=position_embeddings,
            **kwargs,
        )
        hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))
        return hidden_states


@auto_docstring
class Qwen2_5_VLPreTrainedModel(PreTrainedModel):
    config: Qwen2_5_VLConfig
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["Qwen2_5_VLDecoderLayer", "Qwen2_5_VLVisionBlock"]
    _skip_keys_device_placement = "past_key_values"
    _supports_flash_attn = True
    _supports_sdpa = True

    _can_compile_fullgraph = True
    _supports_attention_backend = True


class Qwen2_5_VisionTransformerPretrainedModel(Qwen2_5_VLPreTrainedModel):
    config: Qwen2_5_VLVisionConfig
    _no_split_modules = ["Qwen2_5_VLVisionBlock"]

    def __init__(self, config, *inputs, **kwargs) -> None:
        super().__init__(config, *inputs, **kwargs)
        self.spatial_merge_size = config.spatial_merge_size
        self.patch_size = config.patch_size
        self.fullatt_block_indexes = config.fullatt_block_indexes
        self.window_size = config.window_size
        self.spatial_merge_unit = self.spatial_merge_size * self.spatial_merge_size

        self.patch_embed = Qwen2_5_VisionPatchEmbed(
            patch_size=config.patch_size,
            temporal_patch_size=config.temporal_patch_size,
            in_channels=config.in_channels,
            embed_dim=config.hidden_size,
        )

        head_dim = config.hidden_size // config.num_heads
        self.rotary_pos_emb = Qwen2_5_VisionRotaryEmbedding(head_dim // 2)

        self.blocks = nn.ModuleList([Qwen2_5_VLVisionBlock(config) for _ in range(config.depth)])
        self.merger = Qwen2_5_VLPatchMerger(
            dim=config.out_hidden_size,
            context_dim=config.hidden_size,
            spatial_merge_size=config.spatial_merge_size,
        )
        self.gradient_checkpointing = False

    def rot_pos_emb(self, grid_thw):
        pos_ids = []
        for t, h, w in grid_thw:
            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
            hpos_ids = hpos_ids.reshape(
                h // self.spatial_merge_size,
                self.spatial_merge_size,
                w // self.spatial_merge_size,
                self.spatial_merge_size,
            )
            hpos_ids = hpos_ids.permute(0, 2, 1, 3)
            hpos_ids = hpos_ids.flatten()

            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
            wpos_ids = wpos_ids.reshape(
                h // self.spatial_merge_size,
                self.spatial_merge_size,
                w // self.spatial_merge_size,
                self.spatial_merge_size,
            )
            wpos_ids = wpos_ids.permute(0, 2, 1, 3)
            wpos_ids = wpos_ids.flatten()
            pos_ids.append(torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
        pos_ids = torch.cat(pos_ids, dim=0)
        max_grid_size = grid_thw[:, 1:].max()
        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
        return rotary_pos_emb

    def get_window_index(self, grid_thw):
        window_index: list = []
        cu_window_seqlens: list = [0]
        window_index_id = 0
        vit_merger_window_size = self.window_size // self.spatial_merge_size // self.patch_size

        for grid_t, grid_h, grid_w in grid_thw:
            llm_grid_h, llm_grid_w = (
                grid_h // self.spatial_merge_size,
                grid_w // self.spatial_merge_size,
            )
            index = torch.arange(grid_t * llm_grid_h * llm_grid_w).reshape(grid_t, llm_grid_h, llm_grid_w)
            pad_h = vit_merger_window_size - llm_grid_h % vit_merger_window_size
            pad_w = vit_merger_window_size - llm_grid_w % vit_merger_window_size
            num_windows_h = (llm_grid_h + pad_h) // vit_merger_window_size
            num_windows_w = (llm_grid_w + pad_w) // vit_merger_window_size
            index_padded = F.pad(index, (0, pad_w, 0, pad_h), "constant", -100)
            index_padded = index_padded.reshape(
                grid_t,
                num_windows_h,
                vit_merger_window_size,
                num_windows_w,
                vit_merger_window_size,
            )
            index_padded = index_padded.permute(0, 1, 3, 2, 4).reshape(
                grid_t,
                num_windows_h * num_windows_w,
                vit_merger_window_size,
                vit_merger_window_size,
            )
            seqlens = (index_padded != -100).sum([2, 3]).reshape(-1)
            index_padded = index_padded.reshape(-1)
            index_new = index_padded[index_padded != -100]
            window_index.append(index_new + window_index_id)
            cu_seqlens_tmp = seqlens.cumsum(0) * self.spatial_merge_unit + cu_window_seqlens[-1]
            cu_window_seqlens.extend(cu_seqlens_tmp.tolist())
            window_index_id += (grid_t * llm_grid_h * llm_grid_w).item()
        window_index = torch.cat(window_index, dim=0)

        return window_index, cu_window_seqlens

    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        Args:
            hidden_states (`torch.Tensor` of shape `(seq_len, hidden_size)`):
                The final hidden states of the model.
            grid_thw (`torch.Tensor` of shape `(num_images_or_videos, 3)`):
                The temporal, height and width of feature shape of each image in LLM.

        Returns:
            `torch.Tensor`: hidden_states.
        """
        hidden_states = self.patch_embed(hidden_states)
        rotary_pos_emb = self.rot_pos_emb(grid_thw)
        window_index, cu_window_seqlens = self.get_window_index(grid_thw)
        cu_window_seqlens = torch.tensor(
            cu_window_seqlens,
            device=hidden_states.device,
            dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,
        )
        cu_window_seqlens = torch.unique_consecutive(cu_window_seqlens)

        seq_len, _ = hidden_states.size()
        hidden_states = hidden_states.reshape(seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
        hidden_states = hidden_states[window_index, :, :]
        hidden_states = hidden_states.reshape(seq_len, -1)
        rotary_pos_emb = rotary_pos_emb.reshape(seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
        rotary_pos_emb = rotary_pos_emb[window_index, :, :]
        rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)
        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)
        position_embeddings = (emb.cos(), emb.sin())

        cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]).cumsum(
            dim=0,
            # Select dtype based on the following factors:
            #  - FA2 requires that cu_seqlens_q must have dtype int32
            #  - torch.onnx.export requires that cu_seqlens_q must have same dtype as grid_thw
            # See https://github.com/huggingface/transformers/pull/34852 for more information
            dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,
        )
        cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)

        for layer_num, blk in enumerate(self.blocks):
            if layer_num in self.fullatt_block_indexes:
                cu_seqlens_now = cu_seqlens
            else:
                cu_seqlens_now = cu_window_seqlens

            hidden_states = blk(
                hidden_states,
                cu_seqlens=cu_seqlens_now,
                position_embeddings=position_embeddings,
                **kwargs,
            )

        hidden_states = self.merger(hidden_states)
        reverse_indices = torch.argsort(window_index)
        hidden_states = hidden_states[reverse_indices, :]

        return hidden_states


@dataclass
@auto_docstring(
    custom_intro="""
    Base class for Llava outputs, with hidden states and attentions.
    """
)
class Qwen2_5_VLModelOutputWithPast(ModelOutput):
    r"""
    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)

        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
        `past_key_values` input) to speed up sequential decoding.
    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):
        The rope index difference between sequence length and multimodal rope.
    """

    last_hidden_state: torch.FloatTensor = None
    past_key_values: Optional[list[torch.FloatTensor]] = None
    hidden_states: Optional[tuple[torch.FloatTensor]] = None
    attentions: Optional[tuple[torch.FloatTensor]] = None
    rope_deltas: Optional[torch.LongTensor] = None


class Qwen2_5_VLRotaryEmbedding(nn.Module):
    inv_freq: torch.Tensor  # fix linting for `register_buffer`

    def __init__(self, config: Qwen2_5_VLTextConfig, device=None):
        super().__init__()
        # BC: "rope_type" was originally "type"
        if hasattr(config, "rope_scaling") and config.rope_scaling is not None:
            self.rope_type = config.rope_scaling.get("rope_type", config.rope_scaling.get("type"))
        else:
            self.rope_type = "default"
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config
        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = self.inv_freq

    @torch.no_grad()
    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
    def forward(self, x, position_ids):
        # In contrast to other models, Qwen2_5_VL has different position ids for the grids
        # So we expand the inv_freq to shape (3, ...)
        inv_freq_expanded = self.inv_freq[None, None, :, None].float().expand(3, position_ids.shape[1], -1, 1)
        position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)

        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):  # Force float32
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


class Qwen2MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


def apply_multimodal_rotary_pos_emb(q, k, cos, sin, mrope_section, unsqueeze_dim=1):
    """Applies Rotary Position Embedding with Multimodal Sections to the query and key tensors (https://qwenlm.github.io/blog/qwen2-vl/).

    Explanation:
        Multimodal 3D rotary position embedding is an extension to 1D rotary position embedding. The input embedding
        sequence contains vision (images / videos) embedding and text embedding or just contains text embedding. For
        vision embedding part, we apply rotary position embedding on temporal, height and width dimension separately.
        Here we split the channel dimension to 3 chunks for the temporal, height and width rotary position embedding.
        For text embedding part, we just apply 1D rotary position embedding. The three rotary position index (temporal,
        height and width) of text embedding is always the same, so the text embedding rotary position embedding has no
        difference with modern LLMs.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`):
            The position indices of the tokens corresponding to the query and key tensors. For example, this can be
            used to pass offsetted position ids when working with a KV-cache.
        mrope_section(`List(int)`):
            Multimodal rope section is for channel dimension of temporal, height and width in rope calculation.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    mrope_section = mrope_section * 2
    cos = torch.cat([m[i % 3] for i, m in enumerate(cos.split(mrope_section, dim=-1))], dim=-1).unsqueeze(
        unsqueeze_dim
    )
    sin = torch.cat([m[i % 3] for i, m in enumerate(sin.split(mrope_section, dim=-1))], dim=-1).unsqueeze(
        unsqueeze_dim
    )

    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


class Qwen2_5_VLAttention(nn.Module):
    """
    Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer
    and "Generating Long Sequences with Sparse Transformers".
    """

    def __init__(self, config: Qwen2_5_VLTextConfig, layer_idx: Optional[int] = None):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        if layer_idx is None:
            logger.warning_once(
                f"Instantiating {self.__class__.__name__} without passing `layer_idx` is not recommended and will "
                "to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` "
                "when creating this class."
            )

        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
        self.is_causal = True
        self.attention_dropout = config.attention_dropout
        self.rope_scaling = config.rope_scaling
        self.scaling = self.head_dim**-0.5

        if (self.head_dim * self.num_heads) != self.hidden_size:
            raise ValueError(
                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
                f" and `num_heads`: {self.num_heads})."
            )
        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)
        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)
        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)
        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == "sliding_attention" else None

        self.rotary_emb = Qwen2_5_VLRotaryEmbedding(config=config)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:
        bsz, q_len, _ = hidden_states.size()

        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)

        cos, sin = position_embeddings
        query_states, key_states = apply_multimodal_rotary_pos_emb(
            query_states, key_states, cos, sin, self.rope_scaling["mrope_section"]
        )

        if past_key_value is not None:
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}  # Specific to RoPE models
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            sliding_window=self.sliding_window,
            position_ids=position_ids,  # pass positions for FA2
            **kwargs,
        )

        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights


class Qwen2_5_VLDecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: Qwen2_5_VLTextConfig, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size

        if config.use_sliding_window and config._attn_implementation != "flash_attention_2":
            logger.warning_once(
                f"Sliding Window Attention is enabled but not implemented for `{config._attn_implementation}`; "
                "unexpected results may be encountered."
            )
        self.self_attn = Qwen2_5_VLAttention(config, layer_idx)

        self.mlp = Qwen2MLP(config)
        self.input_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.attention_type = config.layer_types[layer_idx]

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[tuple[torch.Tensor]] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:
        """
        Args:
            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size
                `(batch, sequence_length)` where padding elements are indicated by 0.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
            use_cache (`bool`, *optional*):
                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
                (see `past_key_values`).
            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
                Indices depicting the position of the input sequence tokens in the sequence.
            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):
                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,
                with `head_dim` being the embedding dimension of each attention head.
            kwargs (`dict`, *optional*):
                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code
                into the model
        """

        residual = hidden_states

        hidden_states = self.input_layernorm(hidden_states)

        # Self Attention
        hidden_states, self_attn_weights = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
            cache_position=cache_position,
            position_embeddings=position_embeddings,
            **kwargs,
        )
        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        outputs = (hidden_states,)

        if output_attentions:
            outputs += (self_attn_weights,)

        return outputs


@auto_docstring
class Qwen2_5_VLTextModel(Qwen2_5_VLPreTrainedModel):
    config: Qwen2_5_VLTextConfig

    def __init__(self, config: Qwen2_5_VLTextConfig):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers = nn.ModuleList(
            [Qwen2_5_VLDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )
        self._attn_implementation = config._attn_implementation
        self.norm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = Qwen2_5_VLRotaryEmbedding(config=config)
        self.has_sliding_layers = "sliding_attention" in self.config.layer_types

        self.gradient_checkpointing = False
        # Initialize weights and apply final processing
        self.post_init()

    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Union[tuple, BaseModelOutputWithPast]:
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache

        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        if self.gradient_checkpointing and self.training:
            if use_cache:
                logger.warning_once(
                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
                )
                use_cache = False

        # torch.jit.trace() doesn't support cache objects in the output
        if use_cache and past_key_values is None and not torch.jit.is_tracing():
            past_key_values = DynamicCache()

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )

        # the hard coded `3` is for temporal, height and width.
        if position_ids is None:
            position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)
        elif position_ids.ndim == 2:
            position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)

        # NOTE: we need to pass text position ids for packing. Qwen2-VL uses 3D positions
        # where each dim indicates visual spatial positions for temporal/height/width grids.
        # There are two scenarios when FA2-like packed masking might be activated.
        # 1. User specifically passed packed `position_ids` and no attention mask.
        #    In this case we expect the useer to create correct position ids for all 3 grids
        #    and prepend text-only position ids to it. The final tensor will be [4, bs, seq-len]
        # 2. User runs forward with no attention mask and no position ids. In this case, position ids
        #    are prepared by the model (`get_rope_index`) as `[4, bs, seq-len]` tensor. Text-only positions are
        #    prepended by us when creating positions so that the mask is constructed correctly. NOTE: failing to pass
        #    text-only positions will cause incorrect mask construction, do not change `prepare_input_for_generation`
        if position_ids.ndim == 3 and position_ids.shape[0] == 4:
            text_position_ids = position_ids[0]
            position_ids = position_ids[1:]
        else:
            text_position_ids = position_ids[0]

        # It may already have been prepared by e.g. `generate`
        if not isinstance(causal_mask_mapping := attention_mask, dict):
            # Prepare mask arguments
            mask_kwargs = {
                "config": self.config,
                "input_embeds": inputs_embeds,
                "attention_mask": attention_mask,
                "cache_position": cache_position,
                "past_key_values": past_key_values,
                "position_ids": text_position_ids,
            }
            # Create the masks
            causal_mask_mapping = {
                "full_attention": create_causal_mask(**mask_kwargs),
            }
            # The sliding window alternating layers are not always activated depending on the config
            if self.has_sliding_layers:
                causal_mask_mapping["sliding_attention"] = create_sliding_window_causal_mask(**mask_kwargs)

        hidden_states = inputs_embeds

        # create position embeddings to be shared across the decoder layers
        position_embeddings = self.rotary_emb(hidden_states, position_ids)

        # decoder layers
        all_hidden_states = () if output_hidden_states else None
        all_self_attns = () if output_attentions else None

        for decoder_layer in self.layers:
            if output_hidden_states:
                all_hidden_states += (hidden_states,)

            layer_outputs = decoder_layer(
                hidden_states,
                attention_mask=causal_mask_mapping[decoder_layer.attention_type],
                position_ids=text_position_ids,
                past_key_value=past_key_values,
                output_attentions=output_attentions,
                use_cache=use_cache,
                cache_position=cache_position,
                position_embeddings=position_embeddings,
                **kwargs,
            )

            hidden_states = layer_outputs[0]

            if output_attentions:
                all_self_attns += (layer_outputs[1],)

        hidden_states = self.norm(hidden_states)

        # add hidden states from the last decoder layer
        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        if not return_dict:
            return tuple(
                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns] if v is not None
            )
        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=past_key_values,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
        )



@auto_docstring
class Qwen2_5_VLModel(Qwen2_5_VLPreTrainedModel):
    base_model_prefix = ""
    _checkpoint_conversion_mapping = {"^model": "language_model"}
    config_class = Qwen2_5_VLConfig
    _no_split_modules = ["Qwen2_5_VLDecoderLayer", "Qwen2_5_VLVisionBlock"]

    def __init__(self, config):
        super().__init__(config)
        self.visual = Qwen2_5_VisionTransformerPretrainedModel._from_config(config.vision_config)
        self.language_model = Qwen2_5_VLTextModel._from_config(config.text_config)
        self.rope_deltas = None  # cache rope_deltas here
        self.number_encoder = nn.Linear(5, config.hidden_size, bias=False) #added
        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.language_model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.language_model.set_input_embeddings(value)

    def get_rope_index(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        image_grid_thw: Optional[torch.LongTensor] = None,
        video_grid_thw: Optional[torch.LongTensor] = None,
        second_per_grid_ts: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Calculate the 3D rope index based on image and video's temporal, height and width in LLM.

        Explanation:
            Each embedding sequence contains vision embedding and text embedding or just contains text embedding.

            For pure text embedding sequence, the rotary position embedding has no difference with modern LLMs.
            Examples:
                input_ids: [T T T T T], here T is for text.
                temporal position_ids: [0, 1, 2, 3, 4]
                height position_ids: [0, 1, 2, 3, 4]
                width position_ids: [0, 1, 2, 3, 4]

            For vision and text embedding sequence, we calculate 3D rotary position embedding for vision part
            and 1D rotary position embedding for text part.
            Examples:
                Temporal (Time): 3 patches, representing different segments of the video in time.
                Height: 2 patches, dividing each frame vertically.
                Width: 2 patches, dividing each frame horizontally.
                We also have some important parameters:
                fps (Frames Per Second): The video's frame rate, set to 1. This means one frame is processed each second.
                tokens_per_second: This is a crucial parameter. It dictates how many "time-steps" or "temporal tokens" are conceptually packed into a one-second interval of the video. In this case, we have 25 tokens per second. So each second of the video will be represented with 25 separate time points. It essentially defines the temporal granularity.
                temporal_patch_size: The number of frames that compose one temporal patch. Here, it's 2 frames.
                interval: The step size for the temporal position IDs, calculated as tokens_per_second * temporal_patch_size / fps. In this case, 25 * 2 / 1 = 50. This means that each temporal patch will be have a difference of 50 in the temporal position IDs.
                input_ids: [V V V V V V V V V V V V T T T T T], here V is for vision.
                vision temporal position_ids: [0, 0, 0, 0, 50, 50, 50, 50, 100, 100, 100, 100]
                vision height position_ids: [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]
                vision width position_ids: [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
                text temporal position_ids: [101, 102, 103, 104, 105]
                text height position_ids: [101, 102, 103, 104, 105]
                text width position_ids: [101, 102, 103, 104, 105]
                Here we calculate the text start position_ids as the max vision position_ids plus 1.

        Args:
            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
                it.
            image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):
                The temporal, height and width of feature shape of each image in LLM.
            video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):
                The temporal, height and width of feature shape of each video in LLM.
            second_per_grid_ts (`torch.Tensor` of shape `(num_videos)`, *optional*):
                The time interval (in seconds) for each grid along the temporal dimension in the 3D position IDs.
            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

                - 1 for tokens that are **not masked**,
                - 0 for tokens that are **masked**.

        Returns:
            position_ids (`torch.LongTensor` of shape `(3, batch_size, sequence_length)`)
            mrope_position_deltas (`torch.Tensor` of shape `(batch_size)`)
        """
        spatial_merge_size = self.config.vision_config.spatial_merge_size
        image_token_id = self.config.image_token_id
        video_token_id = self.config.video_token_id
        vision_start_token_id = self.config.vision_start_token_id
        mrope_position_deltas = []
        if input_ids is not None and (image_grid_thw is not None or video_grid_thw is not None):
            total_input_ids = input_ids
            if attention_mask is None:
                attention_mask = torch.ones_like(total_input_ids)
            position_ids = torch.ones(
                3,
                input_ids.shape[0],
                input_ids.shape[1],
                dtype=input_ids.dtype,
                device=input_ids.device,
            )
            image_index, video_index = 0, 0
            attention_mask = attention_mask.to(total_input_ids.device)
            for i, input_ids in enumerate(total_input_ids):
                input_ids = input_ids[attention_mask[i] == 1]
                image_nums, video_nums = 0, 0
                vision_start_indices = torch.argwhere(input_ids == vision_start_token_id).squeeze(1)
                vision_tokens = input_ids[vision_start_indices + 1]
                image_nums = (vision_tokens == image_token_id).sum()
                video_nums = (vision_tokens == video_token_id).sum()
                input_tokens = input_ids.tolist()
                llm_pos_ids_list: list = []
                st = 0
                remain_images, remain_videos = image_nums, video_nums
                for _ in range(image_nums + video_nums):
                    if image_token_id in input_tokens and remain_images > 0:
                        ed_image = input_tokens.index(image_token_id, st)
                    else:
                        ed_image = len(input_tokens) + 1
                    if video_token_id in input_tokens and remain_videos > 0:
                        ed_video = input_tokens.index(video_token_id, st)
                    else:
                        ed_video = len(input_tokens) + 1
                    if ed_image < ed_video:
                        t, h, w = (
                            image_grid_thw[image_index][0],
                            image_grid_thw[image_index][1],
                            image_grid_thw[image_index][2],
                        )
                        second_per_grid_t = 0
                        image_index += 1
                        remain_images -= 1
                        ed = ed_image

                    else:
                        t, h, w = (
                            video_grid_thw[video_index][0],
                            video_grid_thw[video_index][1],
                            video_grid_thw[video_index][2],
                        )
                        if second_per_grid_ts is not None:
                            second_per_grid_t = second_per_grid_ts[video_index]
                        else:
                            second_per_grid_t = 1.0
                        video_index += 1
                        remain_videos -= 1
                        ed = ed_video
                    llm_grid_t, llm_grid_h, llm_grid_w = (
                        t.item(),
                        h.item() // spatial_merge_size,
                        w.item() // spatial_merge_size,
                    )
                    text_len = ed - st

                    st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0
                    llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)

                    range_tensor = torch.arange(llm_grid_t).view(-1, 1)
                    expanded_range = range_tensor.expand(-1, llm_grid_h * llm_grid_w)

                    ## normalize type, send to device.
                    second_per_grid_t = torch.as_tensor(
                        second_per_grid_t, dtype=range_tensor.dtype, device=range_tensor.device
                    )

                    time_tensor = expanded_range * second_per_grid_t * self.config.vision_config.tokens_per_second

                    time_tensor_long = time_tensor.long()
                    t_index = time_tensor_long.flatten()

                    h_index = torch.arange(llm_grid_h).view(1, -1, 1).expand(llm_grid_t, -1, llm_grid_w).flatten()
                    w_index = torch.arange(llm_grid_w).view(1, 1, -1).expand(llm_grid_t, llm_grid_h, -1).flatten()
                    llm_pos_ids_list.append(torch.stack([t_index, h_index, w_index]) + text_len + st_idx)
                    st = ed + llm_grid_t * llm_grid_h * llm_grid_w

                if st < len(input_tokens):
                    st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0
                    text_len = len(input_tokens) - st
                    llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)

                llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)
                position_ids[..., i, attention_mask[i] == 1] = llm_positions.to(position_ids.device)
                mrope_position_deltas.append(llm_positions.max() + 1 - len(total_input_ids[i]))
            mrope_position_deltas = torch.tensor(mrope_position_deltas, device=input_ids.device).unsqueeze(1)
            return position_ids, mrope_position_deltas
        else:
            if attention_mask is not None:
                position_ids = attention_mask.long().cumsum(-1) - 1
                position_ids.masked_fill_(attention_mask == 0, 1)
                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1).to(attention_mask.device)
                max_position_ids = position_ids.max(0, keepdim=False)[0].max(-1, keepdim=True)[0]
                mrope_position_deltas = max_position_ids + 1 - attention_mask.shape[-1]
            else:
                position_ids = (
                    torch.arange(input_ids.shape[1], device=input_ids.device)
                    .view(1, 1, -1)
                    .expand(3, input_ids.shape[0], -1)
                )
                mrope_position_deltas = torch.zeros(
                    [input_ids.shape[0], 1],
                    device=input_ids.device,
                    dtype=input_ids.dtype,
                )

            return position_ids, mrope_position_deltas

    # @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        number_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        pixel_values: Optional[torch.Tensor] = None,
        pixel_values_videos: Optional[torch.FloatTensor] = None,
        image_grid_thw: Optional[torch.LongTensor] = None,
        video_grid_thw: Optional[torch.LongTensor] = None,
        rope_deltas: Optional[torch.LongTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
        second_per_grid_ts: Optional[torch.Tensor] = None,
    ) -> Union[Tuple, Qwen2_5_VLModelOutputWithPast]:
        r"""
        pixel_values_videos (`torch.FloatTensor` of shape `(seq_length, num_channels * temporal_size * image_size * image_size)):
            The tensors corresponding to the input videos. Pixel values can be obtained using
            [`AutoImageProcessor`]. See [`Qwen2_5_VLImageProcessor.__call__`] for details. [`Qwen2_5_VLProcessor`] uses
            [`Qwen2_5_VLImageProcessor`] for processing videos.
        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):
            The temporal, height and width of feature shape of each image in LLM.
        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):
            The temporal, height and width of feature shape of each video in LLM.
        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):
            The rope index difference between sequence length and multimodal rope.
        second_per_grid_ts (`torch.Tensor` of shape `(num_videos)`, *optional*):
            The time interval (in seconds) for each grid along the temporal dimension in the 3D position IDs.
        """

        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        if inputs_embeds is None:
            inputs_embeds = self.get_input_embeddings()(input_ids)
            
            if number_ids is not None:
                # if number_ids.shape==input_ids.shape:
                #     numbers_embeds = number_ids.to(inputs_embeds.dtype).unsqueeze(-1)
                # else:
                #     numbers_embeds = torch.ones_like(input_ids, dtype = inputs_embeds.dtype)
                #     # numbers_embeds = self.number_encoder(numbers_values) #added
                #     numbers_mask = (input_ids==self.config.number_token_id)
                #     numbers_embeds = numbers_embeds.masked_scatter(numbers_mask, number_ids.to(numbers_embeds.dtype)).unsqueeze(-1)
                #     print("dtype = ", number_ids.shape, input_ids.shape(), flush = True)
                numbers_embeds = self.number_encoder(number_ids)
                numbers_mask = (input_ids==self.config.number_token_id)
                inputs_embeds = torch.where(numbers_mask.unsqueeze(-1), inputs_embeds * numbers_embeds, inputs_embeds)
                # inputs_embeds = inputs_embeds*numbers_embeds
            if pixel_values is not None:
                pixel_values = pixel_values.type(self.visual.dtype)
                image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)
                n_image_tokens = (input_ids == self.config.image_token_id).sum().item()
                n_image_features = image_embeds.shape[0]
                if n_image_tokens != n_image_features:
                    raise ValueError(
                        f"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}"
                    )

                mask = input_ids == self.config.image_token_id
                mask_unsqueezed = mask.unsqueeze(-1)
                mask_expanded = mask_unsqueezed.expand_as(inputs_embeds)
                image_mask = mask_expanded.to(inputs_embeds.device)

                image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)
                inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)

            if pixel_values_videos is not None:
                pixel_values_videos = pixel_values_videos.type(self.visual.dtype)
                video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)
                n_video_tokens = (input_ids == self.config.video_token_id).sum().item()
                n_video_features = video_embeds.shape[0]
                if n_video_tokens != n_video_features:
                    raise ValueError(
                        f"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}"
                    )

                mask = input_ids == self.config.video_token_id
                mask_unsqueezed = mask.unsqueeze(-1)
                mask_expanded = mask_unsqueezed.expand_as(inputs_embeds)
                video_mask = mask_expanded.to(inputs_embeds.device)

                video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)
                inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)

            if attention_mask is not None:
                attention_mask = attention_mask.to(inputs_embeds.device)

        # if we get 4D attention mask we cannot calculate rope deltas anymore. TODO @raushan fixme
        if position_ids is None and (attention_mask is None or attention_mask.ndim == 2):
            # calculate RoPE index once per generation in the pre-fill stage only
            if (
                (cache_position is not None and cache_position[0] == 0)
                or self.rope_deltas is None
                or (past_key_values is None or past_key_values.get_seq_length() == 0)
            ):
                position_ids, rope_deltas = self.get_rope_index(
                    input_ids,
                    image_grid_thw,
                    video_grid_thw,
                    second_per_grid_ts,
                    attention_mask,
                )
                self.rope_deltas = rope_deltas
            # then use the prev pre-calculated rope-deltas to get the correct position ids
            else:
                batch_size, seq_length, _ = inputs_embeds.shape
                delta = (
                    (cache_position[0] + self.rope_deltas).to(inputs_embeds.device)
                    if cache_position is not None
                    else 0
                )
                position_ids = torch.arange(seq_length, device=inputs_embeds.device)
                position_ids = position_ids.view(1, -1).expand(batch_size, -1)
                if cache_position is not None:  # otherwise `deltas` is an int `0`
                    delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)
                position_ids = position_ids.add(delta)
                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)

        outputs = self.language_model(
            input_ids=None,
            position_ids=position_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=True,
            cache_position=cache_position,
        )

        output = Qwen2_5_VLModelOutputWithPast(
            last_hidden_state=outputs.last_hidden_state,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            rope_deltas=self.rope_deltas,
        )
        return output if return_dict else output.to_tuple()


@dataclass
class Qwen2_5_VLCausalLMOutputWithPast(ModelOutput):
    """
    Base class for Qwen2_5_VL causal language model (or autoregressive) outputs.

    Args:
        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):
            Language modeling loss (for next-token prediction).
        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):
            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)

            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
            `past_key_values` input) to speed up sequential decoding.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +
            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.
        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`.

            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
            heads.
        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):
            The rope index difference between sequence length and multimodal rope.
    """

    loss: Optional[torch.FloatTensor] = None
    lm_loss: Optional[torch.FloatTensor] = None
    l1_loss: Optional[torch.FloatTensor] = None
    iou_loss: Optional[torch.FloatTensor] = None
    kld_loss: Optional[torch.FloatTensor] = None
    coord_loss: Optional[torch.FloatTensor] = None
    logits: Optional[torch.FloatTensor] = None
    logits_number: Optional[torch.FloatTensor] = None
    past_key_values: Optional[List[torch.FloatTensor]] = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None
    rope_deltas: Optional[torch.LongTensor] = None

# Instead of using nn.MLP, define the equivalent with Linear layers
class LeLNumberHead(nn.Module):
    def __init__(self, input_dim, hidden_dim=None, output_dim=5, dropout=0.1, coord_scale=2.0):
        super().__init__()
        
        if hidden_dim is None:
            hidden_dim = input_dim // 2
            
        self.coord_scale = coord_scale  # Ã‰chelle max des coordonnÃ©es
        
        self.linear1 = nn.Linear(input_dim, hidden_dim)
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.activation1 = nn.GELU()
        self.dropout1 = nn.Dropout(dropout)
        
        self.linear2 = nn.Linear(hidden_dim, hidden_dim // 2)
        self.norm2 = nn.LayerNorm(hidden_dim // 2)
        self.activation2 = nn.GELU()
        self.dropout2 = nn.Dropout(dropout)
        
        self.linear3 = nn.Linear(hidden_dim // 2, output_dim)
        
        self._init_weights()
        
    def _init_weights(self):
        """Initialisation adaptÃ©e aux coordonnÃ©es rÃ©elles"""
        for module in [self.linear1, self.linear2]:
            nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        
        # Initialisation plus petite pour la couche finale
        nn.init.xavier_uniform_(self.linear3.weight, gain=0.1)
        
        with torch.no_grad():
            # Biais adaptÃ©s Ã  l'Ã©chelle rÃ©elle de vos donnÃ©es
            self.linear3.bias[0] = 1.0  # cx au centre (environ 1.0 pour des coords ~[0,2])
            self.linear3.bias[1] = 1.0  # cy au centre
            self.linear3.bias[2] = 0.5  # largeur raisonnable
            self.linear3.bias[3] = 0.5  # hauteur raisonnable
            self.linear3.bias[4] = 0.0  # angle
    
    def forward(self, x):
        x = self.linear1(x)
        x = self.norm1(x)
        x = self.activation1(x)
        x = self.dropout1(x)
        
        x = self.linear2(x)
        x = self.norm2(x)
        x = self.activation2(x)
        x = self.dropout2(x)
        
        x = self.linear3(x)
        
        # Post-traitement adaptÃ© Ã  vos Ã©chelles
        cx, cy, w, h, angle = x.split(1, dim=-1)
        
        # Appliquer sigmoid puis rescaler Ã  [0, coord_scale]
        cx = torch.sigmoid(cx) * self.coord_scale
        cy = torch.sigmoid(cy) * self.coord_scale
        w = torch.sigmoid(w) * self.coord_scale  # largeur max = coord_scale
        h = torch.sigmoid(h) * self.coord_scale  # hauteur max = coord_scale
        
        # Angle normalisÃ© dans [-Ï€/2, Ï€/2]
        angle = torch.tanh(angle) * (torch.pi / 2)
        
        return torch.cat([cx, cy, w, h, angle], dim=-1)

class LeL2_ForConditionalGeneration(Qwen2_5_VLPreTrainedModel, GenerationMixin):
    _checkpoint_conversion_mapping = {
        "^visual": "model.visual",
        r"^model(?!\.(language_model|visual))": "model.language_model",
    }
    _tied_weights_keys = ["lm_head.weight"]

    def __init__(self, config):
        super().__init__(config)
        self.model = Qwen2_5_VLModel(config)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        # self.routing_head = nn.Linear(config.hidden_size, 2, bias=False) #added
        self.numbers_head = LeLNumberHead(config.hidden_size,config.hidden_size, 5) #added
        # self.localisation_loss = HybridLocalisationLoss(
        #                     l1_weight=1.0,
        #                     iou_weight=5.0,
        #                     iou_type='ciou',
        #                     coord_format='cxcywha', 
        #                     coord_scale=2.0
        #                 ).to(self.device)
        # self.localisation_loss = KLDHybridLocalizationLoss(l1_weight = 10.0,
        #          kld_weight = 1.0,
        #          coord_format = 'xyxy',  # 'cxcywha' ou 'xyxy'
        #          angle_normalization = 'symmetric_pi', # ou 'positive_pi' ou None
        #          eps = 1e-7,
        #          coord_max = 2.0).to(self.device)
        self.localisation_loss = EnhancedKLDHybridLocalizationLoss( 
                loss_type = 'kld',  # 'kld', 'iou', 'giou', 'diou', 'ciou', 'siou'
                l1_weight = 10.0,
                main_loss_weight = 1.0,
                coord_format = 'xyxy',
                angle_normalization = 'symmetric_pi',
                eps = 1e-7,
                coord_max = 2.0)
        self.post_init()

    def get_input_embeddings(self):
        return self.model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.model.set_input_embeddings(value)

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model = decoder

    def get_decoder(self):
        return self.model

    # Make modules available throught conditional class for BC
    @property
    def language_model(self):
        return self.model.language_model

    @property
    def visual(self):
        return self.model.visual

    @can_return_tuple
    # @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        number_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        pixel_values: Optional[torch.Tensor] = None,
        pixel_values_videos: Optional[torch.FloatTensor] = None,
        image_grid_thw: Optional[torch.LongTensor] = None,
        video_grid_thw: Optional[torch.LongTensor] = None,
        rope_deltas: Optional[torch.LongTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
        second_per_grid_ts: Optional[torch.Tensor] = None,
    ) -> Union[Tuple, Qwen2_5_VLCausalLMOutputWithPast]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
        pixel_values_videos (`torch.FloatTensor` of shape `(seq_length, num_channels * temporal_size * image_size * image_size)):
            The tensors corresponding to the input videos. Pixel values can be obtained using
            [`AutoImageProcessor`]. See [`Qwen2_5_VLImageProcessor.__call__`] for details. [`Qwen2_5_VLProcessor`] uses
            [`Qwen2_5_VLImageProcessor`] for processing videos.
        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):
            The temporal, height and width of feature shape of each image in LLM.
        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):
            The temporal, height and width of feature shape of each video in LLM.
        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):
            The rope index difference between sequence length and multimodal rope.
        second_per_grid_ts (`torch.Tensor` of shape `(num_videos)`, *optional*):
            The time interval (in seconds) for each grid along the temporal dimension in the 3D position IDs.

        Example:

        ```python
        >>> from PIL import Image
        >>> import requests
        >>> from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration

        >>> model = Qwen2_5_VLForConditionalGeneration.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct")
        >>> processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct")

        >>> messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": "What is shown in this image?"},
                ],
            },
        ]
        >>> url = "https://www.ilankelman.org/stopsigns/australia.jpg"
        >>> image = Image.open(requests.get(url, stream=True).raw)

        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ..."
        ```"""

        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.model(
            input_ids=input_ids, 
            number_ids=number_ids, # ToDo : ajouter number_ids partout pour propager
            pixel_values=pixel_values,
            pixel_values_videos=pixel_values_videos,
            image_grid_thw=image_grid_thw,
            video_grid_thw=video_grid_thw,
            second_per_grid_ts=second_per_grid_ts,
            position_ids=position_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )

        hidden_states = outputs[0]
        logits = self.lm_head(hidden_states)
        logits_number = self.numbers_head(hidden_states)
        # logits_number = torch.clamp(logits_number, min = 0, max = 2)
        loss = None
        lm_loss = None
        l1_loss = None
        iou_loss = None
        kld_loss = None
        coord_loss = None
        if labels is not None:
            text_label, number_label = labels
            lm_loss = self.loss_function(logits=logits, labels=text_label, vocab_size=self.config.vocab_size)
            number_mask = (text_label == self.config.number_token_id).unsqueeze(-1)
            # number_loss = torch.sum(F.smooth_l1_loss(input=logits_number[:,:-1].squeeze(-1)*number_mask[:,1:],target=number_ids[:,1:]*number_mask[:,1:], reduction="none"))/torch.sum(number_mask)
            # coord_loss = number_loss_fn(logits_number, number_label, number_mask)
            coord_loss = self.localisation_loss(logits_number, number_label, number_mask)
            loss_components = self.localisation_loss.get_loss_components()
            l1_loss = loss_components.get("l1_loss", None)
            iou_loss = loss_components.get("iou_loss", None)
            kld_loss = loss_components.get("kld_loss", None)
            loss = 1*lm_loss + 1*coord_loss

        if not return_dict:
            output = (logits,logits_number, ) + outputs[1:]
            return (loss,) + output if loss is not None else output

        return Qwen2_5_VLCausalLMOutputWithPast(
            loss=loss,
            lm_loss = lm_loss,
            l1_loss = l1_loss,
            iou_loss = iou_loss,
            kld_loss = kld_loss,
            coord_loss = coord_loss,
            logits=logits,
            logits_number=logits_number,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            rope_deltas=outputs.rope_deltas,
        )

    def prepare_inputs_for_generation(
        self,
        input_ids,
        number_ids,
        past_key_values=None,
        attention_mask=None,
        inputs_embeds=None,
        cache_position=None,
        position_ids=None,
        use_cache=True,
        pixel_values=None,
        pixel_values_videos=None,
        image_grid_thw=None,
        video_grid_thw=None,
        second_per_grid_ts=None,
        **kwargs,
    ):
        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model
    
        model_inputs = super().prepare_inputs_for_generation(
            input_ids,
            past_key_values=past_key_values,
            attention_mask=attention_mask,
            inputs_embeds=inputs_embeds,
            cache_position=cache_position,
            position_ids=position_ids,
            pixel_values=pixel_values,
            pixel_values_videos=pixel_values_videos,
            image_grid_thw=image_grid_thw,
            video_grid_thw=video_grid_thw,
            second_per_grid_ts=second_per_grid_ts,
            use_cache=use_cache,
            **kwargs,
        )
    
        # Qwen2-5-VL position_ids are prepared with rope_deltas in forward
        model_inputs["position_ids"] = None
        
        # Gestion des valeurs numÃ©riques
        if number_ids is not None and hasattr(self.config, 'number_token_id'):
            numbers_mask = (input_ids == self.config.number_token_id)
            
            # VÃ©rifier s'il y a des tokens numÃ©riques
            if numbers_mask.any():
                # Convertir number_ids en tenseur si nÃ©cessaire
                if isinstance(number_ids, (list, tuple)):
                    number_ids = torch.tensor(number_ids, dtype=number_ids.dtype, device=input_ids.device)
                else:
                    number_ids = number_ids.to(dtype=torch.bfloat16, device=input_ids.device)
                
                # CrÃ©er le tableau des valeurs numÃ©riques
                numbers_array = torch.ones((*input_ids.shape,5), dtype=torch.bfloat16, device=input_ids.device)
                
                # Gestion des diffÃ©rents formats d'entrÃ©e
                if number_ids[:,:,0].numel() == numbers_mask.sum().item():
                    # Format liste: seulement les valeurs numÃ©riques
                    numbers_array = numbers_array.masked_scatter(numbers_mask.unsqueeze(-1), number_ids)
                elif number_ids.shape[:,:,0] == input_ids.shape:
                    # Format tenseur complet
                    numbers_array = number_ids
                else:
                    print(f"Warning: Format number_ids incompatible: {number_ids.shape} vs input_ids: {input_ids.shape}")
                    model_inputs["number_ids"] = None
                    
                # Extraire les positions courantes et vÃ©rifier s'il y a des tokens numÃ©riques
                if "number_ids" not in model_inputs or model_inputs["number_ids"] is not None:
                    current_numbers = torch.index_select(numbers_array, 1, cache_position)
                    current_mask = torch.index_select(numbers_mask, 1, cache_position)
                    
                    # Si pas de tokens numÃ©riques dans les positions courantes, mettre Ã  None
                    model_inputs["number_ids"] = current_numbers if current_mask.any() else None
            else:
                # Pas de tokens numÃ©riques trouvÃ©s
                model_inputs["number_ids"] = None
        else:
            # number_ids est None ou pas de configuration
            model_inputs["number_ids"] = None
        
        if cache_position[0] != 0:
            model_inputs["pixel_values"] = None
            model_inputs["pixel_values_videos"] = None
        
        # Debug optionnel (dÃ©commente si besoin)
        # print("\n"*2,"-"*40,"dim avant generation")
        # print("input_ids :", input_ids.shape if input_ids is not None else None)
        # print("number_ids :", model_inputs["number_ids"].shape if model_inputs["number_ids"] is not None else None)
        # print("cache_position :", cache_position.shape if cache_position is not None else None)
        # print("-"*40,"\n"*2)
        
        return model_inputs

    def _get_image_nums_and_video_nums(
        self,
        input_ids: Optional[torch.LongTensor],
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Get the number of images and videos for each sample to calculate the separation length of the sample tensor.
        These parameters are not passed through the processor to avoid unpredictable impacts from interface modifications.

        Args:
            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
                Indices of input sequence tokens in the vocabulary.

        Returns:
            image_nums (`torch.LongTensor` of shape `(batch_size, num_images_sample)`)
            video_nums (`torch.LongTensor` of shape `(batch_size, num_videos_sample)`)
        """
        image_token_id = self.config.image_token_id
        video_token_id = self.config.video_token_id
        vision_start_token_id = self.config.vision_start_token_id

        vision_start_mask = input_ids == vision_start_token_id
        vision_first_mask = torch.roll(vision_start_mask, shifts=1, dims=1)
        image_mask = input_ids == image_token_id
        video_mask = input_ids == video_token_id
        image_nums = torch.sum(vision_first_mask & image_mask, dim=1)
        video_nums = torch.sum(vision_first_mask & video_mask, dim=1)

        return image_nums, video_nums

    def _expand_inputs_for_generation(
        self,
        expand_size: int = 1,
        is_encoder_decoder: bool = False,
        input_ids: Optional[torch.LongTensor] = None,
        **model_kwargs,
    ) -> Tuple[torch.LongTensor, Dict[str, Any]]:
        # Overwritten -- Support for expanding tensors without a batch size dimension
        # e.g., pixel_values, image_grid_thw, pixel_values_videos, video_grid_thw, second_per_grid_t
        # pixel_values.shape[0] is sum(seqlen_images for samples)
        # image_grid_thw.shape[0] is sum(num_images for samples)
        # ToDo : ajoutÃ© la gestion des chiffres

        if expand_size == 1:
            return input_ids, model_kwargs

        visual_keys = ["pixel_values", "image_grid_thw", "pixel_values_videos", "video_grid_thw", "second_per_grid_ts"]

        def _expand_dict_for_generation_visual(dict_to_expand):
            image_grid_thw = model_kwargs.get("image_grid_thw", None)
            video_grid_thw = model_kwargs.get("video_grid_thw", None)
            image_nums, video_nums = self._get_image_nums_and_video_nums(input_ids)

            def _repeat_interleave_samples(x, lengths, repeat_times):
                samples = torch.split(x, lengths)
                repeat_args = [repeat_times] + [1] * (x.dim() - 1)
                result = torch.cat([sample.repeat(*repeat_args) for sample in samples], dim=0)
                return result

            for key in dict_to_expand:
                if key == "pixel_values":
                    # split images into samples
                    samples = torch.split(image_grid_thw, list(image_nums))
                    # compute the sequence length of images for each sample
                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "image_grid_thw":
                    # get the num of images for each sample
                    lengths = list(image_nums)
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "pixel_values_videos":
                    samples = torch.split(video_grid_thw, list(video_nums))
                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "video_grid_thw":
                    lengths = list(video_nums)
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "second_per_grid_ts":
                    if not isinstance(dict_to_expand[key], list):
                        raise TypeError(
                            f"Expected value for key '{key}' to be a list, but got {type(dict_to_expand[key])} instead."
                        )
                    tensor = torch.tensor(dict_to_expand[key])
                    lengths = list(video_nums)
                    tensor = _repeat_interleave_samples(tensor, lengths=lengths, repeat_times=expand_size)
                    dict_to_expand[key] = tensor.tolist()
            return dict_to_expand

        def _expand_dict_for_generation(dict_to_expand):
            for key in dict_to_expand:
                if (
                    key != "cache_position"
                    and dict_to_expand[key] is not None
                    and isinstance(dict_to_expand[key], torch.Tensor)
                    and key not in visual_keys
                ):
                    dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)
            return dict_to_expand

        # input_ids is required for expanding visual inputs
        # If input_ids is unavailable, visual inputs will not be used; therefore, there is no need to expand visual inputs.
        if input_ids is not None and input_ids.numel() != 0:
            model_kwargs = _expand_dict_for_generation_visual(model_kwargs)

        if input_ids is not None:
            input_ids = input_ids.repeat_interleave(expand_size, dim=0)

        model_kwargs = _expand_dict_for_generation(model_kwargs)

        if is_encoder_decoder:
            if model_kwargs.get("encoder_outputs") is None:
                raise ValueError("If `is_encoder_decoder` is True, make sure that `encoder_outputs` is defined.")
            model_kwargs["encoder_outputs"] = _expand_dict_for_generation(model_kwargs["encoder_outputs"])

        return input_ids, model_kwargs

    @staticmethod
    def _prepare_4d_causal_attention_mask_with_cache_position(
        attention_mask: torch.Tensor,
        sequence_length: int,
        target_length: int,
        dtype: torch.dtype,
        cache_position: torch.Tensor,
        batch_size: int,
        **kwargs,
    ):
        """
        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape
        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.

        Args:
            attention_mask (`torch.Tensor`):
                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape
                `(batch_size, 1, query_length, key_value_length)`.
            sequence_length (`int`):
                The sequence length being processed.
            target_length (`int`):
                The target length: when generating with static cache, the mask should be as long as the static cache,
                to account for the 0 padding, the part of the cache that is not filled yet.
            dtype (`torch.dtype`):
                The dtype to use for the 4D attention mask.
            cache_position (`torch.Tensor`):
                Indices depicting the position of the input sequence tokens in the sequence.
            batch_size (`torch.Tensor`):
                Batch size.
        """
        if attention_mask is not None and attention_mask.dim() == 4:
            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.
            causal_mask = attention_mask
        else:
            min_dtype = torch.finfo(dtype).min
            causal_mask = torch.full(
                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device
            )
            if sequence_length != 1:
                causal_mask = torch.triu(causal_mask, diagonal=1)
            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)
            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)
            if attention_mask is not None:
                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit
                mask_length = attention_mask.shape[-1]
                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(
                    causal_mask.device
                )
                padding_mask = padding_mask == 0
                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(
                    padding_mask, min_dtype
                )

        return causal_mask

    @torch.no_grad()
    def generate(
        self,
        inputs: Optional[torch.Tensor] = None,
        generation_config: Optional[GenerationConfig] = None,
        logits_processor: Optional[LogitsProcessorList] = None,
        stopping_criteria: Optional[StoppingCriteriaList] = None,
        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,
        synced_gpus: Optional[bool] = None,
        assistant_model: Optional["PreTrainedModel"] = None,
        streamer: Optional["BaseStreamer"] = None,
        negative_prompt_ids: Optional[torch.Tensor] = None,
        negative_prompt_attention_mask: Optional[torch.Tensor] = None,
        use_model_defaults: Optional[bool] = None,
        **kwargs,
    ) -> Union[GenerateOutput, torch.LongTensor]:
        r"""

        Generates sequences of token ids for models with a language modeling head.

        <Tip warning={true}>

        Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the
        model's default generation configuration. You can override any `generation_config` by passing the corresponding
        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.

        For an overview of generation strategies and code examples, check out the [following
        guide](../generation_strategies).

        </Tip>

        Parameters:
            inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):
                The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the
                method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`
                should be in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of
                `input_ids`, `input_values`, `input_features`, or `pixel_values`.
            generation_config ([`~generation.GenerationConfig`], *optional*):
                The generation configuration to be used as base parametrization for the generation call. `**kwargs`
                passed to generate matching the attributes of `generation_config` will override them. If
                `generation_config` is not provided, the default will be used, which has the following loading
                priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model
                configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s
                default values, whose documentation should be checked to parameterize generation.
            logits_processor (`LogitsProcessorList`, *optional*):
                Custom logits processors that complement the default logits processors built from arguments and
                generation config. If a logit processor is passed that is already created with the arguments or a
                generation config an error is thrown. This feature is intended for advanced users.
            stopping_criteria (`StoppingCriteriaList`, *optional*):
                Custom stopping criteria that complements the default stopping criteria built from arguments and a
                generation config. If a stopping criteria is passed that is already created with the arguments or a
                generation config an error is thrown. If your stopping criteria depends on the `scores` input, make
                sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`. This feature is
                intended for advanced users.
            prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):
                If provided, this function constraints the beam search to allowed tokens only at each step. If not
                provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and
                `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned
                on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful
                for constrained generation conditioned on the prefix, as described in [Autoregressive Entity
                Retrieval](https://arxiv.org/abs/2010.00904).
            synced_gpus (`bool`, *optional*):
                Whether to continue running the while loop until max_length. Unless overridden, this flag will be set
                to `True` if using `FullyShardedDataParallel` or DeepSpeed ZeRO Stage 3 with multiple GPUs to avoid
                deadlocking if one GPU finishes generating before other GPUs. Otherwise, defaults to `False`.
            assistant_model (`PreTrainedModel`, *optional*):
                An assistant model that can be used to accelerate generation. The assistant model must have the exact
                same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistant model
                is much faster than running generation with the model you're calling generate from. As such, the
                assistant model should be much smaller.
            streamer (`BaseStreamer`, *optional*):
                Streamer object that will be used to stream the generated sequences. Generated tokens are passed
                through `streamer.put(token_ids)` and the streamer is responsible for any further processing.
            negative_prompt_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
                The negative prompt needed for some processors such as CFG. The batch size must match the input batch
                size. This is an experimental feature, subject to breaking API changes in future versions.
            negative_prompt_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
                Attention_mask for `negative_prompt_ids`.
            use_model_defaults (`bool`, *optional*):
                When it is `True`, unset parameters in `generation_config` will be set to the model-specific default
                generation configuration (`model.generation_config`), as opposed to the global defaults
                (`GenerationConfig()`). If unset, models saved starting from `v4.50` will consider this flag to be
                `True`.
            kwargs (`Dict[str, Any]`, *optional*):
                Ad hoc parametrization of `generation_config` and/or additional model-specific kwargs that will be
                forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder
                specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.

        Return:
            [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`
            or when `config.return_dict_in_generate=True`) or a `torch.LongTensor`.

                If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible
                [`~utils.ModelOutput`] types are:

                    - [`~generation.GenerateDecoderOnlyOutput`],
                    - [`~generation.GenerateBeamDecoderOnlyOutput`]

                If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible
                [`~utils.ModelOutput`] types are:

                    - [`~generation.GenerateEncoderDecoderOutput`],
                    - [`~generation.GenerateBeamEncoderDecoderOutput`]
        """

        # 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call
        tokenizer = kwargs.pop("tokenizer", None)  # Pull this out first, we only use it for stopping criteria
        assistant_tokenizer = kwargs.pop("assistant_tokenizer", None)  # only used for assisted generation

        generation_config, model_kwargs = self._prepare_generation_config(
            generation_config, use_model_defaults, **kwargs
        ) # ToDo : ajouter la gestion des chiffres dans prepare
        self._validate_model_kwargs(model_kwargs.copy())
        self._validate_assistant(assistant_model, tokenizer, assistant_tokenizer)

        # 2. Set generation parameters if not already defined
        if synced_gpus is None:
            synced_gpus = (is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)) and dist.get_world_size() > 1

        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()
        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()

        accepts_attention_mask = "attention_mask" in set(inspect.signature(self.forward).parameters.keys())
        requires_attention_mask = "encoder_outputs" not in model_kwargs
        kwargs_has_attention_mask = model_kwargs.get("attention_mask", None) is not None

        # 3. Define model inputs
        inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(
            inputs, generation_config.bos_token_id, model_kwargs
        )
        batch_size = inputs_tensor.shape[0]

        device = inputs_tensor.device
        self._prepare_special_tokens(generation_config, kwargs_has_attention_mask, device=device)

        # decoder-only models must use left-padding for batched generation.
        if not self.config.is_encoder_decoder:
            # If `input_ids` was given, check if the last id in any sequence is `pad_token_id`
            # Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.
            if (
                generation_config._pad_token_tensor is not None
                and batch_size > 1
                and len(inputs_tensor.shape) == 2
                and torch.sum(inputs_tensor[:, -1] == generation_config._pad_token_tensor) > 0
            ):
                logger.warning(
                    "A decoder-only architecture is being used, but right-padding was detected! For correct "
                    "generation results, please set `padding_side='left'` when initializing the tokenizer."
                )

        # 4. Define other model kwargs
        # decoder-only models with inputs_embeds forwarding must use caching (otherwise we can't detect whether we are
        # generating the first new token or not, and we only want to use the embeddings for the first new token)
        if not self.config.is_encoder_decoder and model_input_name == "inputs_embeds":
            generation_config.use_cache = True

        if not kwargs_has_attention_mask and requires_attention_mask and accepts_attention_mask:
            model_kwargs["attention_mask"] = self._prepare_attention_mask_for_generation(
                inputs_tensor, generation_config, model_kwargs
            )
        elif kwargs_has_attention_mask:
            # TODO (joao): generalize this check with other types of inputs
            if model_input_name == "input_ids" and len(model_kwargs["attention_mask"].shape) > 2:
                raise ValueError("`attention_mask` passed to `generate` must be 2D.")

        if self.config.is_encoder_decoder and "encoder_outputs" not in model_kwargs:
            # if model is encoder decoder encoder_outputs are created and added to `model_kwargs`
            model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
                inputs_tensor, model_kwargs, model_input_name, generation_config
            )

        # 5. Prepare `input_ids` which will be used for auto-regressive generation
        if self.config.is_encoder_decoder:
            input_ids, model_kwargs = self._prepare_decoder_input_ids_for_generation(
                batch_size=batch_size,
                model_input_name=model_input_name,
                model_kwargs=model_kwargs,
                decoder_start_token_id=generation_config._decoder_start_token_tensor,
                device=inputs_tensor.device,
            )
        else:
            input_ids = inputs_tensor if model_input_name == "input_ids" else model_kwargs.pop("input_ids")
            number_ids = model_kwargs.pop("number_ids", None)
            

        if generation_config.token_healing:
            input_ids = self.heal_tokens(input_ids, tokenizer)

        if streamer is not None:
            streamer.put(input_ids.cpu())

        # 6. Prepare `max_length` depending on other stopping criteria.
        input_ids_length = input_ids.shape[1]
        has_default_max_length = kwargs.get("max_length") is None and generation_config.max_length is not None
        has_default_min_length = kwargs.get("min_length") is None and generation_config.min_length is not None
        generation_config = self._prepare_generated_length(
            generation_config=generation_config,
            has_default_max_length=has_default_max_length,
            has_default_min_length=has_default_min_length,
            model_input_name=model_input_name,
            inputs_tensor=inputs_tensor,
            input_ids_length=input_ids_length,
        )

        # If the model supports `logits_to_keep` in forward(), set it to 1 to avoid computing the whole
        # logit matrix. This can save a lot of memory during the first forward pass. Note that assisted decoding
        # dynamically overrides this value as it can need more than the last token logits
        if self._supports_logits_to_keep() and "logits_to_keep" not in model_kwargs:
            model_kwargs["logits_to_keep"] = 1

        self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)

        # 7. Prepare the cache.
        # - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.
        # - different models have a different cache name expected by the model (default = "past_key_values")
        # - `max_length`, prepared above, is used to determine the maximum cache length
        max_cache_length = generation_config.max_length - 1
        if (
            inputs_tensor.shape[1] != input_ids_length
            and model_input_name == "inputs_embeds"
            and not self.config.is_encoder_decoder
        ):
            max_cache_length += inputs_tensor.shape[1]
        self._prepare_cache_for_generation(
            generation_config, model_kwargs, assistant_model, batch_size, max_cache_length, device
        )

        # 8. determine generation mode
        generation_mode = generation_config.get_generation_mode(assistant_model)

        if streamer is not None and (generation_config.num_beams > 1):
            raise ValueError(
                "`streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1."
            )

        if self.device.type != input_ids.device.type:
            warnings.warn(
                "You are calling .generate() with the `input_ids` being on a device type different"
                f" than your model's device. `input_ids` is on {input_ids.device.type}, whereas the model"
                f" is on {self.device.type}. You may experience unexpected behaviors or slower generation."
                " Please make sure that you have put `input_ids` to the"
                f" correct device by calling for example input_ids = input_ids.to('{self.device.type}') before"
                " running `.generate()`.",
                UserWarning,
            )

        # 9. prepare logits processors and stopping criteria
        prepared_logits_processor = self._get_logits_processor(
            generation_config=generation_config,
            input_ids_seq_length=input_ids_length,
            encoder_input_ids=inputs_tensor,
            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,
            logits_processor=logits_processor,
            device=inputs_tensor.device,
            model_kwargs=model_kwargs,
            negative_prompt_ids=negative_prompt_ids,
            negative_prompt_attention_mask=negative_prompt_attention_mask,
        )
        prepared_stopping_criteria = self._get_stopping_criteria(
            generation_config=generation_config, stopping_criteria=stopping_criteria, tokenizer=tokenizer, **kwargs
        )

        # Set model_kwargs `use_cache` so we can use it later in forward runs
        model_kwargs["use_cache"] = generation_config.use_cache

        # 10. go into different generation modes

        if generation_mode in (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):
            # 11. expand input_ids with `num_return_sequences` additional sequences per batch
            input_ids, model_kwargs = self._expand_inputs_for_generation(
                input_ids=input_ids,
                expand_size=generation_config.num_return_sequences,
                is_encoder_decoder=self.config.is_encoder_decoder,
                **model_kwargs,
            )

            # 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)
            result = self._sample(
                input_ids,
                number_ids,
                logits_processor=prepared_logits_processor,
                stopping_criteria=prepared_stopping_criteria,
                generation_config=generation_config,
                synced_gpus=synced_gpus,
                streamer=streamer,
                **model_kwargs,
            )
        # Convert to legacy cache format if requested
        if (
            generation_config.return_legacy_cache is True
            and hasattr(result, "past_key_values")
            and getattr(result.past_key_values, "to_legacy_cache") is not None
        ):
            result.past_key_values = result.past_key_values.to_legacy_cache()
        return result

    def _sample(
        self,
        input_ids: torch.LongTensor,
        number_ids,
        logits_processor: LogitsProcessorList,
        stopping_criteria: StoppingCriteriaList,
        generation_config: GenerationConfig,
        synced_gpus: bool,
        streamer: Optional["BaseStreamer"],
        **model_kwargs,
    ) -> Union[GenerateNonBeamOutput, torch.LongTensor]:
        r"""
        Generates sequences of token ids for models with a language modeling head using **multinomial sampling** and
        can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.

        Parameters:
            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
                The sequence used as a prompt for the generation.
            logits_processor (`LogitsProcessorList`):
                An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]
                used to modify the prediction scores of the language modeling head applied at each generation step.
            stopping_criteria (`StoppingCriteriaList`):
                An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]
                used to tell if the generation loop should stop.
            generation_config ([`~generation.GenerationConfig`]):
                The generation configuration to be used as parametrization of the decoding method.
            synced_gpus (`bool`):
                Whether to continue running the while loop until max_length (needed to avoid deadlocking with
                `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).
            streamer (`BaseStreamer`, *optional*):
                Streamer object that will be used to stream the generated sequences. Generated tokens are passed
                through `streamer.put(token_ids)` and the streamer is responsible for any further processing.
            model_kwargs:
                Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is
                an encoder-decoder model the kwargs should include `encoder_outputs`.

        Return:
            [`~generation.GenerateDecoderOnlyOutput`], [`~generation.GenerateEncoderDecoderOutput`] or `torch.LongTensor`:
            A `torch.LongTensor` containing the generated tokens (default behaviour) or a
            [`~generation.GenerateDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and
            `return_dict_in_generate=True` or a [`~generation.GenerateEncoderDecoderOutput`] if
            `model.config.is_encoder_decoder=True`.
        """
        # init values
        print("new_sample", flush = True)
        pad_token_id = generation_config._pad_token_tensor
        output_attentions = generation_config.output_attentions
        output_hidden_states = generation_config.output_hidden_states
        output_scores = generation_config.output_scores
        output_logits = generation_config.output_logits
        return_dict_in_generate = generation_config.return_dict_in_generate
        has_eos_stopping_criteria = any(hasattr(criteria, "eos_token_id") for criteria in stopping_criteria)
        do_sample = generation_config.do_sample

        # init attention / hidden states / scores tuples
        scores = () if (return_dict_in_generate and output_scores) else None
        raw_logits = () if (return_dict_in_generate and output_logits) else None
        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None
        cross_attentions = () if (return_dict_in_generate and output_attentions) else None
        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None

        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states
        if return_dict_in_generate and self.config.is_encoder_decoder:
            encoder_attentions = model_kwargs["encoder_outputs"].get("attentions") if output_attentions else None
            encoder_hidden_states = (
                model_kwargs["encoder_outputs"].get("hidden_states") if output_hidden_states else None
            )

        # keep track of which sequences are already finished
        batch_size, cur_len = input_ids.shape[:2]
        this_peer_finished = False
        unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)
        model_kwargs = self._get_initial_cache_position(cur_len, input_ids.device, model_kwargs)

        model_forward = self.__call__
        compile_forward = self._valid_auto_compile_criteria(model_kwargs, generation_config)
        if compile_forward:
            os.environ["TOKENIZERS_PARALLELISM"] = "0"
            model_forward = self.get_compiled_call(generation_config.compile_config)

        if generation_config.prefill_chunk_size is not None:
            model_kwargs = self._prefill_chunking(input_ids, generation_config, **model_kwargs)
            is_prefill = False
        else:
            is_prefill = True

        while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):
            # prepare model inputs
            model_inputs = self.prepare_inputs_for_generation(input_ids, number_ids, **model_kwargs)
            # print("model input :",model_inputs,"\n","-"*40,"\nmodel_kwargs :",model_kwargs, "\n","-"*40,"\ninput ids :", input_ids, flush = True)

            # prepare variable output controls (note: some models won't accept all output controls)
            model_inputs.update({"output_attentions": output_attentions} if output_attentions else {})
            model_inputs.update({"output_hidden_states": output_hidden_states} if output_hidden_states else {})

            if is_prefill:
                outputs = self(**model_inputs, return_dict=True)
                is_prefill = False
            else:
                outputs = model_forward(**model_inputs, return_dict=True)

            # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping
            model_kwargs = self._update_model_kwargs_for_generation(
                outputs,
                model_kwargs,
                is_encoder_decoder=self.config.is_encoder_decoder,
            )
            if synced_gpus and this_peer_finished:
                continue

            # Copy is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration
            # (the clone itself is always small)
            next_token_logits = outputs.logits[:, -1, :].to(copy=True, dtype=torch.float32, device=input_ids.device)
            next_number_logits = outputs.logits_number[:, -1, :].to(copy=True, dtype=torch.float32, device=input_ids.device)

            # pre-process distribution
            next_token_scores = logits_processor(input_ids, next_token_logits)

            # Store scores, attentions and hidden_states when required
            if return_dict_in_generate:
                if output_scores:
                    scores += (next_token_scores,)
                if output_logits:
                    raw_logits += (next_token_logits,)
                if output_attentions:
                    decoder_attentions += (
                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)
                    )
                    if self.config.is_encoder_decoder:
                        cross_attentions += (outputs.cross_attentions,)

                if output_hidden_states:
                    decoder_hidden_states += (
                        (outputs.decoder_hidden_states,)
                        if self.config.is_encoder_decoder
                        else (outputs.hidden_states,)
                    )

            # token selection
            if do_sample:
                probs = nn.functional.softmax(next_token_scores, dim=-1)
                # TODO (joao): this OP throws "skipping cudagraphs due to ['incompatible ops']", find solution
                next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
            else:
                next_tokens = torch.argmax(next_token_scores, dim=-1)
            # next_tokens = torch.where(next_tokens == self.config.number_token_id, next_number_logits, next_tokens) #ToDo fournir au modele les chiffres et le texte separement
            # finished sentences should have their next token be a padding token
            if has_eos_stopping_criteria:
                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)

            # update generated ids, model inputs, and length for next step
            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)
            if next_tokens == self.config.number_token_id:
                if number_ids is not None:
                    number_ids = torch.cat([number_ids, next_number_logits[:, None]], dim=1)
                else:
                    number_ids = next_number_logits[:,None]
                print(number_ids.shape, flush = True)
                    
            if streamer is not None:
                streamer.put(next_tokens.cpu())

            unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)
            this_peer_finished = unfinished_sequences.max() == 0
            cur_len += 1

            # This is needed to properly delete outputs.logits which may be very large for first iteration
            # Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration
            del outputs

        if streamer is not None:
            streamer.end()

        if return_dict_in_generate:
            if self.config.is_encoder_decoder:
                return GenerateEncoderDecoderOutput(
                    sequences=input_ids,
                    scores=scores,
                    logits=raw_logits,
                    encoder_attentions=encoder_attentions,
                    encoder_hidden_states=encoder_hidden_states,
                    decoder_attentions=decoder_attentions,
                    cross_attentions=cross_attentions,
                    decoder_hidden_states=decoder_hidden_states,
                    past_key_values=model_kwargs.get("past_key_values"),
                )
            else:
                return GenerateDecoderOnlyOutput(
                    sequences=input_ids,
                    scores=scores,
                    logits=raw_logits,
                    attentions=decoder_attentions,
                    hidden_states=decoder_hidden_states,
                    past_key_values=model_kwargs.get("past_key_values"),
                )
        else:
            return input_ids, number_ids



import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import torchvision.ops as ops

class KLDLossGaussian(nn.Module):
    """
    Calcule la Kullback-Leibler Divergence (KLD) entre deux distributions Gaussiennes 2D.
    UtilisÃ©e pour la rÃ©gression des boÃ®tes englobantes orientÃ©es (OBB).
    """

    def __init__(self, eps: float = 1e-7):
        super().__init__()
        self.eps = eps

    def forward(self, mu1: torch.Tensor, sigma1: torch.Tensor, mu2: torch.Tensor, sigma2: torch.Tensor) -> torch.Tensor:
        """
        Args:
            mu1 (torch.Tensor): Moyennes des Gaussiennes prÃ©dites (N, 2) [cx, cy]
            sigma1 (torch.Tensor): Matrices de covariance des Gaussiennes prÃ©dites (N, 2, 2)
            mu2 (torch.Tensor): Moyennes des Gaussiennes cibles (N, 2) [cx, cy]
            sigma2 (torch.Tensor): Matrices de covariance des Gaussiennes cibles (N, 2, 2)

        Returns:
            torch.Tensor: KLD Loss moyenne sur le lot.
        """
        # Assurer que les matrices de covariance sont inversibles et ont des dÃ©terminants positifs
        sigma1 = sigma1 + self.eps * torch.eye(2, device=sigma1.device).unsqueeze(0)
        sigma2 = sigma2 + self.eps * torch.eye(2, device=sigma2.device).unsqueeze(0)

        # Calcul des inverses et dÃ©terminants
        inv_sigma2 = torch.linalg.inv(sigma2)
        det_sigma1 = torch.linalg.det(sigma1)
        det_sigma2 = torch.linalg.det(sigma2)

        # Terme de trace
        trace_term = torch.diagonal(torch.matmul(inv_sigma2, sigma1), dim1=-2, dim2=-1).sum(-1)

        # Terme de diffÃ©rence des moyennes
        mu_diff = mu2 - mu1  # (N, 2)
        mu_diff_term = torch.matmul(mu_diff.unsqueeze(1), torch.matmul(inv_sigma2, mu_diff.unsqueeze(2))).squeeze(-1).squeeze(-1)

        # Terme de dÃ©terminant
        det_term = torch.log(det_sigma2 / (det_sigma1 + self.eps))

        # KLD Loss
        kld_loss_per_box = 0.5 * (trace_term + mu_diff_term - 2 + det_term)

        return torch.relu(kld_loss_per_box).mean()


class EnhancedKLDHybridLocalizationLoss(nn.Module):
    """
    Loss hybride avancÃ©e pour la localisation avec choix flexible du type de loss :
    - Choix entre KLD et diffÃ©rents types d'IoU (IoU, GIoU, DIoU, CIoU, SIoU)
    - Interface simplifiÃ©e : get_loss_components() retourne toujours 'iou_loss' et 'kld_loss'
    - Si loss principale = KLD : 'kld_loss' = loss, 'iou_loss' = mÃ©trique
    - Si loss principale = IoU : 'iou_loss' = loss, 'kld_loss' = mÃ©trique
    """

    def __init__(self,
                 loss_type: str = 'kld',  # 'kld', 'iou', 'giou', 'diou', 'ciou', 'siou'
                 l1_weight: float = 1.0,
                 main_loss_weight: float = 5.0,
                 coord_format: str = 'cxcywha',
                 angle_normalization: str = 'symmetric_pi',
                 eps: float = 1e-7,
                 coord_max: float = 1.0):
        super().__init__()

        self.loss_type = loss_type
        self.l1_weight = l1_weight
        self.main_loss_weight = main_loss_weight
        self.coord_format = coord_format
        self.angle_normalization = angle_normalization
        self.eps = eps
        self.coord_max = coord_max

        # Validation du type de loss
        valid_types = ['kld', 'iou', 'giou', 'diou', 'ciou', 'siou']
        if loss_type not in valid_types:
            raise ValueError(f"Type de loss '{loss_type}' non supportÃ©. Utilisez: {valid_types}")

        # Calculateurs de loss
        self.kld_calculator = KLDLossGaussian(eps=eps)
        
        # Storage pour monitoring - interface simplifiÃ©e
        self._last_l1_loss = None
        self._last_iou_loss = None  # Toujours prÃ©sent (loss ou mÃ©trique)
        self._last_kld_loss = None  # Toujours prÃ©sent (loss ou mÃ©trique)
        self._last_weighted_loss = None

    def forward(self, logits: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        device = logits.device

        # PrÃ©paration des donnÃ©es
        pred_boxes = logits[:, :-1]
        target_boxes = targets[:, 1:]
        valid_mask = mask[:, 1:]

        if valid_mask.dim() == 3:
            valid_mask = valid_mask.squeeze(-1)

        self._validate_shapes(pred_boxes, target_boxes, valid_mask)

        mask_sum = torch.sum(valid_mask)
        if mask_sum == 0:
            zero_loss = torch.tensor(0.0, device=device, requires_grad=True)
            self._update_monitoring_values(zero_loss, zero_loss, zero_loss, zero_loss)
            return zero_loss

        # Filtrage des Ã©lÃ©ments valides
        valid_indices = torch.nonzero(valid_mask.flatten(), as_tuple=False).squeeze(1)
        flat_pred = pred_boxes.reshape(-1, pred_boxes.shape[-1])
        flat_target = target_boxes.reshape(-1, target_boxes.shape[-1])
        valid_pred_boxes = flat_pred[valid_indices]
        valid_target_boxes = flat_target[valid_indices]

        # Calcul de la Smooth L1 Loss
        l1_loss = F.smooth_l1_loss(valid_pred_boxes, valid_target_boxes, reduction="mean")

        # PrÃ©paration des boÃ®tes pour les calculs gÃ©omÃ©triques
        valid_pred_boxes_processed = self._prepare_boxes_for_loss(valid_pred_boxes)
        valid_target_boxes_processed = self._prepare_boxes_for_loss(valid_target_boxes)

        # Calcul des deux types de loss/mÃ©triques
        kld_value = self._compute_kld_loss(valid_pred_boxes_processed, valid_target_boxes_processed)
        iou_value = self._compute_iou_metric(valid_pred_boxes_processed, valid_target_boxes_processed)
        
        # Si on utilise un type d'IoU comme loss principale, on recalcule avec gradients
        if self.loss_type != 'kld':
            iou_value = self._compute_iou_loss(valid_pred_boxes_processed, valid_target_boxes_processed, self.loss_type)

        # DÃ©termination de la loss principale
        if self.loss_type == 'kld':
            main_loss = kld_value
        else:
            main_loss = iou_value

        # Calcul de la perte totale pondÃ©rÃ©e
        weighted_loss = self.l1_weight * l1_loss + self.main_loss_weight * main_loss

        # Mise Ã  jour des valeurs de monitoring avec interface simplifiÃ©e
        self._update_monitoring_values(l1_loss, iou_value, kld_value, weighted_loss)

        return weighted_loss

    def _compute_kld_loss(self, pred_boxes: torch.Tensor, target_boxes: torch.Tensor) -> torch.Tensor:
        """Calcule la KLD loss entre les boÃ®tes prÃ©dites et cibles."""
        device = pred_boxes.device
        try:
            pred_mu, pred_sigma = self._obb_to_gaussian_params(pred_boxes)
            target_mu, target_sigma = self._obb_to_gaussian_params(target_boxes)
            return self.kld_calculator(pred_mu, pred_sigma, target_mu, target_sigma)
        except Exception as e:
            print(f"Warning: KLD loss computation failed: {e}")
            return torch.tensor(100.0, device=device, requires_grad=True)

    def _compute_iou_loss(self, pred_boxes: torch.Tensor, target_boxes: torch.Tensor, iou_type: str) -> torch.Tensor:
        """Calcule la loss IoU du type spÃ©cifiÃ©."""
        device = pred_boxes.device
        try:
            if iou_type == 'iou':
                iou = self.oriented_iou(pred_boxes, target_boxes)
                loss = 1.0 - iou
            elif iou_type == 'giou':
                iou, giou = self.oriented_giou(pred_boxes, target_boxes)
                loss = 1.0 - giou
            elif iou_type == 'diou':
                iou, diou = self.oriented_diou(pred_boxes, target_boxes)
                loss = 1.0 - diou
            elif iou_type == 'ciou':
                iou, ciou = self.oriented_ciou(pred_boxes, target_boxes)
                loss = 1.0 - ciou
            elif iou_type == 'siou':
                iou, siou = self.oriented_siou(pred_boxes, target_boxes)
                loss = 1.0 - siou
            else:
                raise ValueError(f"Type IoU '{iou_type}' non supportÃ©")
            
            return loss.mean()
        except Exception as e:
            print(f"Warning: {iou_type.upper()} loss computation failed: {e}")
            return torch.tensor(1.0, device=device, requires_grad=True)

    def _compute_iou_metric(self, pred_boxes: torch.Tensor, target_boxes: torch.Tensor) -> torch.Tensor:
        """Calcule la mÃ©trique IoU (peut Ãªtre avec ou sans gradients selon le contexte)."""
        device = pred_boxes.device
        
        # Si KLD est la loss principale, calculer IoU comme mÃ©trique (sans gradients)
        if self.loss_type == 'kld':
            with torch.no_grad():
                try:
                    pred_xyxy = self._convert_cxcywha_to_xyxy(pred_boxes)
                    target_xyxy = self._convert_cxcywha_to_xyxy(target_boxes)
                    
                    iou_matrix = ops.box_iou(pred_xyxy, target_xyxy)
                    if iou_matrix.numel() > 0:
                        return 1.0 - torch.diag(iou_matrix).mean()  # Retourne 1-IoU pour cohÃ©rence
                    else:
                        return torch.tensor(1.0, device=device)
                except Exception as e:
                    print(f"Warning: IoU metric computation failed: {e}")
                    return torch.tensor(1.0, device=device)
        else:
            # Si IoU est la loss principale, elle sera calculÃ©e avec gradients dans _compute_iou_loss
            # On retourne quand mÃªme une mÃ©trique IoU basique pour cohÃ©rence
            with torch.no_grad():
                try:
                    iou = self.oriented_iou(pred_boxes, target_boxes)
                    return 1.0 - iou.mean()
                except Exception as e:
                    print(f"Warning: IoU metric computation failed: {e}")
                    return torch.tensor(1.0, device=device)

    # ===== MÃ‰THODES IoU ORIENTÃ‰ES (depuis OrientedIoULoss) =====

    def oriented_iou(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> torch.Tensor:
        """Version optimisÃ©e du calcul IoU orientÃ©e"""
        poly1 = self.obb_to_polygon(boxes1)
        poly2 = self.obb_to_polygon(boxes2)
        
        area1 = boxes1[:, 2] * boxes1[:, 3]
        area2 = boxes2[:, 2] * boxes2[:, 3]
        
        intersection = self.compute_polygon_intersection_area(poly1, poly2)
        union = area1 + area2 - intersection
        
        iou = intersection / (union + self.eps)
        return torch.clamp(iou, 0.0, 1.0)

    def oriented_giou(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> tuple:
        """Calcule le Generalized IoU (GIoU) orientÃ©"""
        iou = self.oriented_iou(boxes1, boxes2)
        
        poly1 = self.obb_to_polygon(boxes1)
        poly2 = self.obb_to_polygon(boxes2)
        
        area1 = boxes1[:, 2] * boxes1[:, 3]
        area2 = boxes2[:, 2] * boxes2[:, 3]
        
        min_xy = torch.min(torch.min(poly1, dim=1)[0], torch.min(poly2, dim=1)[0])
        max_xy = torch.max(torch.max(poly1, dim=1)[0], torch.max(poly2, dim=1)[0])
        
        enclosing_area = (max_xy[:, 0] - min_xy[:, 0]) * (max_xy[:, 1] - min_xy[:, 1])
        union = area1 + area2 - self.compute_polygon_intersection_area(poly1, poly2)
        
        giou = iou - (enclosing_area - union) / (enclosing_area + self.eps)
        return iou, torch.clamp(giou, min=-1.0, max=1.0)

    def oriented_diou(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> tuple:
        """Calcule le Distance IoU (DIoU) orientÃ©"""
        iou = self.oriented_iou(boxes1, boxes2)
        
        centers1 = boxes1[:, :2]
        centers2 = boxes2[:, :2]
        d2 = torch.sum((centers1 - centers2) ** 2, dim=1)
        
        poly1 = self.obb_to_polygon(boxes1)
        poly2 = self.obb_to_polygon(boxes2)
        
        min_xy = torch.min(torch.min(poly1, dim=1)[0], torch.min(poly2, dim=1)[0])
        max_xy = torch.max(torch.max(poly1, dim=1)[0], torch.max(poly2, dim=1)[0])
        
        c2 = torch.sum((max_xy - min_xy) ** 2, dim=1)
        diou = iou - d2 / (c2 + self.eps)
        
        return iou, torch.clamp(diou, min=-1.0, max=1.0)

    def oriented_ciou(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> tuple:
        """Version corrigÃ©e du CIoU orientÃ© avec stabilitÃ© numÃ©rique"""
        iou = self.oriented_iou(boxes1, boxes2)
        
        centers1 = boxes1[:, :2]
        centers2 = boxes2[:, :2]
        d2 = torch.sum((centers1 - centers2) ** 2, dim=1)
        
        poly1 = self.obb_to_polygon(boxes1)
        poly2 = self.obb_to_polygon(boxes2)
        
        min_xy = torch.min(torch.min(poly1, dim=1)[0], torch.min(poly2, dim=1)[0])
        max_xy = torch.max(torch.max(poly1, dim=1)[0], torch.max(poly2, dim=1)[0])
        
        c2 = torch.sum((max_xy - min_xy) ** 2, dim=1)
        distance_term = torch.clamp(d2 / (c2 + self.eps), min=0.0, max=1.0)
        
        w1, h1, a1 = boxes1[:, 2], boxes1[:, 3], boxes1[:, 4]
        w2, h2, a2 = boxes2[:, 2], boxes2[:, 3], boxes2[:, 4]
        
        # Calcul amÃ©liorÃ© du terme d'aspect ratio
        a1_norm = torch.atan2(torch.sin(2*a1), torch.cos(2*a1)) / 2
        a2_norm = torch.atan2(torch.sin(2*a2), torch.cos(2*a2)) / 2
        
        ar1 = w1 / torch.clamp(h1, min=self.eps)
        ar2 = w2 / torch.clamp(h2, min=self.eps)
        
        angle_diff = torch.abs(a1_norm - a2_norm)
        should_flip = angle_diff > (math.pi / 4)
        ar1_adjusted = torch.where(should_flip, 1.0 / ar1, ar1)
        
        atan_diff = torch.atan(ar2) - torch.atan(ar1_adjusted)
        v = (4.0 / (math.pi ** 2)) * torch.pow(atan_diff, 2)
        v = torch.clamp(v, min=0.0, max=1.0)
        
        denominator = (1.0 - iou + v + self.eps)
        alpha = v / denominator
        alpha = torch.clamp(alpha, min=0.0, max=1.0)
        
        aspect_weight = torch.sigmoid(2 * (0.5 - iou))
        aspect_penalty = aspect_weight * alpha * v
        aspect_penalty = torch.clamp(aspect_penalty, min=0.0, max=0.5)
        
        ciou = iou - distance_term - aspect_penalty
        ciou = torch.clamp(ciou, min=-1.0, max=1.0)
        
        return iou, ciou

    def oriented_siou(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> tuple:
        """Calcule le Scale-Invariant IoU (SIoU) orientÃ©"""
        iou = self.oriented_iou(boxes1, boxes2)
        
        centers1 = boxes1[:, :2]
        centers2 = boxes2[:, :2]
        d2 = torch.sum((centers1 - centers2) ** 2, dim=1)
        
        poly1 = self.obb_to_polygon(boxes1)
        poly2 = self.obb_to_polygon(boxes2)
        
        min_xy = torch.min(torch.min(poly1, dim=1)[0], torch.min(poly2, dim=1)[0])
        max_xy = torch.max(torch.max(poly1, dim=1)[0], torch.max(poly2, dim=1)[0])
        
        c2 = torch.sum((max_xy - min_xy) ** 2, dim=1)
        distance_term = d2 / (c2 + self.eps)
        
        w1, h1, a1 = boxes1[:, 2], boxes1[:, 3], boxes1[:, 4]
        w2, h2, a2 = boxes2[:, 2], boxes2[:, 3], boxes2[:, 4]
        
        a1_normalized = torch.atan2(torch.sin(a1), torch.cos(a1))
        a2_normalized = torch.atan2(torch.sin(a2), torch.cos(a2))
        
        # Terme d'aspect ratio
        ar1 = w1 / (h1 + self.eps)
        ar2 = w2 / (h2 + self.eps)
        
        angle_diff = torch.abs(a1_normalized - a2_normalized)
        angle_adjustment = torch.abs(torch.sin(angle_diff))
        
        ar1_adjusted = ar1 * (1 - angle_adjustment) + (1 / ar1) * angle_adjustment
        v = (4.0 / (math.pi ** 2)) * torch.pow(torch.atan(ar2) - torch.atan(ar1_adjusted), 2)
        
        # Terme d'Ã©chelle
        area1 = w1 * h1
        area2 = w2 * h2
        scale_ratio = torch.maximum(area1 / (area2 + self.eps), area2 / (area1 + self.eps))
        theta = 2 * torch.atan(torch.sqrt(scale_ratio) - 1)
        scale_term = torch.pow(torch.sin(theta), 2)
        
        # Terme d'angle
        angle_term = torch.pow(torch.sin(angle_diff / 2), 2)
        
        # Combinaison des termes
        alpha = v / ((1 - iou) + v + self.eps)
        quality_weight = torch.clamp(1.0 - iou, min=0.2, max=1.0)
        
        aspect_weight = 0.8 * quality_weight
        scale_weight = 0.4 * quality_weight
        angle_weight = 0.2 * quality_weight
        
        siou = iou - distance_term - aspect_weight * alpha * v - scale_weight * scale_term - angle_weight * angle_term
        
        return iou, torch.clamp(siou, min=-1.0, max=1.0)

    def obb_to_polygon(self, boxes: torch.Tensor) -> torch.Tensor:
        """Conversion OBB -> polygone optimisÃ©e"""
        cx, cy, w, h, angle = boxes.unbind(-1)
        
        w_half, h_half = w / 2, h / 2
        cos_a, sin_a = torch.cos(angle), torch.sin(angle)
        
        corners_x = torch.stack([
            -w_half * cos_a + h_half * sin_a + cx,
            w_half * cos_a + h_half * sin_a + cx,
            w_half * cos_a - h_half * sin_a + cx,
            -w_half * cos_a - h_half * sin_a + cx
        ], dim=1)
        
        corners_y = torch.stack([
            -w_half * sin_a - h_half * cos_a + cy,
            w_half * sin_a - h_half * cos_a + cy,
            w_half * sin_a + h_half * cos_a + cy,
            -w_half * sin_a + h_half * cos_a + cy
        ], dim=1)
        
        return torch.stack([corners_x, corners_y], dim=-1)

    def compute_polygon_intersection_area(self, poly1: torch.Tensor, poly2: torch.Tensor) -> torch.Tensor:
        """Calcule l'aire d'intersection entre deux ensembles de polygones"""
        device = poly1.device
        batch_size = poly1.shape[0]
        areas = torch.zeros(batch_size, device=device)
        
        for i in range(batch_size):
            clipped_polygon = self.clip_polygon(poly1[i], poly2[i])
            if clipped_polygon.shape[0] > 2:
                areas[i] = self.compute_polygon_area(clipped_polygon)
        
        return areas

    def clip_polygon(self, subject_polygon: torch.Tensor, clip_polygon: torch.Tensor) -> torch.Tensor:
        """Algorithme de dÃ©coupage Sutherland-Hodgman"""
        device = subject_polygon.device
        output_list = subject_polygon
        
        for i in range(clip_polygon.shape[0]):
            clip_edge_start = clip_polygon[i]
            clip_edge_end = clip_polygon[(i + 1) % clip_polygon.shape[0]]
            
            input_list = output_list
            output_list = torch.zeros((0, 2), device=device)
            
            if input_list.shape[0] == 0:
                continue
                
            s = input_list[-1]
            for e in input_list:
                if self.is_inside(e, clip_edge_start, clip_edge_end):
                    if not self.is_inside(s, clip_edge_start, clip_edge_end):
                        intersection = self.compute_intersection(s, e, clip_edge_start, clip_edge_end)
                        output_list = torch.cat([output_list, intersection.unsqueeze(0)], dim=0)
                    
                    output_list = torch.cat([output_list, e.unsqueeze(0)], dim=0)
                elif self.is_inside(s, clip_edge_start, clip_edge_end):
                    intersection = self.compute_intersection(s, e, clip_edge_start, clip_edge_end)
                    output_list = torch.cat([output_list, intersection.unsqueeze(0)], dim=0)
                
                s = e
        
        return output_list

    def is_inside(self, point: torch.Tensor, edge_start: torch.Tensor, edge_end: torch.Tensor) -> bool:
        """DÃ©termine si un point est Ã  l'intÃ©rieur par rapport Ã  une arÃªte"""
        return ((edge_end[0] - edge_start[0]) * (point[1] - edge_start[1]) -
                (edge_end[1] - edge_start[1]) * (point[0] - edge_start[0])) > 0

    def compute_intersection(self, s: torch.Tensor, e: torch.Tensor, 
                           edge_start: torch.Tensor, edge_end: torch.Tensor) -> torch.Tensor:
        """Calcule l'intersection entre deux segments"""
        dc = torch.stack([s - e, edge_start - edge_end], dim=0)
        n1 = s[0] * e[1] - s[1] * e[0]
        n2 = edge_start[0] * edge_end[1] - edge_start[1] * edge_end[0] 
        n3 = 1.0 / (dc[0, 0] * dc[1, 1] - dc[0, 1] * dc[1, 0] + self.eps)
        
        return torch.tensor([
            (n1 * dc[1, 0] - n2 * dc[0, 0]) * n3,
            (n1 * dc[1, 1] - n2 * dc[0, 1]) * n3
        ], device=s.device)

    def compute_polygon_area(self, polygon: torch.Tensor) -> torch.Tensor:
        """Calcule l'aire d'un polygone avec la formule de Shoelace"""
        x = polygon[:, 0]
        y = polygon[:, 1]
        
        return torch.abs(
            torch.sum(x * torch.roll(y, -1)) - torch.sum(y * torch.roll(x, -1))
        ) * 0.5

    # ===== MÃ‰THODES DE SUPPORT =====

    def _update_monitoring_values(self, l1_loss: torch.Tensor, iou_loss: torch.Tensor,
                                  kld_loss: torch.Tensor, weighted_loss: torch.Tensor):
        """Met Ã  jour les attributs de monitoring avec interface simplifiÃ©e"""
        self._last_l1_loss = l1_loss.detach()
        self._last_iou_loss = iou_loss.detach()
        self._last_kld_loss = kld_loss.detach()
        self._last_weighted_loss = weighted_loss.detach()

    def _validate_shapes(self, pred_boxes: torch.Tensor, target_boxes: torch.Tensor, valid_mask: torch.Tensor):
        """Validation des dimensions des tenseurs"""
        assert pred_boxes.shape == target_boxes.shape, \
            f"Shape mismatch: {pred_boxes.shape} vs {target_boxes.shape}"
        assert pred_boxes.shape[:-1] == valid_mask.shape, \
            f"Mask shape mismatch: {pred_boxes.shape[:-1]} vs {valid_mask.shape}"

    def _prepare_boxes_for_loss(self, boxes: torch.Tensor) -> torch.Tensor:
        """PrÃ©paration et validation des boÃ®tes"""
        if boxes.dim() == 1:
            boxes = boxes.unsqueeze(0)

        processed_boxes = boxes.clone()

        if self.coord_format == 'xyxy':
            if processed_boxes.shape[-1] == 4:
                angles = torch.zeros(processed_boxes.shape[0], 1, device=processed_boxes.device, dtype=processed_boxes.dtype)
                processed_boxes = torch.cat([processed_boxes, angles], dim=-1)
            processed_boxes = self._convert_xyxy_to_cxcywha(processed_boxes)
        elif self.coord_format == 'cxcywha':
            if processed_boxes.shape[-1] == 4:
                angles = torch.zeros(processed_boxes.shape[0], 1, device=processed_boxes.device, dtype=processed_boxes.dtype)
                processed_boxes = torch.cat([processed_boxes, angles], dim=-1)
        else:
            raise ValueError(f"Format {self.coord_format} non supportÃ©")

        cx, cy, w, h, angles = processed_boxes.unbind(-1)

        cx_clamped = torch.clamp(cx, min=0.0, max=self.coord_max)
        cy_clamped = torch.clamp(cy, min=0.0, max=self.coord_max)
        w_fixed = torch.clamp(w, min=self.eps, max=self.coord_max)
        h_fixed = torch.clamp(h, min=self.eps, max=self.coord_max)

        if self.angle_normalization == 'symmetric_pi':
            angles_normalized = self._normalize_angle_symmetric_pi(angles)
        elif self.angle_normalization == 'positive_pi':
            angles_normalized = self._normalize_angle_positive_pi(angles)
        else:
            angles_normalized = angles

        return torch.stack([cx_clamped, cy_clamped, w_fixed, h_fixed, angles_normalized], dim=-1)

    def _convert_xyxy_to_cxcywha(self, boxes_xyxy_angle: torch.Tensor) -> torch.Tensor:
        """Conversion (xmin,ymin,xmax,ymax,angle) -> (cx,cy,w,h,angle)"""
        xmin, ymin, xmax, ymax, angle = boxes_xyxy_angle.unbind(-1)

        cx = (xmin + xmax) / 2
        cy = (ymin + ymax) / 2
        w = torch.abs(xmax - xmin)
        h = torch.abs(ymax - ymin)

        return torch.stack([cx, cy, w, h, angle], dim=1)

    def _convert_cxcywha_to_xyxy(self, boxes_cxcywha: torch.Tensor) -> torch.Tensor:
        """Conversion (cx,cy,w,h,angle) -> (xmin,ymin,xmax,ymax) pour IoU AABB"""
        cx, cy, w, h, _ = boxes_cxcywha.unbind(-1)
        xmin = cx - w / 2
        ymin = cy - h / 2
        xmax = cx + w / 2
        ymax = cy + h / 2
        return torch.stack([xmin, ymin, xmax, ymax], dim=-1)

    def _normalize_angle_symmetric_pi(self, angles: torch.Tensor) -> torch.Tensor:
        """Normalisation dans [-Ï€/2, Ï€/2]"""
        angles_mod = angles % (2 * math.pi)
        angles_wrapped = torch.where(angles_mod > math.pi, angles_mod - 2 * math.pi, angles_mod)
        angles_sym1 = torch.where(angles_wrapped > math.pi / 2, angles_wrapped - math.pi, angles_wrapped)
        angles_sym2 = torch.where(angles_sym1 < -math.pi / 2, angles_sym1 + math.pi, angles_sym1)
        return angles_sym2

    def _normalize_angle_positive_pi(self, angles: torch.Tensor) -> torch.Tensor:
        """Normalisation dans [0, Ï€]"""
        return angles % math.pi

    def _obb_to_gaussian_params(self, boxes: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        """Convertit les paramÃ¨tres d'OBB en paramÃ¨tres de Gaussienne 2D"""
        cx, cy, w, h, angle = boxes.unbind(-1)

        mu = torch.stack([cx, cy], dim=-1)

        w_sq_4 = (w**2 / 4)
        h_sq_4 = (h**2 / 4)

        cos_a = torch.cos(angle)
        sin_a = torch.sin(angle)

        R = torch.stack([
            torch.stack([cos_a, -sin_a], dim=-1),
            torch.stack([sin_a, cos_a], dim=-1)
        ], dim=-2)

        S = torch.diag_embed(torch.stack([w_sq_4, h_sq_4], dim=-1))
        Sigma = torch.matmul(R, torch.matmul(S, R.transpose(-1, -2)))

        return mu, Sigma

    def get_loss_components(self):
        """
        Retourne les composants de perte pour le monitoring avec interface simplifiÃ©e.
        Toujours 'iou_loss' et 'kld_loss' peu importe le type de loss principale.
        """
        if self._last_l1_loss is None:
            device = next(self.parameters()).device if hasattr(self, 'parameters') and next(self.parameters(), None) is not None else torch.device('cpu')
            zero = torch.tensor(0.0, device=device)
            return {
                'l1_loss': zero, 
                'iou_loss': zero, 
                'kld_loss': zero, 
                'weighted_loss': zero
            }

        return {
            'l1_loss': self._last_l1_loss,
            'iou_loss': self._last_iou_loss,
            'kld_loss': self._last_kld_loss,
            'weighted_loss': self._last_weighted_loss
        }

    def get_loss_info(self):
        """Retourne les informations sur la configuration de la loss"""
        return {
            'loss_type': self.loss_type,
            'l1_weight': self.l1_weight,
            'main_loss_weight': self.main_loss_weight,
            'is_kld_main': self.loss_type == 'kld',
            'is_iou_main': self.loss_type != 'kld'
        }

# import math
# import torch
# import torch.nn as nn
# import torch.nn.functional as F


# class LocalisationLoss(nn.Module):
#     """
#     Loss combinÃ©e L1 + IoU orientÃ©e optimisÃ©e pour la dÃ©tection de boÃ®tes orientÃ©es.
#     Utilise un calcul prÃ©cis d'intersection de polygones pour les boÃ®tes orientÃ©es.
#     """
    
#     def __init__(self, 
#                  l1_weight: float = 1.0,
#                  iou_weight: float = 5.0,
#                  iou_type: str = 'ciou',  # 'iou', 'giou', 'diou', 'ciou'
#                  coord_format: str = 'cxcywha',  # 'xyxy', 'cxcywha'
#                  angle_normalization: str = 'symmetric_pi',
#                  eps: float = 1e-7):
#         super().__init__()
        
#         self.l1_weight = l1_weight
#         self.iou_weight = iou_weight
#         self.coord_format = coord_format
#         self.eps = eps
#         self.l1_loss = torch.tensor(0)
#         self.iou_loss = torch.tensor(0)
        
#         # Loss IoU orientÃ©e intÃ©grÃ©e
#         self.oriented_iou_loss = OrientedIoULoss(
#             loss_type=iou_type,
#             reduction='mean',
#             eps=eps
#         )
        
#         # Fonction de normalisation d'angle
#         self.angle_normalization = angle_normalization
    
#     def forward(self, logits: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
#         """
#         Args:
#             logits: (batch_size, seq_len, 4 ou 5) - prÃ©dictions des boÃ®tes
#             targets: (batch_size, seq_len, 4 ou 5) - boÃ®tes cibles  
#             mask: (batch_size, seq_len) - masque de validitÃ©
#         """
#         device = logits.device
        
#         # PrÃ©paration des donnÃ©es (dÃ©calage temporel)
#         pred_boxes = logits[:, :-1]  # (batch_size, seq_len-1, 4/5)
#         target_boxes = targets[:, 1:]  # (batch_size, seq_len-1, 4/5)
#         valid_mask = mask[:, 1:]  # (batch_size, seq_len-1)
        
#         # Normalisation des dimensions du masque
#         if valid_mask.dim() == 3:
#             valid_mask = valid_mask.squeeze(-1)
        
#         # Validation des dimensions
#         self._validate_shapes(pred_boxes, target_boxes, valid_mask)
        
#         # Calcul du nombre d'Ã©lÃ©ments valides
#         mask_sum = torch.sum(valid_mask)
#         if mask_sum == 0:
#             return torch.tensor(0.0, device=device, requires_grad=True)
        
#         # Calcul des losses
#         self.l1_loss = self._compute_l1_loss(pred_boxes, target_boxes, valid_mask, mask_sum)
#         self.iou_loss = self._compute_iou_loss(pred_boxes, target_boxes, valid_mask)
        
        
#         return self.l1_weight * self.l1_loss + self.iou_weight * self.iou_loss
    
#     def _validate_shapes(self, pred_boxes: torch.Tensor, target_boxes: torch.Tensor, valid_mask: torch.Tensor):
#         """Validation des dimensions des tenseurs"""
#         assert pred_boxes.shape == target_boxes.shape, \
#             f"Shape mismatch: {pred_boxes.shape} vs {target_boxes.shape}"
#         assert pred_boxes.shape[:-1] == valid_mask.shape, \
#             f"Mask shape mismatch: {pred_boxes.shape[:-1]} vs {valid_mask.shape}"
    
#     def _compute_l1_loss(self, pred_boxes: torch.Tensor, target_boxes: torch.Tensor, 
#                         valid_mask: torch.Tensor, mask_sum: torch.Tensor) -> torch.Tensor:
#         """Calcul optimisÃ© de la loss L1 avec masquage"""
#         # Masquage vectorisÃ©
#         mask_expanded = valid_mask.unsqueeze(-1)  # (batch_size, seq_len-1, 1)
        
#         # Calcul L1 seulement sur les Ã©lÃ©ments valides
#         per_element_loss = F.smooth_l1_loss(pred_boxes, target_boxes, reduction="none")  # (B, S, 4/5)
#         masked_loss = per_element_loss * mask_expanded
        
#         # Moyenne sur les Ã©lÃ©ments valides
#         num_coords = pred_boxes.shape[-1]  # 4 ou 5 coordonnÃ©es
#         return torch.sum(masked_loss) / (mask_sum * num_coords)
    
#     def _compute_iou_loss(self, pred_boxes: torch.Tensor, target_boxes: torch.Tensor, 
#                          valid_mask: torch.Tensor) -> torch.Tensor:
#         """Calcul optimisÃ© de la loss IoU orientÃ©e"""
#         device = pred_boxes.device
        
#         # Filtrage efficace des Ã©lÃ©ments valides
#         valid_indices = torch.nonzero(valid_mask.flatten(), as_tuple=False).squeeze(1)
        
#         if len(valid_indices) == 0:
#             return torch.tensor(1.0, device=device, requires_grad=True)
        
#         # Extraction des boÃ®tes valides (vectorisÃ©)
#         flat_pred = pred_boxes.reshape(-1, pred_boxes.shape[-1])
#         flat_target = target_boxes.reshape(-1, target_boxes.shape[-1])
        
#         valid_pred_boxes = flat_pred[valid_indices]
#         valid_target_boxes = flat_target[valid_indices]
        
#         # PrÃ©paration des boÃ®tes selon le format
#         valid_pred_boxes = self._prepare_boxes(valid_pred_boxes)
#         valid_target_boxes = self._prepare_boxes(valid_target_boxes)
        
#         # Calcul de la loss IoU orientÃ©e
#         try:
#             return self.oriented_iou_loss(valid_pred_boxes, valid_target_boxes)
#         except Exception as e:
#             print(f"Warning: Oriented IoU loss computation failed: {e}")
#             return torch.tensor(0.0, device=device, requires_grad=True)
    
#     def _prepare_boxes(self, boxes: torch.Tensor) -> torch.Tensor:
#         """PrÃ©paration et validation des boÃ®tes selon le format"""
#         if self.coord_format == 'xyxy':
#             # Format (xmin, ymin, xmax, ymax) ou (xmin, ymin, xmax, ymax, angle)
#             if boxes.shape[-1] == 4:
#                 # Pas d'angle fourni - on assume angle = 0
#                 angles = torch.zeros(boxes.shape[0], 1, device=boxes.device, dtype=boxes.dtype)
#                 boxes_with_angle = torch.cat([boxes, angles], dim=-1)
#             elif boxes.shape[-1] == 5:
#                 # Format (xmin, ymin, xmax, ymax, angle) - dÃ©jÃ  avec angle
#                 boxes_with_angle = boxes
#             else:
#                 raise ValueError(f"Format xyxy attendu avec 4 ou 5 dimensions, reÃ§u {boxes.shape[-1]}")
                
#             # Conversion vers format cxcywha
#             boxes_cxcywha = self._convert_xyxy_to_cxcywha(boxes_with_angle)
#             return self._ensure_valid_oriented_boxes(boxes_cxcywha)
            
#         elif self.coord_format == 'cxcywha':
#             # Format (cx, cy, w, h) ou (cx, cy, w, h, angle) - dÃ©jÃ  compatible
#             if boxes.shape[-1] == 4:
#                 # Pas d'angle fourni - on assume angle = 0
#                 angles = torch.zeros(boxes.shape[0], 1, device=boxes.device, dtype=boxes.dtype)
#                 boxes_with_angle = torch.cat([boxes, angles], dim=-1)
#             elif boxes.shape[-1] == 5:
#                 # Format (cx, cy, w, h, angle) - dÃ©jÃ  complet
#                 boxes_with_angle = boxes
#             else:
#                 raise ValueError(f"Format cxcywha attendu avec 4 ou 5 dimensions, reÃ§u {boxes.shape[-1]}")
                
#             return self._ensure_valid_oriented_boxes(boxes_with_angle)
#         else:
#             raise ValueError(f"Format {self.coord_format} non supportÃ©")

#     def _convert_xyxy_to_cxcywha(self, boxes: torch.Tensor) -> torch.Tensor:
#         """Conversion (xmin,ymin,xmax,ymax,angle) -> (cx,cy,w,h,angle)"""
#         xmin, ymin, xmax, ymax = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
#         angle = boxes[:, 4] if boxes.shape[-1] == 5 else torch.zeros_like(xmin)
        
#         cx = (xmin + xmax) / 2
#         cy = (ymin + ymax) / 2
#         w = torch.abs(xmax - xmin)
#         h = torch.abs(ymax - ymin)
        
#         return torch.stack([cx, cy, w, h, angle], dim=1)
    
#     def _ensure_valid_oriented_boxes(self, boxes: torch.Tensor) -> torch.Tensor:
#         """Validation et correction des boÃ®tes orientÃ©es - SANS opÃ©rations in-place"""
#         # Clamp des coordonnÃ©es (sauf l'angle) - crÃ©ation nouveau tenseur
#         coords_clamped = torch.clamp(boxes[:, :4], min=0.0)
        
#         # S'assurer que width et height sont positives - crÃ©ation nouveaux tenseurs
#         w_fixed = torch.maximum(coords_clamped[:, 2], torch.full_like(coords_clamped[:, 2], self.eps))
#         h_fixed = torch.maximum(coords_clamped[:, 3], torch.full_like(coords_clamped[:, 3], self.eps))
        
#         # Normalisation de l'angle - crÃ©ation nouveau tenseur
#         if self.angle_normalization == 'symmetric_pi':
#             angles_normalized = self._normalize_angle_symmetric_pi(boxes[:, 4])
#         elif self.angle_normalization == 'positive_pi':
#             angles_normalized = self._normalize_angle_positive_pi(boxes[:, 4])
#         else:
#             angles_normalized = boxes[:, 4]
        
#         # Reconstruction complÃ¨te du tenseur - AUCUNE opÃ©ration in-place
#         boxes_fixed = torch.stack([
#             coords_clamped[:, 0],  # cx
#             coords_clamped[:, 1],  # cy
#             w_fixed,               # w
#             h_fixed,               # h
#             angles_normalized      # angle
#         ], dim=1)
        
#         return boxes_fixed
    
#     def _normalize_angle_symmetric_pi(self, angles: torch.Tensor) -> torch.Tensor:
#         """Normalisation dans [-Ï€/2, Ï€/2] - SANS opÃ©rations in-place"""
#         # Normalisation de base dans [-Ï€, Ï€]
#         angles_mod = angles % (2 * math.pi)
#         angles_wrapped = torch.where(angles_mod > math.pi, angles_mod - 2 * math.pi, angles_mod)
        
#         # Normalisation dans [-Ï€/2, Ï€/2]
#         angles_sym1 = torch.where(angles_wrapped > math.pi / 2, angles_wrapped - math.pi, angles_wrapped)
#         angles_sym2 = torch.where(angles_sym1 < -math.pi / 2, angles_sym1 + math.pi, angles_sym1)
        
#         return angles_sym2
    
#     def _normalize_angle_positive_pi(self, angles: torch.Tensor) -> torch.Tensor:
#         """Normalisation dans [0, Ï€] - SANS opÃ©rations in-place"""
#         return angles % math.pi


# class OrientedIoULoss(nn.Module):
#     """
#     Calcule la loss IoU orientÃ©e avec support pour diffÃ©rents types d'IoU:
#     - IoU: Intersection over Union standard
#     - GIoU: Generalized IoU (pÃ©nalise les boÃ®tes Ã©loignÃ©es)
#     - DIoU: Distance IoU (considÃ¨re la distance entre centres)
#     - CIoU: Complete IoU (considÃ¨re distance, taille et orientation)
#     """
    
#     def __init__(self, loss_type: str = 'iou', reduction: str = 'mean', eps: float = 1e-7):
#         super().__init__()
#         self.loss_type = loss_type
#         self.reduction = reduction
#         self.eps = eps
        
#         if loss_type not in ['iou', 'giou', 'diou', 'ciou']:
#             raise ValueError(f"Type de loss '{loss_type}' non supportÃ©. "
#                            f"Utilisez 'iou', 'giou', 'diou' ou 'ciou'.")
    
#     def forward(self, pred_boxes: torch.Tensor, target_boxes: torch.Tensor) -> torch.Tensor:
#         """
#         Args:
#             pred_boxes: (N, 5) [cx, cy, w, h, angle]
#             target_boxes: (N, 5) [cx, cy, w, h, angle]
#         """
#         if self.loss_type == 'iou':
#             iou = self.oriented_iou(pred_boxes, target_boxes)
#             loss = 1.0 - iou
#         elif self.loss_type == 'giou':
#             iou, giou = self.oriented_giou(pred_boxes, target_boxes)
#             loss = 1.0 - giou
#         elif self.loss_type == 'diou':
#             iou, diou = self.oriented_diou(pred_boxes, target_boxes)
#             loss = 1.0 - diou
#         elif self.loss_type == 'ciou':
#             iou, ciou = self.oriented_ciou(pred_boxes, target_boxes)
#             loss = 1.0 - ciou
#         elif self.loss_type == 'siou':
#             iou, siou = self.oriented_siou(pred_boxes, target_boxes)
#             loss = 1.0 - siou
#         else:
#             raise ValueError(f"Type de loss '{self.loss_type}' non supportÃ©.")
        
#         if self.reduction == 'mean':
#             return loss.mean()
#         elif self.reduction == 'sum':
#             return loss.sum()
#         else:
#             return loss
    
#     def oriented_iou(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> torch.Tensor:
#         """Version optimisÃ©e du calcul IoU orientÃ©e avec CUDA"""
#         # Conversion en polygones
#         poly1 = self.obb_to_polygon(boxes1)
#         poly2 = self.obb_to_polygon(boxes2)
        
#         # Aires des boÃ®tes
#         area1 = boxes1[:, 2] * boxes1[:, 3]  # w * h
#         area2 = boxes2[:, 2] * boxes2[:, 3]  # w * h
        
#         # Calcul intersection prÃ©cis
#         intersection = self.compute_polygon_intersection_area(poly1, poly2)
        
#         # Union
#         union = area1 + area2 - intersection
        
#         # IoU avec stabilitÃ© numÃ©rique
#         iou = intersection / (union + self.eps)
#         return torch.clamp(iou, 0.0, 1.0)
    
#     def oriented_giou(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> tuple:
#         """
#         Calcule le Generalized IoU (GIoU) orientÃ© entre deux ensembles de boÃ®tes.
#         GIoU = IoU - |C\(AâˆªB)|/|C|, oÃ¹ C est la plus petite boÃ®te englobante.
        
#         Args:
#             boxes1: Tensor de forme (N, 5) [cx, cy, w, h, angle]
#             boxes2: Tensor de forme (N, 5) [cx, cy, w, h, angle]
            
#         Returns:
#             tuple (iou, giou): IoU et GIoU entre les boÃ®tes
#         """
#         # Calcule d'abord le IoU standard
#         iou = self.oriented_iou(boxes1, boxes2)
        
#         # Conversion en polygones
#         poly1 = self.obb_to_polygon(boxes1)
#         poly2 = self.obb_to_polygon(boxes2)
        
#         # Aires des boÃ®tes
#         area1 = boxes1[:, 2] * boxes1[:, 3]  # w * h
#         area2 = boxes2[:, 2] * boxes2[:, 3]  # w * h
        
#         # Calcul de la plus petite boÃ®te englobant les deux polygones
#         min_xy = torch.min(torch.min(poly1, dim=1)[0], torch.min(poly2, dim=1)[0])
#         max_xy = torch.max(torch.max(poly1, dim=1)[0], torch.max(poly2, dim=1)[0])
        
#         # Aire de la boÃ®te englobante
#         enclosing_area = (max_xy[:, 0] - min_xy[:, 0]) * (max_xy[:, 1] - min_xy[:, 1])
        
#         # Calcul de l'union
#         union = area1 + area2 - self.compute_polygon_intersection_area(poly1, poly2)
        
#         # Calcul du GIoU
#         giou = iou - (enclosing_area - union) / (enclosing_area + self.eps)
        
#         return iou, torch.clamp(giou, min=-1.0, max=1.0)
    
#     def oriented_diou(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> tuple:
#         """
#         Calcule le Distance IoU (DIoU) orientÃ© entre deux ensembles de boÃ®tes.
#         DIoU = IoU - dÂ²/cÂ², oÃ¹ d est la distance entre centres et c est la diagonale de la boÃ®te englobante.
        
#         Args:
#             boxes1: Tensor de forme (N, 5) [cx, cy, w, h, angle]
#             boxes2: Tensor de forme (N, 5) [cx, cy, w, h, angle]
            
#         Returns:
#             tuple (iou, diou): IoU et DIoU entre les boÃ®tes
#         """
#         # Calcule d'abord le IoU standard
#         iou = self.oriented_iou(boxes1, boxes2)
        
#         # Extraction des coordonnÃ©es des centres
#         centers1 = boxes1[:, :2]  # [cx, cy]
#         centers2 = boxes2[:, :2]  # [cx, cy]
        
#         # Distance euclidienne au carrÃ© entre les centres
#         d2 = torch.sum((centers1 - centers2) ** 2, dim=1)
        
#         # Conversion en polygones pour obtenir les points extrÃªmes
#         poly1 = self.obb_to_polygon(boxes1)
#         poly2 = self.obb_to_polygon(boxes2)
        
#         # Calcul de la plus petite boÃ®te englobant les deux polygones
#         min_xy = torch.min(torch.min(poly1, dim=1)[0], torch.min(poly2, dim=1)[0])
#         max_xy = torch.max(torch.max(poly1, dim=1)[0], torch.max(poly2, dim=1)[0])
        
#         # Diagonale au carrÃ© de la boÃ®te englobante
#         c2 = torch.sum((max_xy - min_xy) ** 2, dim=1)
        
#         # Calcul du DIoU
#         diou = iou - d2 / (c2 + self.eps)
        
#         return iou, torch.clamp(diou, min=-1.0, max=1.0)

#     def oriented_ciou(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> tuple:
#         """
#         Version corrigÃ©e du CIoU orientÃ© avec stabilitÃ© numÃ©rique amÃ©liorÃ©e
#         """
#         # Calcule d'abord le IoU standard
#         iou = self.oriented_iou(boxes1, boxes2)
        
#         # Extraction des coordonnÃ©es des centres
#         centers1 = boxes1[:, :2]  # [cx, cy]
#         centers2 = boxes2[:, :2]  # [cx, cy]
        
#         # Distance euclidienne au carrÃ© entre les centres
#         d2 = torch.sum((centers1 - centers2) ** 2, dim=1)
        
#         # Conversion en polygones pour obtenir les points extrÃªmes
#         poly1 = self.obb_to_polygon(boxes1)
#         poly2 = self.obb_to_polygon(boxes2)
        
#         # Calcul de la plus petite boÃ®te englobant les deux polygones
#         min_xy = torch.min(torch.min(poly1, dim=1)[0], torch.min(poly2, dim=1)[0])
#         max_xy = torch.max(torch.max(poly1, dim=1)[0], torch.max(poly2, dim=1)[0])
        
#         # Diagonale au carrÃ© de la boÃ®te englobante
#         c2 = torch.sum((max_xy - min_xy) ** 2, dim=1)
        
#         # Term de distance (DIoU) - clampÃ© pour Ã©viter les valeurs nÃ©gatives
#         distance_term = torch.clamp(d2 / (c2 + self.eps), min=0.0, max=1.0)
        
#         # Extraction des dimensions et angles
#         w1, h1, a1 = boxes1[:, 2], boxes1[:, 3], boxes1[:, 4]
#         w2, h2, a2 = boxes2[:, 2], boxes2[:, 3], boxes2[:, 4]
        
#         # === CALCUL AMÃ‰LIORÃ‰ DU TERME D'ASPECT RATIO ===
        
#         # Normalisation des angles entre -Ï€/2 et Ï€/2
#         a1_norm = torch.atan2(torch.sin(2*a1), torch.cos(2*a1)) / 2
#         a2_norm = torch.atan2(torch.sin(2*a2), torch.cos(2*a2)) / 2
        
#         # Calcul des ratios d'aspect avec protection contre division par zÃ©ro
#         ar1 = w1 / torch.clamp(h1, min=self.eps)
#         ar2 = w2 / torch.clamp(h2, min=self.eps)
        
#         # Gestion des orientations : si l'angle diffÃ¨re de ~90Â°, on inverse le ratio
#         angle_diff = torch.abs(a1_norm - a2_norm)
#         should_flip = angle_diff > (math.pi / 4)
#         ar1_adjusted = torch.where(should_flip, 1.0 / ar1, ar1)
        
#         # Calcul stable du terme v avec clipping
#         atan_diff = torch.atan(ar2) - torch.atan(ar1_adjusted)
#         v = (4.0 / (math.pi ** 2)) * torch.pow(atan_diff, 2)
#         v = torch.clamp(v, min=0.0, max=1.0)  # Limitation de v
        
#         # Calcul d'alpha avec dÃ©nominateur stabilisÃ©
#         denominator = (1.0 - iou + v + self.eps)
#         alpha = v / denominator
#         alpha = torch.clamp(alpha, min=0.0, max=1.0)  # Limitation d'alpha
        
#         # === PONDÃ‰RATION ADAPTATIVE ===
        
#         # RÃ©duction de l'impact du terme d'aspect quand IoU est dÃ©jÃ  bon
#         # Utilisation d'une fonction plus douce
#         aspect_weight = torch.sigmoid(2 * (0.5 - iou))  # [0, 1] avec transition douce
        
#         # Limitation globale du terme d'aspect
#         aspect_penalty = aspect_weight * alpha * v
#         aspect_penalty = torch.clamp(aspect_penalty, min=0.0, max=0.5)  # Max 0.5
        
#         # === CALCUL FINAL AVEC GARANTIES ===
        
#         # CIoU final avec clipping agressif
#         ciou = iou - distance_term - aspect_penalty
#         ciou = torch.clamp(ciou, min=-1.0, max=1.0)
        
#         # Debug optionnel
#         if torch.any(torch.isnan(ciou)) or torch.any(ciou < -1.1):
#             print(f"Warning: CIoU instable - IoU: {iou.mean():.3f}, "
#                   f"distance: {distance_term.mean():.3f}, "
#                   f"aspect: {aspect_penalty.mean():.3f}")
        
#         return iou, ciou

#     def oriented_siou(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> tuple:
#         """
#         Calcule le Scale-Invariant IoU (SIoU) orientÃ© entre deux ensembles de boÃ®tes.
#         SIoU Ã©tend CIoU en ajoutant un terme de consistance d'Ã©chelle.
        
#         Args:
#             boxes1: Tensor de forme (N, 5) [cx, cy, w, h, angle]
#             boxes2: Tensor de forme (N, 5) [cx, cy, w, h, angle]
            
#         Returns:
#             tuple (iou, siou): IoU et SIoU entre les boÃ®tes
#         """
#         # Calcule d'abord le IoU standard
#         iou = self.oriented_iou(boxes1, boxes2)
        
#         # Extraction des coordonnÃ©es des centres
#         centers1 = boxes1[:, :2]  # [cx, cy]
#         centers2 = boxes2[:, :2]  # [cx, cy]
        
#         # Distance euclidienne au carrÃ© entre les centres
#         d2 = torch.sum((centers1 - centers2) ** 2, dim=1)
        
#         # Conversion en polygones pour obtenir les points extrÃªmes
#         poly1 = self.obb_to_polygon(boxes1)
#         poly2 = self.obb_to_polygon(boxes2)
        
#         # Calcul de la plus petite boÃ®te englobant les deux polygones
#         min_xy = torch.min(torch.min(poly1, dim=1)[0], torch.min(poly2, dim=1)[0])
#         max_xy = torch.max(torch.max(poly1, dim=1)[0], torch.max(poly2, dim=1)[0])
        
#         # Diagonale au carrÃ© de la boÃ®te englobante
#         c2 = torch.sum((max_xy - min_xy) ** 2, dim=1)
        
#         # Term de distance (DIoU)
#         distance_term = d2 / (c2 + self.eps)
        
#         # Extraction des dimensions et angles
#         w1, h1, a1 = boxes1[:, 2], boxes1[:, 3], boxes1[:, 4]
#         w2, h2, a2 = boxes2[:, 2], boxes2[:, 3], boxes2[:, 4]
        
#         # Angles normalisÃ©s pour comparer des boÃ®tes similaires
#         a1_normalized = torch.atan2(torch.sin(a1), torch.cos(a1))
#         a2_normalized = torch.atan2(torch.sin(a2), torch.cos(a2))
        
#         # ---------- 1. Terme d'aspect ratio (comme dans CIoU) ----------
#         # Calcul des rapports d'aspect
#         ar1 = w1 / (h1 + self.eps)
#         ar2 = w2 / (h2 + self.eps)
        
#         # Correction d'angle: quand angle diffÃ¨re d'environ 90Â°, inverser le ratio
#         angle_diff = torch.abs(a1_normalized - a2_normalized)
#         angle_adjustment = torch.abs(torch.sin(angle_diff))
        
#         # Ajustement dynamique du ratio d'aspect
#         ar1_adjusted = ar1 * (1 - angle_adjustment) + (1 / ar1) * angle_adjustment
        
#         # Terme v: mesure de dissimilaritÃ© des ratios d'aspect
#         v = (4.0 / (math.pi ** 2)) * torch.pow(
#             torch.atan(ar2) - torch.atan(ar1_adjusted), 2)
        
#         # ---------- 2. Nouveau terme d'Ã©chelle ----------
#         # Calcul des surfaces des boÃ®tes
#         area1 = w1 * h1
#         area2 = w2 * h2
        
#         # Rapport d'Ã©chelle (toujours >= 1)
#         scale_ratio = torch.maximum(area1 / (area2 + self.eps), area2 / (area1 + self.eps))
        
#         # Terme de pÃ©nalitÃ© d'Ã©chelle (mapping sigmoid pour contrÃ´ler l'impact)
#         # Transformation logarithmique pour rÃ©duire l'effet des grandes diffÃ©rences
#         theta = 2 * torch.atan(torch.sqrt(scale_ratio) - 1)  # Transforme [1, âˆž) en [0, Ï€/2)
#         scale_term = torch.pow(torch.sin(theta), 2)  # [0, 1] avec croissance lente
        
#         # ---------- 3. Terme d'angle ----------
#         # PÃ©nalitÃ© pour diffÃ©rence d'angle (normalisÃ©e entre 0 et 1)
#         # Utilisation de sinÂ²(Î”Î¸/2) pour une mesure symÃ©trique et bornÃ©e
#         angle_term = torch.pow(torch.sin(angle_diff / 2), 2)
        
#         # ---------- 4. Combinaison des termes ----------
#         # Facteur alpha pour le terme d'aspect (comme dans CIoU)
#         alpha = v / ((1 - iou) + v + self.eps)
        
#         # Balance entre les termes avec pondÃ©ration adaptative
#         # RÃ©duit l'influence quand IoU est dÃ©jÃ  Ã©levÃ©
#         quality_weight = torch.clamp(1.0 - iou, min=0.2, max=1.0)
        
#         # Importance relative des termes: distance > aspect > Ã©chelle > angle
#         aspect_weight = 0.8 * quality_weight
#         scale_weight = 0.4 * quality_weight
#         angle_weight = 0.2 * quality_weight
        
#         # Calcul final du SIoU
#         siou = iou - distance_term - aspect_weight * alpha * v - scale_weight * scale_term - angle_weight * angle_term
        
#         return iou, torch.clamp(siou, min=-1.0, max=1.0)
    

    
#     def obb_to_polygon(self, boxes: torch.Tensor) -> torch.Tensor:
#         """
#         Conversion OBB -> polygone optimisÃ©e - SANS opÃ©rations in-place
        
#         Args:
#             boxes: Tensor de forme (N, 5) [cx, cy, w, h, angle]
            
#         Returns:
#             Tensor de forme (N, 4, 2) - polygones correspondants
#         """
#         cx, cy, w, h, angle = boxes.unbind(-1)
        
#         # Points locaux
#         w_half, h_half = w / 2, h / 2
        
#         # Calcul trigonomÃ©trique optimisÃ©
#         cos_a, sin_a = torch.cos(angle), torch.sin(angle)
        
#         # Matrice de rotation appliquÃ©e aux 4 coins - crÃ©ation nouveaux tenseurs
#         corners_x = torch.stack([
#             -w_half * cos_a + h_half * sin_a + cx,
#             w_half * cos_a + h_half * sin_a + cx,
#             w_half * cos_a - h_half * sin_a + cx,
#             -w_half * cos_a - h_half * sin_a + cx
#         ], dim=1)
        
#         corners_y = torch.stack([
#             -w_half * sin_a - h_half * cos_a + cy,
#             w_half * sin_a - h_half * cos_a + cy,
#             w_half * sin_a + h_half * cos_a + cy,
#             -w_half * sin_a + h_half * cos_a + cy
#         ], dim=1)
        
#         return torch.stack([corners_x, corners_y], dim=-1)  # (N, 4, 2)
    
#     def compute_polygon_intersection_area(self, poly1: torch.Tensor, poly2: torch.Tensor) -> torch.Tensor:
#         """
#         Calcule l'aire d'intersection entre deux ensembles de polygones avec CUDA.
        
#         Args:
#             poly1: Tensor de forme (N, 4, 2) - polygones des boÃ®tes prÃ©dites
#             poly2: Tensor de forme (N, 4, 2) - polygones des boÃ®tes cibles
            
#         Returns:
#             Tensor de forme (N,) - aires d'intersection
#         """
#         device = poly1.device
#         batch_size = poly1.shape[0]
        
#         # PrÃ©allocation des rÃ©sultats
#         areas = torch.zeros(batch_size, device=device)
        
#         # Traitement par lot pour optimisation CUDA
#         for i in range(batch_size):
#             # DÃ©coupage du polygone avec l'algorithme de Sutherland-Hodgman
#             clipped_polygon = self.clip_polygon(poly1[i], poly2[i])
            
#             # Calcul de l'aire si un polygone d'intersection existe
#             if clipped_polygon.shape[0] > 2:  # Au moins 3 points forment un polygone
#                 areas[i] = self.compute_polygon_area(clipped_polygon)
        
#         return areas
    
#     def clip_polygon(self, subject_polygon: torch.Tensor, clip_polygon: torch.Tensor) -> torch.Tensor:
#         """
#         ImplÃ©mentation PyTorch/CUDA de l'algorithme de dÃ©coupage Sutherland-Hodgman.
        
#         Args:
#             subject_polygon: Tensor de forme (4, 2) - polygone Ã  dÃ©couper
#             clip_polygon: Tensor de forme (4, 2) - polygone de dÃ©coupage
            
#         Returns:
#             Tensor - polygone rÃ©sultant du dÃ©coupage
#         """
#         device = subject_polygon.device
#         output_list = subject_polygon
        
#         # Pour chaque arÃªte du polygone de dÃ©coupage
#         for i in range(clip_polygon.shape[0]):
#             clip_edge_start = clip_polygon[i]
#             clip_edge_end = clip_polygon[(i + 1) % clip_polygon.shape[0]]
            
#             input_list = output_list
#             output_list = torch.zeros((0, 2), device=device)
            
#             # Pas de points Ã  dÃ©couper
#             if input_list.shape[0] == 0:
#                 continue
                
#             # Traitement de chaque arÃªte du polygone sujet
#             s = input_list[-1]
#             for e in input_list:
#                 # Point courant est Ã  l'intÃ©rieur
#                 if self.is_inside(e, clip_edge_start, clip_edge_end):
#                     # Point prÃ©cÃ©dent Ã©tait Ã  l'extÃ©rieur
#                     if not self.is_inside(s, clip_edge_start, clip_edge_end):
#                         # Ajouter le point d'intersection
#                         intersection = self.compute_intersection(
#                             s, e, clip_edge_start, clip_edge_end)
#                         output_list = torch.cat([output_list, intersection.unsqueeze(0)], dim=0)
                    
#                     # Ajouter le point courant
#                     output_list = torch.cat([output_list, e.unsqueeze(0)], dim=0)
#                 # Point courant est Ã  l'extÃ©rieur mais point prÃ©cÃ©dent Ã©tait Ã  l'intÃ©rieur
#                 elif self.is_inside(s, clip_edge_start, clip_edge_end):
#                     # Ajouter le point d'intersection
#                     intersection = self.compute_intersection(
#                         s, e, clip_edge_start, clip_edge_end)
#                     output_list = torch.cat([output_list, intersection.unsqueeze(0)], dim=0)
                
#                 # Mise Ã  jour du point prÃ©cÃ©dent
#                 s = e
        
#         return output_list
    
#     def is_inside(self, point: torch.Tensor, edge_start: torch.Tensor, edge_end: torch.Tensor) -> bool:
#         """
#         DÃ©termine si un point est Ã  l'intÃ©rieur par rapport Ã  une arÃªte (Ã  gauche de l'arÃªte orientÃ©e).
        
#         Args:
#             point: Tensor de forme (2,) - point Ã  tester
#             edge_start: Tensor de forme (2,) - dÃ©but de l'arÃªte
#             edge_end: Tensor de forme (2,) - fin de l'arÃªte
            
#         Returns:
#             Boolean - True si le point est Ã  l'intÃ©rieur
#         """
#         return ((edge_end[0] - edge_start[0]) * (point[1] - edge_start[1]) -
#                 (edge_end[1] - edge_start[1]) * (point[0] - edge_start[0])) > 0
    
#     def compute_intersection(self, s: torch.Tensor, e: torch.Tensor, 
#                            edge_start: torch.Tensor, edge_end: torch.Tensor) -> torch.Tensor:
#         """
#         Calcule l'intersection entre deux segments avec optimisation CUDA.
        
#         Args:
#             s: Tensor de forme (2,) - dÃ©but du premier segment
#             e: Tensor de forme (2,) - fin du premier segment
#             edge_start: Tensor de forme (2,) - dÃ©but du deuxiÃ¨me segment
#             edge_end: Tensor de forme (2,) - fin du deuxiÃ¨me segment
            
#         Returns:
#             Tensor de forme (2,) - point d'intersection
#         """
#         dc = torch.stack([s - e, edge_start - edge_end], dim=0)
#         dp = torch.stack([s, edge_start], dim=0)
#         n1 = s[0] * e[1] - s[1] * e[0]
#         n2 = edge_start[0] * edge_end[1] - edge_start[1] * edge_end[0] 
#         n3 = 1.0 / (dc[0, 0] * dc[1, 1] - dc[0, 1] * dc[1, 0] + self.eps)
        
#         return torch.tensor([
#             (n1 * dc[1, 0] - n2 * dc[0, 0]) * n3,
#             (n1 * dc[1, 1] - n2 * dc[0, 1]) * n3
#         ], device=s.device)
    
#     def compute_polygon_area(self, polygon: torch.Tensor) -> torch.Tensor:
#         """
#         Calcule l'aire d'un polygone avec la formule de l'aire de Shoelace (formule du lacet).
        
#         Args:
#             polygon: Tensor de forme (n, 2) - polygone dont on veut calculer l'aire
            
#         Returns:
#             Tensor - aire du polygone
#         """
#         x = polygon[:, 0]
#         y = polygon[:, 1]
        
#         # Formule de Shoelace optimisÃ©e pour PyTorch
#         return torch.abs(
#             torch.sum(x * torch.roll(y, -1)) - torch.sum(y * torch.roll(x, -1))
#         ) * 0.5


# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import math
# from typing import Tuple, Optional


# class OptimizedLocalisationLoss(nn.Module):
#     """
#     Version entiÃ¨rement optimisÃ©e de LocalisationLoss avec monitoring des losses individuelles.
#     Permet de rÃ©cupÃ©rer l1_loss et iou_loss non pondÃ©rÃ©es pour le monitoring.
#     """
    
#     def __init__(self, 
#                  l1_weight: float = 10.0,
#                  iou_weight: float = 2.0,
#                  iou_type: str = 'iou',
#                  coord_format: str = 'cxcywha',
#                  angle_normalization: str = 'symmetric_pi',
#                  eps: float = 1e-7,
#                  grid_size: int = 32,
#                  coord_max: float = 2.0):
#         super().__init__()
        
#         self.l1_weight = l1_weight
#         self.iou_weight = iou_weight
#         self.coord_format = coord_format
#         self.eps = eps
#         self.grid_size = grid_size
#         self.coord_max = coord_max
        
#         # Buffers prÃ©-allouÃ©s pour optimisation GPU
#         self.register_buffer('grid_x', torch.linspace(-1, 1, grid_size))
#         self.register_buffer('grid_y', torch.linspace(-1, 1, grid_size))
        
#         # Loss IoU orientÃ©e optimisÃ©e
#         self.oriented_iou_loss = OptimizedOrientedIoULoss(
#             loss_type=iou_type,
#             reduction='mean',
#             eps=eps,
#             grid_size=grid_size
#         )
        
#         self.angle_normalization = angle_normalization
        
#         # Variables pour le monitoring - stockage des losses individuelles
#         self._last_l1_loss = None
#         self._last_iou_loss = None
#         self._last_weighted_loss = None
    
#     def forward(self, logits: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
#         """Version vectorisÃ©e complÃ¨te du forward pass avec monitoring"""
#         device = logits.device
        
#         # PrÃ©paration vectorisÃ©e des donnÃ©es
#         pred_boxes = logits[:, :-1]
#         target_boxes = targets[:, 1:]
#         valid_mask = mask[:, 1:]
        
#         if valid_mask.dim() == 3:
#             valid_mask = valid_mask.squeeze(-1)
        
#         # Validation rapide
#         self._validate_shapes(pred_boxes, target_boxes, valid_mask)
        
#         # Calcul vectorisÃ© du nombre d'Ã©lÃ©ments valides
#         mask_sum = torch.sum(valid_mask)
#         if mask_sum == 0:
#             zero_loss = torch.tensor(0.0, device=device, requires_grad=True)
#             # Stockage pour monitoring mÃªme en cas de masque vide
#             self._last_l1_loss = zero_loss.detach()
#             self._last_iou_loss = zero_loss.detach()
#             self._last_weighted_loss = zero_loss.detach()
#             return zero_loss
        
#         # Calculs vectorisÃ©s parallÃ¨les avec indexation directe
#         l1_loss_raw = self._compute_l1_loss_direct_indexing(pred_boxes, target_boxes, valid_mask, mask_sum)
#         iou_loss_raw = self._compute_iou_loss_direct_indexing(pred_boxes, target_boxes, valid_mask)
#         # Stockage des losses individuelles NON PONDÃ‰RÃ‰ES pour monitoring
#         self._last_l1_loss = l1_loss_raw.detach()
#         self._last_iou_loss = iou_loss_raw.detach()
        
#         # Calcul de la loss finale pondÃ©rÃ©e
#         weighted_loss = self.l1_weight * l1_loss_raw + self.iou_weight * iou_loss_raw
#         self._last_weighted_loss = weighted_loss.detach()
        
#         return weighted_loss
    
#     def get_loss_components(self) -> Dict[str, torch.Tensor]:
#         """
#         Retourne les composantes individuelles de la loss pour le monitoring.
        
#         Returns:
#             Dict contenant:
#                 - 'l1_loss': Loss L1 non pondÃ©rÃ©e
#                 - 'iou_loss': Loss IoU non pondÃ©rÃ©e  
#                 - 'weighted_loss': Loss finale pondÃ©rÃ©e
#                 - 'l1_weighted': Loss L1 pondÃ©rÃ©e (l1_loss * l1_weight)
#                 - 'iou_weighted': Loss IoU pondÃ©rÃ©e (iou_loss * iou_weight)
#         """
#         if self._last_l1_loss is None or self._last_iou_loss is None:
#             # Pas encore de forward pass effectuÃ©
#             device = next(self.parameters()).device
#             zero = torch.tensor(0.0, device=device)
#             return {
#                 'l1_loss': zero,
#                 'iou_loss': zero,
#                 'weighted_loss': zero,
#                 'l1_weighted': zero,
#                 'iou_weighted': zero
#             }
        
#         return {
#             'l1_loss': self._last_l1_loss,                                    # NON pondÃ©rÃ©e
#             'iou_loss': self._last_iou_loss,                                  # NON pondÃ©rÃ©e
#             'weighted_loss': self._last_weighted_loss,                        # Finale pondÃ©rÃ©e
#             'l1_weighted': self._last_l1_loss * self.l1_weight,              # L1 pondÃ©rÃ©e
#             'iou_weighted': self._last_iou_loss * self.iou_weight             # IoU pondÃ©rÃ©e
#         }
    
#     def get_l1_loss(self) -> torch.Tensor:
#         """Retourne la loss L1 NON pondÃ©rÃ©e du dernier forward pass"""
#         return self._last_l1_loss if self._last_l1_loss is not None else torch.tensor(0.0)
    
#     def get_iou_loss(self) -> torch.Tensor:
#         """Retourne la loss IoU NON pondÃ©rÃ©e du dernier forward pass"""
#         return self._last_iou_loss if self._last_iou_loss is not None else torch.tensor(0.0)
    
#     def get_weighted_loss(self) -> torch.Tensor:
#         """Retourne la loss finale pondÃ©rÃ©e du dernier forward pass"""
#         return self._last_weighted_loss if self._last_weighted_loss is not None else torch.tensor(0.0)
    
#     def reset_monitoring(self):
#         """Remet Ã  zÃ©ro les variables de monitoring"""
#         self._last_l1_loss = None
#         self._last_iou_loss = None
#         self._last_weighted_loss = None
    
#     def _validate_shapes(self, pred_boxes: torch.Tensor, target_boxes: torch.Tensor, valid_mask: torch.Tensor):
#         """Validation rapide des dimensions"""
#         assert pred_boxes.shape == target_boxes.shape
#         assert pred_boxes.shape[:-1] == valid_mask.shape
    
#     def _compute_l1_loss_direct_indexing(self, pred_boxes: torch.Tensor, target_boxes: torch.Tensor, 
#                                        valid_mask: torch.Tensor, mask_sum: torch.Tensor) -> torch.Tensor:
#         """Version ultra-optimisÃ©e du calcul L1 avec indexation directe"""
#         # APPROCHE OPTIMALE: Masquage vectorisÃ© direct sans reshape
#         mask_expanded = valid_mask.unsqueeze(-1)  # (batch_size, seq_len-1, 1)
        
#         # Calcul L1 vectorisÃ© avec masquage direct
#         loss_per_element = F.smooth_l1_loss(pred_boxes, target_boxes, reduction="none")
#         masked_loss = loss_per_element * mask_expanded
        
#         # Somme vectorisÃ©e efficace
#         num_coords = pred_boxes.shape[-1]
#         return torch.sum(masked_loss) / (mask_sum * num_coords)
    
#     def _compute_iou_loss_direct_indexing(self, pred_boxes: torch.Tensor, target_boxes: torch.Tensor, 
#                                         valid_mask: torch.Tensor) -> torch.Tensor:
#         """Version entiÃ¨rement optimisÃ©e avec indexation directe - AUCUN reshape/view"""
#         device = pred_boxes.device
        
#         # APPROCHE OPTIMALE: Indexation directe sans reshape
#         valid_positions = valid_mask.nonzero(as_tuple=False)  # (N_valid, 2)
        
#         if valid_positions.shape[0] == 0:
#             return torch.tensor(1.0, device=device, requires_grad=True)
        
#         # Extraction directe avec indexation avancÃ©e - 0% overhead
#         batch_indices, seq_indices = valid_positions.unbind(-1)
#         valid_pred_boxes = pred_boxes[batch_indices, seq_indices]    # (N_valid, coord_dim)
#         valid_target_boxes = target_boxes[batch_indices, seq_indices]  # (N_valid, coord_dim)
        
#         # PrÃ©paration vectorisÃ©e des boÃ®tes
#         valid_pred_prepared = self._prepare_boxes_direct_indexing(valid_pred_boxes)
#         valid_target_prepared = self._prepare_boxes_direct_indexing(valid_target_boxes)
        
#         # Calcul IoU vectorisÃ©
#         try:
#             return self.oriented_iou_loss(valid_pred_prepared, valid_target_prepared)
#         except Exception as e:
#             print(f"Warning: IoU vectorisÃ© Ã©chouÃ©: {e}")
#             return torch.tensor(0.0, device=device, requires_grad=True)
    
#     def _prepare_boxes_direct_indexing(self, boxes: torch.Tensor) -> torch.Tensor:
#         """PrÃ©paration vectorisÃ©e ultra-rapide des boÃ®tes avec indexation directe"""
#         if self.coord_format == 'xyxy':
#             if boxes.shape[-1] == 4:
#                 # Ajout vectorisÃ© d'angles zÃ©ro - indexation directe
#                 zeros = torch.zeros(boxes.shape[0], 1, device=boxes.device, dtype=boxes.dtype)
#                 boxes_with_angle = torch.cat([boxes, zeros], dim=-1)
#             else:
#                 boxes_with_angle = boxes
            
#             boxes_cxcywha = self._convert_xyxy_to_cxcywha_direct(boxes_with_angle)
#             return self._ensure_valid_boxes_direct(boxes_cxcywha)
            
#         elif self.coord_format == 'cxcywha':
#             if boxes.shape[-1] == 4:
#                 zeros = torch.zeros(boxes.shape[0], 1, device=boxes.device, dtype=boxes.dtype)
#                 boxes_with_angle = torch.cat([boxes, zeros], dim=-1)
#             else:
#                 boxes_with_angle = boxes
            
#             return self._ensure_valid_boxes_direct(boxes_with_angle)
#         else:
#             raise ValueError(f"Format {self.coord_format} non supportÃ©")
    
#     def _convert_xyxy_to_cxcywha_direct(self, boxes: torch.Tensor) -> torch.Tensor:
#         """Conversion vectorisÃ©e ultra-rapide xyxy -> cxcywha avec indexation directe"""
#         # Extraction directe par colonnes - plus efficace que unbind
#         xmin, ymin, xmax, ymax, angle = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3], boxes[:, 4]
        
#         # Calculs vectorisÃ©s purs
#         cx = (xmin + xmax) * 0.5
#         cy = (ymin + ymax) * 0.5
#         w = torch.abs(xmax - xmin)
#         h = torch.abs(ymax - ymin)
        
#         # Construction directe du tenseur rÃ©sultat
#         return torch.stack([cx, cy, w, h, angle], dim=-1)
    
#     def _ensure_valid_boxes_direct(self, boxes: torch.Tensor) -> torch.Tensor:
#         """Validation adaptÃ©e aux coordonnÃ©es rÃ©elles"""
#         cx, cy, w, h, angles = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3], boxes[:, 4]
        
#         # Clamp dans la plage rÃ©elle de vos donnÃ©es [0, coord_max]
#         cx_clamped = torch.clamp(cx, min=0.0, max=self.coord_max)
#         cy_clamped = torch.clamp(cy, min=0.0, max=self.coord_max)
#         w_fixed = torch.clamp(w, min=self.eps, max=self.coord_max)
#         h_fixed = torch.clamp(h, min=self.eps, max=self.coord_max)
        
#         # Normalisation des angles
#         angles_normalized = self._normalize_angles_direct(angles)
        
#         return torch.stack([cx_clamped, cy_clamped, w_fixed, h_fixed, angles_normalized], dim=-1)
    
#     def _normalize_angles_direct(self, angles: torch.Tensor) -> torch.Tensor:
#         """Normalisation vectorisÃ©e ultra-rapide des angles avec indexation directe"""
#         if self.angle_normalization == 'symmetric_pi':
#             # Normalisation vectorisÃ©e dans [-Ï€/2, Ï€/2] - version optimisÃ©e
#             angles_mod = angles % (2 * math.pi)
#             angles_wrapped = torch.where(angles_mod > math.pi, angles_mod - 2 * math.pi, angles_mod)
#             angles_sym = torch.where(angles_wrapped > math.pi / 2, angles_wrapped - math.pi, angles_wrapped)
#             return torch.where(angles_sym < -math.pi / 2, angles_sym + math.pi, angles_sym)
#         elif self.angle_normalization == 'positive_pi':
#             return angles % math.pi
#         else:
#             return angles

# class OptimizedOrientedIoULoss(nn.Module):
#     """
#     Version ultra-optimisÃ©e de OrientedIoULoss avec indexation directe 
#     et approximation vectorisÃ©e de l'intersection de polygones.
#     """
    
#     def __init__(self, loss_type: str = 'iou', reduction: str = 'mean', 
#                  eps: float = 1e-7, grid_size: int = 32):
#         super().__init__()
#         self.loss_type = loss_type
#         self.reduction = reduction
#         self.eps = eps
#         self.grid_size = grid_size
        
#         # PrÃ©-calcul de la grille d'Ã©chantillonnage
#         self.register_buffer('sample_grid', self._create_sample_grid(grid_size))
    
#     def _create_sample_grid(self, grid_size: int) -> torch.Tensor:
#         """CrÃ©ation d'une grille d'Ã©chantillonnage pour l'approximation d'intersection"""
#         x = torch.linspace(-1.5, 1.5, grid_size)
#         y = torch.linspace(-1.5, 1.5, grid_size)
#         grid_x, grid_y = torch.meshgrid(x, y, indexing='ij')
#         return torch.stack([grid_x.flatten(), grid_y.flatten()], dim=-1)  # (grid_size^2, 2)
    
#     def forward(self, pred_boxes: torch.Tensor, target_boxes: torch.Tensor) -> torch.Tensor:
#         """Forward pass ultra-optimisÃ© avec indexation directe"""
#         if self.loss_type == 'iou':
#             iou = self.oriented_iou_direct_indexing(pred_boxes, target_boxes)
#             loss = 1.0 - iou
#         elif self.loss_type == 'giou':
#             iou, giou = self.oriented_giou_direct_indexing(pred_boxes, target_boxes)
#             loss = 1.0 - giou
#         elif self.loss_type == 'diou':
#             iou, diou = self.oriented_diou_direct_indexing(pred_boxes, target_boxes)
#             loss = 1.0 - diou
#         elif self.loss_type == 'ciou':
#             iou, ciou = self.oriented_ciou_direct_indexing(pred_boxes, target_boxes)
#             loss = 1.0 - ciou
#         else:
#             raise ValueError(f"Type de loss '{self.loss_type}' non supportÃ©.")
        
#         if self.reduction == 'mean':
#             return loss.mean()
#         elif self.reduction == 'sum':
#             return loss.sum()
#         else:
#             return loss
    
#     def oriented_iou_direct_indexing(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> torch.Tensor:
#         """Version ultra-rapide du calcul IoU avec indexation directe"""
#         # Aires exactes des boÃ®tes - indexation directe par colonnes
#         area1 = boxes1[:, 2] * boxes1[:, 3]  # w * h
#         area2 = boxes2[:, 2] * boxes2[:, 3]  # w * h
        
#         # Approximation rapide de l'intersection par Ã©chantillonnage
#         intersection = self._fast_intersection_direct_indexing(boxes1, boxes2)
        
#         # IoU avec stabilitÃ© numÃ©rique
#         union = area1 + area2 - intersection
#         iou = intersection / (union + self.eps)
#         return torch.clamp(iou, 0.0, 1.0)
    
#     def _fast_intersection_direct_indexing(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> torch.Tensor:
#         """
#         Approximation ultra-rapide de l'intersection avec indexation directe.
#         Aucun view/reshape/expand - uniquement des opÃ©rations vectorisÃ©es pures.
#         """
#         batch_size = boxes1.shape[0]
#         device = boxes1.device
#         n_samples = self.sample_grid.shape[0]
        
#         # OPTIMISATION: Expansion directe sans view/reshape problÃ©matique
#         grid = self.sample_grid.to(device)  # (N_samples, 2)
        
#         # CrÃ©ation de la grille Ã©tendue via broadcasting direct
#         # Ã‰vite complÃ¨tement expand() qui peut causer des problÃ¨mes de contiguÃ¯tÃ©
#         grid_broadcast = grid.unsqueeze(0)  # (1, N_samples, 2)
#         # Le broadcasting automatique de PyTorch gÃ¨re l'extension Ã  (B, N_samples, 2)
        
#         # Test d'inclusion vectorisÃ© avec indexation directe
#         inside1 = self._point_in_oriented_box_direct_indexing(grid_broadcast, boxes1)  # (B, N_samples)
#         inside2 = self._point_in_oriented_box_direct_indexing(grid_broadcast, boxes2)  # (B, N_samples)
        
#         # Intersection = points dans les deux boÃ®tes
#         intersection_points = inside1 & inside2  # (B, N_samples)
        
#         # Estimation de l'aire par comptage de points - calcul vectorisÃ© pur
#         grid_area = 9.0
#         sample_density = self.grid_size * self.grid_size
#         intersection_counts = intersection_points.float().sum(dim=1)  # (B,)
        
#         return intersection_counts * (grid_area / sample_density)
    
#     def _point_in_oriented_box_direct_indexing(self, points: torch.Tensor, boxes: torch.Tensor) -> torch.Tensor:
#         """
#         Test vectorisÃ© ultra-rapide avec indexation directe pure.
        
#         Args:
#             points: (1, N_points, 2) ou (B, N_points, 2) - points Ã  tester
#             boxes: (B, 5) - boÃ®tes [cx, cy, w, h, angle]
        
#         Returns:
#             (B, N_points) - masque boolÃ©en d'inclusion
#         """
#         # Gestion du broadcasting automatique
#         if points.dim() == 3 and points.shape[0] == 1:
#             # Broadcasting automatique de (1, N_points, 2) vers (B, N_points, 2)
#             points_x = points[0, :, 0]  # (N_points,)
#             points_y = points[0, :, 1]  # (N_points,)
#         else:
#             points_x = points[:, :, 0]  # (B, N_points)
#             points_y = points[:, :, 1]  # (B, N_points)
        
#         # Extraction directe des paramÃ¨tres des boÃ®tes par colonnes
#         cx = boxes[:, 0]  # (B,)
#         cy = boxes[:, 1]  # (B,)
#         w = boxes[:, 2]   # (B,)
#         h = boxes[:, 3]   # (B,)
#         angle = boxes[:, 4]  # (B,)
        
#         # Broadcasting direct sans expand explicite - PyTorch gÃ¨re automatiquement
#         if points.dim() == 3 and points.shape[0] == 1:
#             # Translation avec broadcasting automatique
#             dx = points_x.unsqueeze(0) - cx.unsqueeze(1)  # (B, N_points)
#             dy = points_y.unsqueeze(0) - cy.unsqueeze(1)  # (B, N_points)
#         else:
#             dx = points_x - cx.unsqueeze(1)  # (B, N_points)
#             dy = points_y - cy.unsqueeze(1)  # (B, N_points)
        
#         # TrigonomÃ©trie vectorisÃ©e avec broadcasting automatique
#         cos_angle = torch.cos(-angle).unsqueeze(1)  # (B, 1)
#         sin_angle = torch.sin(-angle).unsqueeze(1)  # (B, 1)
        
#         # Rotation vectorisÃ©e - broadcasting automatique
#         x_rot = dx * cos_angle - dy * sin_angle  # (B, N_points)
#         y_rot = dx * sin_angle + dy * cos_angle  # (B, N_points)
        
#         # Test d'inclusion dans la boÃ®te alignÃ©e - broadcasting automatique
#         half_w = (w * 0.5).unsqueeze(1)  # (B, 1)
#         half_h = (h * 0.5).unsqueeze(1)  # (B, 1)
        
#         inside_x = torch.abs(x_rot) <= half_w  # (B, N_points)
#         inside_y = torch.abs(y_rot) <= half_h  # (B, N_points)
        
#         return inside_x & inside_y  # (B, N_points)
    
#     def oriented_giou_direct_indexing(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
#         """Version vectorisÃ©e du GIoU orientÃ© avec indexation directe"""
#         # IoU de base
#         iou = self.oriented_iou_direct_indexing(boxes1, boxes2)
        
#         # Calcul vectorisÃ© des boÃ®tes englobantes avec indexation directe
#         enclosing_area = self._compute_enclosing_area_direct_indexing(boxes1, boxes2)
        
#         # Aires et intersection - indexation directe par colonnes
#         area1 = boxes1[:, 2] * boxes1[:, 3]  # w * h
#         area2 = boxes2[:, 2] * boxes2[:, 3]  # w * h
#         intersection = self._fast_intersection_direct_indexing(boxes1, boxes2)
#         union = area1 + area2 - intersection
        
#         # GIoU
#         giou = iou - (enclosing_area - union) / (enclosing_area + self.eps)
#         return iou, torch.clamp(giou, min=-1.0, max=1.0)
    
#     def _compute_enclosing_area_direct_indexing(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> torch.Tensor:
#         """Calcul vectorisÃ© ultra-rapide de l'aire englobante avec indexation directe"""
#         # Conversion en polygones pour obtenir les extrema
#         poly1 = self._obb_to_polygon_direct_indexing(boxes1)
#         poly2 = self._obb_to_polygon_direct_indexing(boxes2)
        
#         # Calcul des boÃ®tes englobantes - indexation directe
#         min1 = torch.min(poly1, dim=1)[0]  # (B, 2)
#         max1 = torch.max(poly1, dim=1)[0]  # (B, 2)
#         min2 = torch.min(poly2, dim=1)[0]  # (B, 2)
#         max2 = torch.max(poly2, dim=1)[0]  # (B, 2)
        
#         # BoÃ®te englobante globale - indexation directe
#         min_x = torch.min(min1[:, 0], min2[:, 0])  # (B,)
#         min_y = torch.min(min1[:, 1], min2[:, 1])  # (B,)
#         max_x = torch.max(max1[:, 0], max2[:, 0])  # (B,)
#         max_y = torch.max(max1[:, 1], max2[:, 1])  # (B,)
        
#         # Aire englobante
#         return (max_x - min_x) * (max_y - min_y)  # (B,)
    
#     def _obb_to_polygon_direct_indexing(self, boxes: torch.Tensor) -> torch.Tensor:
#         """Conversion ultra-rapide OBB -> polygone avec indexation directe"""
#         # Extraction directe par colonnes - plus efficace que unbind
#         cx, cy, w, h, angle = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3], boxes[:, 4]
        
#         # Demi-dimensions
#         w_half = w * 0.5
#         h_half = h * 0.5
        
#         # TrigonomÃ©trie vectorisÃ©e optimisÃ©e
#         cos_a = torch.cos(angle)
#         sin_a = torch.sin(angle)
        
#         # Calcul vectorisÃ© des 4 coins avec indexation directe
#         # Construction directe des coordonnÃ©es sans stack intermÃ©diaire
#         corner1_x = cx + (-w_half * cos_a + h_half * sin_a)
#         corner1_y = cy + (-w_half * sin_a - h_half * cos_a)
#         corner2_x = cx + (w_half * cos_a + h_half * sin_a)
#         corner2_y = cy + (w_half * sin_a - h_half * cos_a)
#         corner3_x = cx + (w_half * cos_a - h_half * sin_a)
#         corner3_y = cy + (w_half * sin_a + h_half * cos_a)
#         corner4_x = cx + (-w_half * cos_a - h_half * sin_a)
#         corner4_y = cy + (-w_half * sin_a + h_half * cos_a)
        
#         # Construction finale du polygone - indexation directe optimisÃ©e
#         corners_x = torch.stack([corner1_x, corner2_x, corner3_x, corner4_x], dim=1)  # (B, 4)
#         corners_y = torch.stack([corner1_y, corner2_y, corner3_y, corner4_y], dim=1)  # (B, 4)
        
#         return torch.stack([corners_x, corners_y], dim=-1)  # (B, 4, 2)
    
#     def oriented_diou_direct_indexing(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
#         """Version vectorisÃ©e ultra-rapide du DIoU avec indexation directe"""
#         # IoU de base
#         iou = self.oriented_iou_direct_indexing(boxes1, boxes2)
        
#         # Distance entre centres - indexation directe par colonnes
#         centers1_x, centers1_y = boxes1[:, 0], boxes1[:, 1]
#         centers2_x, centers2_y = boxes2[:, 0], boxes2[:, 1]
#         d2 = (centers1_x - centers2_x) ** 2 + (centers1_y - centers2_y) ** 2
        
#         # Diagonale de la boÃ®te englobante
#         enclosing_diagonal2 = self._compute_enclosing_diagonal2_direct_indexing(boxes1, boxes2)
        
#         # DIoU
#         diou = iou - d2 / (enclosing_diagonal2 + self.eps)
#         return iou, torch.clamp(diou, min=-1.0, max=1.0)
    
#     def _compute_enclosing_diagonal2_direct_indexing(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> torch.Tensor:
#         """Calcul vectorisÃ© de la diagonale au carrÃ© avec indexation directe"""
#         poly1 = self._obb_to_polygon_direct_indexing(boxes1)
#         poly2 = self._obb_to_polygon_direct_indexing(boxes2)
        
#         # Extrema avec indexation directe
#         min1 = torch.min(poly1, dim=1)[0]  # (B, 2)
#         max1 = torch.max(poly1, dim=1)[0]  # (B, 2)
#         min2 = torch.min(poly2, dim=1)[0]  # (B, 2)
#         max2 = torch.max(poly2, dim=1)[0]  # (B, 2)
        
#         # Calcul direct de la diagonale - indexation par colonnes
#         min_x = torch.min(min1[:, 0], min2[:, 0])
#         min_y = torch.min(min1[:, 1], min2[:, 1])
#         max_x = torch.max(max1[:, 0], max2[:, 0])
#         max_y = torch.max(max1[:, 1], max2[:, 1])
        
#         # Diagonale au carrÃ©
#         return (max_x - min_x) ** 2 + (max_y - min_y) ** 2
    
#     def oriented_ciou_direct_indexing(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
#         """Version vectorisÃ©e stabilisÃ©e du CIoU avec indexation directe"""
#         # DIoU de base
#         iou, diou = self.oriented_diou_direct_indexing(boxes1, boxes2)
        
#         # Extraction directe des paramÃ¨tres par colonnes
#         w1, h1, a1 = boxes1[:, 2], boxes1[:, 3], boxes1[:, 4]
#         w2, h2, a2 = boxes2[:, 2], boxes2[:, 3], boxes2[:, 4]
        
#         # Normalisation vectorisÃ©e des angles
#         a1_norm = torch.atan2(torch.sin(2*a1), torch.cos(2*a1)) / 2
#         a2_norm = torch.atan2(torch.sin(2*a2), torch.cos(2*a2)) / 2
        
#         # Ratios d'aspect avec stabilitÃ©
#         ar1 = w1 / torch.clamp(h1, min=self.eps)
#         ar2 = w2 / torch.clamp(h2, min=self.eps)
        
#         # Gestion vectorisÃ©e des orientations
#         angle_diff = torch.abs(a1_norm - a2_norm)
#         should_flip = angle_diff > (math.pi / 4)
#         ar1_adjusted = torch.where(should_flip, 1.0 / ar1, ar1)
        
#         # Terme v stabilisÃ©
#         atan_diff = torch.atan(ar2) - torch.atan(ar1_adjusted)
#         v = (4.0 / (math.pi ** 2)) * torch.pow(atan_diff, 2)
#         v = torch.clamp(v, min=0.0, max=1.0)
        
#         # Alpha avec stabilitÃ©
#         alpha = v / ((1.0 - iou) + v + self.eps)
#         alpha = torch.clamp(alpha, min=0.0, max=1.0)
        
#         # PondÃ©ration adaptative
#         aspect_weight = torch.clamp(1.0 - iou, min=0.2, max=1.0)
#         aspect_penalty = aspect_weight * alpha * v * 0.5
        
#         # CIoU final
#         ciou = diou - aspect_penalty
#         return iou, torch.clamp(ciou, min=-1.0, max=1.0)


# class HybridLocalisationLoss(nn.Module):
#     """
#     Version hybride : exactitude de la non-optimisÃ©e + optimisations ciblÃ©es
#     """
    
#     def __init__(self, 
#                  l1_weight: float = 1.0,
#                  iou_weight: float = 5.0,
#                  iou_type: str = 'ciou',
#                  coord_format: str = 'cxcywha',
#                  eps: float = 1e-7,
#                  coord_max: float = 2.0):
#         super().__init__()
        
#         self.l1_weight = l1_weight
#         self.iou_weight = iou_weight
#         self.coord_format = coord_format
#         self.eps = eps
#         self.coord_max = coord_max
        
#         # Utiliser la version NON-optimisÃ©e pour la prÃ©cision
#         self.oriented_iou_loss = OrientedIoULoss(
#             loss_type=iou_type,
#             reduction='mean',
#             eps=eps
#         )
        
#         # Storage pour monitoring
#         self._last_l1_loss = None
#         self._last_iou_loss = None
#         self._last_weighted_loss = None
    
#     def forward(self, logits: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
#         device = logits.device
        
#         # PrÃ©paration optimisÃ©e (gardez cette partie)
#         pred_boxes = logits[:, :-1]
#         target_boxes = targets[:, 1:]
#         valid_mask = mask[:, 1:]
        
#         if valid_mask.dim() == 3:
#             valid_mask = valid_mask.squeeze(-1)
        
#         mask_sum = torch.sum(valid_mask)
#         if mask_sum == 0:
#             zero_loss = torch.tensor(0.0, device=device, requires_grad=True)
#             self._last_l1_loss = zero_loss.detach()
#             self._last_iou_loss = zero_loss.detach()
#             self._last_weighted_loss = zero_loss.detach()
#             return zero_loss
        
#         # L1 Loss optimisÃ©e (gardez cette version)
#         l1_loss_raw = self._compute_l1_loss_vectorized(pred_boxes, target_boxes, valid_mask, mask_sum)
        
#         # IoU Loss EXACTE (utilisez la version non-optimisÃ©e)
#         iou_loss_raw = self._compute_iou_loss_exact(pred_boxes, target_boxes, valid_mask)
        
#         # Storage et return
#         self._last_l1_loss = l1_loss_raw.detach()
#         self._last_iou_loss = iou_loss_raw.detach()
        
#         weighted_loss = self.l1_weight * l1_loss_raw + self.iou_weight * iou_loss_raw
#         self._last_weighted_loss = weighted_loss.detach()
        
#         return weighted_loss
    
#     def _compute_l1_loss_vectorized(self, pred_boxes, target_boxes, valid_mask, mask_sum):
#         """Gardez la version vectorisÃ©e pour L1 - elle est correcte"""
#         mask_expanded = valid_mask.unsqueeze(-1)
#         per_element_loss = F.smooth_l1_loss(pred_boxes, target_boxes, reduction="none")
#         masked_loss = per_element_loss * mask_expanded
#         return torch.sum(masked_loss) / (mask_sum * pred_boxes.shape[-1])
    
#     def _compute_iou_loss_exact(self, pred_boxes, target_boxes, valid_mask):
#         """Utilise la version EXACTE pour IoU"""
#         device = pred_boxes.device
        
#         # Filtrage des Ã©lÃ©ments valides (gardez cette optimisation)
#         valid_indices = torch.nonzero(valid_mask.flatten(), as_tuple=False).squeeze(1)
        
#         if len(valid_indices) == 0:
#             return torch.tensor(1.0, device=device, requires_grad=True)
        
#         # Extraction vectorisÃ©e (gardez cette optimisation)
#         flat_pred = pred_boxes.reshape(-1, pred_boxes.shape[-1])
#         flat_target = target_boxes.reshape(-1, target_boxes.shape[-1])
        
#         valid_pred_boxes = flat_pred[valid_indices]
#         valid_target_boxes = flat_target[valid_indices]
        
#         # PrÃ©paration (gardez la version optimisÃ©e)
#         valid_pred_boxes = self._prepare_boxes_hybrid(valid_pred_boxes)
#         valid_target_boxes = self._prepare_boxes_hybrid(valid_target_boxes)
        
#         # MAIS utilisez la version EXACTE pour le calcul IoU
#         try:
#             return self.oriented_iou_loss(valid_pred_boxes, valid_target_boxes)
#         except Exception as e:
#             print(f"Warning: Exact IoU computation failed: {e}")
#             return torch.tensor(0.0, device=device, requires_grad=True)
    
#     def _prepare_boxes_hybrid(self, boxes):
#         """Version optimisÃ©e de la prÃ©paration"""
#         if self.coord_format == 'cxcywha':
#             if boxes.shape[-1] == 4:
#                 zeros = torch.zeros(boxes.shape[0], 1, device=boxes.device, dtype=boxes.dtype)
#                 boxes_with_angle = torch.cat([boxes, zeros], dim=-1)
#             else:
#                 boxes_with_angle = boxes
            
#             return self._ensure_valid_boxes_hybrid(boxes_with_angle)
#         else:
#             raise ValueError(f"Format {self.coord_format} non supportÃ©")
    
#     def _ensure_valid_boxes_hybrid(self, boxes):
#         """Validation optimisÃ©e"""
#         cx, cy, w, h, angles = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3], boxes[:, 4]
        
#         cx_clamped = torch.clamp(cx, min=0.0, max=self.coord_max)
#         cy_clamped = torch.clamp(cy, min=0.0, max=self.coord_max)
#         w_fixed = torch.clamp(w, min=self.eps, max=self.coord_max)
#         h_fixed = torch.clamp(h, min=self.eps, max=self.coord_max)
        
#         # Normalisation d'angle simple et efficace
#         angles_normalized = torch.atan2(torch.sin(angles), torch.cos(angles))
        
#         return torch.stack([cx_clamped, cy_clamped, w_fixed, h_fixed, angles_normalized], dim=-1)
    
#     # Ajoutez les mÃ©thodes de monitoring de la version optimisÃ©e
#     def get_loss_components(self):
#         if self._last_l1_loss is None or self._last_iou_loss is None:
#             device = next(self.parameters()).device
#             zero = torch.tensor(0.0, device=device)
#             return {'l1_loss': zero, 'iou_loss': zero, 'weighted_loss': zero}
        
#         return {
#             'l1_loss': self._last_l1_loss,
#             'iou_loss': self._last_iou_loss,
#             'weighted_loss': self._last_weighted_loss
#         }


# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import math
# import torchvision.ops as ops # Pour box_iou

# # --- Classe KLDLossGaussian ( inchangÃ©e, elle est gÃ©nÃ©rique et robuste ) ---
# class KLDLossGaussian(nn.Module):
#     """
#     Calcule la Kullback-Leibler Divergence (KLD) entre deux distributions Gaussiennes 2D.
#     UtilisÃ©e pour la rÃ©gression des boÃ®tes englobantes orientÃ©es (OBB).
#     """

#     def __init__(self, eps: float = 1e-7):
#         super().__init__()
#         self.eps = eps

#     def forward(self, mu1: torch.Tensor, sigma1: torch.Tensor, mu2: torch.Tensor, sigma2: torch.Tensor) -> torch.Tensor:
#         """
#         Args:
#             mu1 (torch.Tensor): Moyennes des Gaussiennes prÃ©dites (N, 2) [cx, cy]
#             sigma1 (torch.Tensor): Matrices de covariance des Gaussiennes prÃ©dites (N, 2, 2)
#             mu2 (torch.Tensor): Moyennes des Gaussiennes cibles (N, 2) [cx, cy]
#             sigma2 (torch.Tensor): Matrices de covariance des Gaussiennes cibles (N, 2, 2)

#         Returns:
#             torch.Tensor: KLD Loss moyenne sur le lot.
#         """
#         # Assurer que les matrices de covariance sont inversibles et ont des dÃ©terminants positifs
#         # Ajout d'une petite identitÃ© pour la stabilitÃ© numÃ©rique
#         sigma1 = sigma1 + self.eps * torch.eye(2, device=sigma1.device).unsqueeze(0)
#         sigma2 = sigma2 + self.eps * torch.eye(2, device=sigma2.device).unsqueeze(0)

#         # Calcul des inverses et dÃ©terminants
#         inv_sigma2 = torch.linalg.inv(sigma2)
#         det_sigma1 = torch.linalg.det(sigma1)
#         det_sigma2 = torch.linalg.det(sigma2)

#         # Terme de trace
#         trace_term = torch.diagonal(torch.matmul(inv_sigma2, sigma1), dim1=-2, dim2=-1).sum(-1)

#         # Terme de diffÃ©rence des moyennes
#         mu_diff = mu2 - mu1 # (N, 2)
#         mu_diff_term = torch.matmul(mu_diff.unsqueeze(1), torch.matmul(inv_sigma2, mu_diff.unsqueeze(2))).squeeze(-1).squeeze(-1) # (N,)

#         # Terme de dÃ©terminant
#         det_term = torch.log(det_sigma2 / (det_sigma1 + self.eps)) # Ajouter eps pour stabilitÃ©

#         # KLD Loss
#         # Formule D_KL(P || Q) = 0.5 * (tr(Sigma2^-1 Sigma1) + (mu2 - mu1)^T Sigma2^-1 (mu2 - mu1) - k + ln(det(Sigma2)/det(Sigma1)))
#         # k est la dimension, ici 2 pour 2D
#         kld_loss_per_box = 0.5 * (trace_term + mu_diff_term - 2 + det_term)

#         # Assurer que la KLD Loss est non-nÃ©gative (elle doit l'Ãªtre thÃ©oriquement)
#         return torch.relu(kld_loss_per_box).mean()


# # --- Nouvelle classe de Loss : KLDHybridLocalizationLoss ---
# class KLDHybridLocalizationLoss(nn.Module):
#     """
#     Loss combinÃ©e pour la localisation :
#     - KLD Loss pour la rÃ©gression des boÃ®tes orientÃ©es (gradients)
#     - Smooth L1 Loss pour les coordonnÃ©es brutes (gradients)
#     - IoU non-orientÃ©e (AABB) pour le monitoring (sans gradients)
#     """

#     def __init__(self,
#                  l1_weight: float = 1.0,
#                  kld_weight: float = 5.0,
#                  coord_format: str = 'cxcywha',  # 'cxcywha' ou 'xyxy'
#                  angle_normalization: str = 'symmetric_pi', # ou 'positive_pi' ou None
#                  eps: float = 1e-7,
#                  coord_max: float = 1.0): # Valeur max attendue pour les coordonnÃ©es normalisÃ©es (e.g., 1.0 pour [0,1])
#         super().__init__()

#         self.l1_weight = l1_weight
#         self.kld_weight = kld_weight
#         self.coord_format = coord_format
#         self.angle_normalization = angle_normalization
#         self.eps = eps
#         self.coord_max = coord_max

#         # KLD Loss calculator
#         self.kld_calculator = KLDLossGaussian(eps=eps)

#         # Storage pour monitoring des composants
#         self._last_l1_loss = None
#         self._last_kld_loss = None
#         self._last_weighted_loss = None
#         self._last_mean_iou_metric = None # Pour l'IoU AABB de monitoring

#     def forward(self, logits: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
#         device = logits.device

#         # PrÃ©paration des donnÃ©es (dÃ©calage temporel)
#         # Supposons que la derniÃ¨re prÃ©diction/cible n'est pas pour la localisation
#         pred_boxes = logits[:, :-1]  # (batch_size, seq_len-1, 4/5)
#         target_boxes = targets[:, 1:]  # (batch_size, seq_len-1, 4/5)
#         valid_mask = mask[:, 1:]  # (batch_size, seq_len-1)

#         # Normalisation des dimensions du masque
#         if valid_mask.dim() == 3:
#             valid_mask = valid_mask.squeeze(-1)

#         # Validation des dimensions
#         self._validate_shapes(pred_boxes, target_boxes, valid_mask)

#         # Calcul du nombre d'Ã©lÃ©ments valides
#         mask_sum = torch.sum(valid_mask)
#         if mask_sum == 0:
#             zero_loss = torch.tensor(0.0, device=device, requires_grad=True)
#             self._update_monitoring_values(zero_loss, zero_loss, zero_loss, torch.tensor(1.0, device=device))
#             return zero_loss

#         # Filtrage efficace des Ã©lÃ©ments valides (pour L1, KLD et IoU)
#         valid_indices = torch.nonzero(valid_mask.flatten(), as_tuple=False).squeeze(1)
#         flat_pred = pred_boxes.reshape(-1, pred_boxes.shape[-1])
#         flat_target = target_boxes.reshape(-1, target_boxes.shape[-1])
#         valid_pred_boxes = flat_pred[valid_indices]
#         valid_target_boxes = flat_target[valid_indices]

#         # --- Calcul de la Smooth L1 Loss ---
#         # Pas besoin d'appliquer de _prepare_boxes_for_loss ici, SmoothL1 fonctionne sur les valeurs brutes
#         l1_loss = F.smooth_l1_loss(valid_pred_boxes, valid_target_boxes, reduction="mean")

#         # --- Calcul de la KLD Loss (pour l'entraÃ®nement) ---
#         # Les boÃ®tes doivent Ãªtre correctement prÃ©parÃ©es pour la KLD Loss
#         valid_pred_boxes_processed = self._prepare_boxes_for_loss(valid_pred_boxes)
#         valid_target_boxes_processed = self._prepare_boxes_for_loss(valid_target_boxes)

#         kld_loss = torch.tensor(0.0, device=device, requires_grad=True)
#         try:
#             pred_mu, pred_sigma = self._obb_to_gaussian_params(valid_pred_boxes_processed)
#             target_mu, target_sigma = self._obb_to_gaussian_params(valid_target_boxes_processed)
#             kld_loss = self.kld_calculator(pred_mu, pred_sigma, target_mu, target_sigma)
#         except Exception as e:
#             print(f"Warning: KLD loss computation failed: {e}. Returning large loss.")
#             kld_loss = torch.tensor(100.0, device=device, requires_grad=True)


#         # --- Calcul de l'IoU AABB pour le monitoring (SANS gradients) ---
#         mean_iou_metric = torch.tensor(1.0, device=device) # Par dÃ©faut Ã  1.0 si problÃ¨me
#         with torch.no_grad():
#             try:
#                 # Convertir cxcywha en xyxy pour box_iou (mÃªme si c'est une OBB Ã  la base)
#                 # Note: box_iou attend (xmin, ymin, xmax, ymax)
#                 pred_xyxy = self._convert_cxcywha_to_xyxy(valid_pred_boxes_processed)
#                 target_xyxy = self._convert_cxcywha_to_xyxy(valid_target_boxes_processed)

#                 # Calcul de l'IoU AABB
#                 iou_matrix = ops.box_iou(pred_xyxy, target_xyxy) # (N, N)
#                 # Nous voulons l'IoU entre paires correspondantes (diag).
#                 # Si pred_boxes et target_boxes sont de mÃªme taille et correspondent 1:1,
#                 # on prend la diagonale. Si le batching est plus complexe (e.g., chaque pred vs toutes targets),
#                 # il faudrait une logique de correspondance plus avancÃ©e (ex: Hungarian matching).
#                 # Pour l'instant, on suppose une correspondance 1:1 pour la mÃ©trique.
#                 if iou_matrix.numel() > 0: # Check if not empty
#                     mean_iou_metric = torch.diag(iou_matrix).mean()
#                 else:
#                      mean_iou_metric = torch.tensor(0.0, device=device) # Si pas de boites, iou 0

#             except Exception as e:
#                 print(f"Warning: IoU metric computation failed: {e}. Returning 0.0 IoU.")
#                 mean_iou_metric = torch.tensor(0.0, device=device) # Retourne 0 si Ã©chec


#         # --- Calcul de la perte totale pondÃ©rÃ©e ---
#         weighted_loss = self.l1_weight * l1_loss + self.kld_weight * kld_loss

#         # Mise Ã  jour des valeurs de monitoring
#         self._update_monitoring_values(l1_loss, kld_loss, weighted_loss, mean_iou_metric)

#         return weighted_loss

#     def _update_monitoring_values(self, l1_loss: torch.Tensor, kld_loss: torch.Tensor,
#                                   weighted_loss: torch.Tensor, mean_iou_metric: torch.Tensor):
#         """Met Ã  jour les attributs de monitoring avec les valeurs detachÃ©es."""
#         self._last_l1_loss = l1_loss.detach()
#         self._last_kld_loss = kld_loss.detach()
#         self._last_weighted_loss = weighted_loss.detach()
#         self._last_mean_iou_metric = mean_iou_metric.detach()


#     def _validate_shapes(self, pred_boxes: torch.Tensor, target_boxes: torch.Tensor, valid_mask: torch.Tensor):
#         """Validation des dimensions des tenseurs"""
#         assert pred_boxes.shape == target_boxes.shape, \
#             f"Shape mismatch: {pred_boxes.shape} vs {target_boxes.shape}"
#         assert pred_boxes.shape[:-1] == valid_mask.shape, \
#             f"Mask shape mismatch: {pred_boxes.shape[:-1]} vs {valid_mask.shape}"

#     def _prepare_boxes_for_loss(self, boxes: torch.Tensor) -> torch.Tensor:
#         """
#         PrÃ©paration et validation des boÃ®tes : conversion vers cxcywha si nÃ©cessaire,
#         clamp des dimensions w/h et normalisation de l'angle.
#         """
#         # Assurez-vous que boxes est de la forme (N, D) oÃ¹ D est 4 ou 5
#         if boxes.dim() == 1:
#             boxes = boxes.unsqueeze(0)

#         processed_boxes = boxes.clone() # Travailler sur une copie pour Ã©viter les modifications in-place inattendues

#         if self.coord_format == 'xyxy':
#             # Si xyxy et seulement 4 dims, ajouter un angle 0 par dÃ©faut
#             if processed_boxes.shape[-1] == 4:
#                 angles = torch.zeros(processed_boxes.shape[0], 1, device=processed_boxes.device, dtype=processed_boxes.dtype)
#                 processed_boxes = torch.cat([processed_boxes, angles], dim=-1)
#             # Puis convertir en cxcywha
#             processed_boxes = self._convert_xyxy_to_cxcywha(processed_boxes)
#         elif self.coord_format == 'cxcywha':
#             # Si cxcywh et seulement 4 dims, ajouter un angle 0 par dÃ©faut
#             if processed_boxes.shape[-1] == 4:
#                 angles = torch.zeros(processed_boxes.shape[0], 1, device=processed_boxes.device, dtype=processed_boxes.dtype)
#                 processed_boxes = torch.cat([processed_boxes, angles], dim=-1)
#             # Sinon, le format est dÃ©jÃ  cxcywha (avec 5 dims)
#         else:
#             raise ValueError(f"Format {self.coord_format} non supportÃ©")

#         # Clamp et normalisation des angles sur le format cxcywha final
#         cx, cy, w, h, angles = processed_boxes.unbind(-1)

#         # Clamp cx, cy dans [0, coord_max]
#         cx_clamped = torch.clamp(cx, min=0.0, max=self.coord_max)
#         cy_clamped = torch.clamp(cy, min=0.0, max=self.coord_max)

#         # Clamp w, h pour qu'ils soient > eps et <= coord_max
#         w_fixed = torch.clamp(w, min=self.eps, max=self.coord_max)
#         h_fixed = torch.clamp(h, min=self.eps, max=self.coord_max)

#         # Normalisation de l'angle
#         if self.angle_normalization == 'symmetric_pi':
#             angles_normalized = self._normalize_angle_symmetric_pi(angles)
#         elif self.angle_normalization == 'positive_pi':
#             angles_normalized = self._normalize_angle_positive_pi(angles)
#         else:
#             angles_normalized = angles # Pas de normalisation si non spÃ©cifiÃ©

#         return torch.stack([cx_clamped, cy_clamped, w_fixed, h_fixed, angles_normalized], dim=-1)

#     def _convert_xyxy_to_cxcywha(self, boxes_xyxy_angle: torch.Tensor) -> torch.Tensor:
#         """Conversion (xmin,ymin,xmax,ymax,angle) -> (cx,cy,w,h,angle)"""
#         xmin, ymin, xmax, ymax, angle = boxes_xyxy_angle.unbind(-1)

#         cx = (xmin + xmax) / 2
#         cy = (ymin + ymax) / 2
#         w = torch.abs(xmax - xmin) # Utiliser abs car xmax peut Ãªtre < xmin si predictions folles
#         h = torch.abs(ymax - ymin) # Utiliser abs car ymax peut Ãªtre < ymin si predictions folles

#         return torch.stack([cx, cy, w, h, angle], dim=1)

#     def _convert_cxcywha_to_xyxy(self, boxes_cxcywha: torch.Tensor) -> torch.Tensor:
#         """Conversion (cx,cy,w,h,angle) -> (xmin,ymin,xmax,ymax) pour IoU AABB (ignore l'angle)"""
#         cx, cy, w, h, _ = boxes_cxcywha.unbind(-1)
#         xmin = cx - w / 2
#         ymin = cy - h / 2
#         xmax = cx + w / 2
#         ymax = cy + h / 2
#         return torch.stack([xmin, ymin, xmax, ymax], dim=-1)


#     def _normalize_angle_symmetric_pi(self, angles: torch.Tensor) -> torch.Tensor:
#         """Normalisation dans [-Ï€/2, Ï€/2]"""
#         angles_mod = angles % (2 * math.pi)
#         angles_wrapped = torch.where(angles_mod > math.pi, angles_mod - 2 * math.pi, angles_mod)
#         angles_sym1 = torch.where(angles_wrapped > math.pi / 2, angles_wrapped - math.pi, angles_wrapped)
#         angles_sym2 = torch.where(angles_sym1 < -math.pi / 2, angles_sym1 + math.pi, angles_sym1)
#         return angles_sym2

#     def _normalize_angle_positive_pi(self, angles: torch.Tensor) -> torch.Tensor:
#         """Normalisation dans [0, Ï€]"""
#         return angles % math.pi

#     def _obb_to_gaussian_params(self, boxes: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
#         """
#         Convertit les paramÃ¨tres d'OBB (cx, cy, w, h, angle) en paramÃ¨tres
#         de distribution Gaussienne 2D (mu, Sigma).
#         """
#         cx, cy, w, h, angle = boxes.unbind(-1)

#         mu = torch.stack([cx, cy], dim=-1) # (N, 2)

#         # w et h sont dÃ©jÃ  clampÃ©s Ã  min=self.eps par _prepare_boxes_for_loss
#         w_sq_4 = (w**2 / 4)
#         h_sq_4 = (h**2 / 4)

#         cos_a = torch.cos(angle)
#         sin_a = torch.sin(angle)

#         R = torch.stack([
#             torch.stack([cos_a, -sin_a], dim=-1),
#             torch.stack([sin_a, cos_a], dim=-1)
#         ], dim=-2) # (N, 2, 2)

#         S = torch.diag_embed(torch.stack([w_sq_4, h_sq_4], dim=-1)) # (N, 2, 2)

#         Sigma = torch.matmul(R, torch.matmul(S, R.transpose(-1, -2))) # (N, 2, 2)

#         return mu, Sigma

#     def get_loss_components(self):
#         """Retourne les composants de perte pour le monitoring."""
#         if self._last_l1_loss is None:
#             device = next(self.parameters()).device if hasattr(self, 'parameters') and next(self.parameters(), None) is not None else torch.device('cpu')
#             zero = torch.tensor(0.0, device=device)
#             return {'l1_loss': zero, 'kld_loss': zero, 'iou_metric': zero, 'weighted_loss': zero}

#         return {
#             'l1_loss': self._last_l1_loss,
#             'kld_loss': self._last_kld_loss,
#             'iou_loss': 1-self._last_mean_iou_metric,
#             'weighted_loss': self._last_weighted_loss
#         }



    # def oriented_ciou(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> tuple:
    #     """
    #     Calcule le Complete IoU (CIoU) orientÃ© entre deux ensembles de boÃ®tes.
    #     CIoU = IoU - dÂ²/cÂ² - Î±*v, oÃ¹ v mesure la consistance du ratio d'aspect.
        
    #     Args:
    #         boxes1: Tensor de forme (N, 5) [cx, cy, w, h, angle]
    #         boxes2: Tensor de forme (N, 5) [cx, cy, w, h, angle]
            
    #     Returns:
    #         tuple (iou, ciou): IoU et CIoU entre les boÃ®tes
    #     """
    #     # Calcule d'abord le IoU standard
    #     iou = self.oriented_iou(boxes1, boxes2)
        
    #     # Extraction des coordonnÃ©es des centres
    #     centers1 = boxes1[:, :2]  # [cx, cy]
    #     centers2 = boxes2[:, :2]  # [cx, cy]
        
    #     # Distance euclidienne au carrÃ© entre les centres
    #     d2 = torch.sum((centers1 - centers2) ** 2, dim=1)
        
    #     # Conversion en polygones pour obtenir les points extrÃªmes
    #     poly1 = self.obb_to_polygon(boxes1)
    #     poly2 = self.obb_to_polygon(boxes2)
        
    #     # Calcul de la plus petite boÃ®te englobant les deux polygones
    #     min_xy = torch.min(torch.min(poly1, dim=1)[0], torch.min(poly2, dim=1)[0])
    #     max_xy = torch.max(torch.max(poly1, dim=1)[0], torch.max(poly2, dim=1)[0])
        
    #     # Diagonale au carrÃ© de la boÃ®te englobante
    #     c2 = torch.sum((max_xy - min_xy) ** 2, dim=1)
        
    #     # Term de distance (DIoU)
    #     distance_term = d2 / (c2 + self.eps)
        
    #     # Calcul du terme de ratio d'aspect
    #     w1, h1, a1 = boxes1[:, 2], boxes1[:, 3], boxes1[:, 4]
    #     w2, h2, a2 = boxes2[:, 2], boxes2[:, 3], boxes2[:, 4]
        
    #     # Pour les boÃ®tes orientÃ©es, nous devons ajuster les largeurs/hauteurs
    #     # en fonction de l'angle pour un calcul prÃ©cis du ratio d'aspect
    #     w1_adjusted = torch.where(torch.abs(torch.sin(a1)) > torch.abs(torch.cos(a1)),
    #                             h1, w1)
    #     h1_adjusted = torch.where(torch.abs(torch.sin(a1)) > torch.abs(torch.cos(a1)),
    #                             w1, h1)
        
    #     w2_adjusted = torch.where(torch.abs(torch.sin(a2)) > torch.abs(torch.cos(a2)),
    #                             h2, w2)
    #     h2_adjusted = torch.where(torch.abs(torch.sin(a2)) > torch.abs(torch.cos(a2)),
    #                             w2, h2)
        
    #     # Calcul de v (terme de ratio d'aspect)
    #     # v = (4 / Ï€Â²) * [arctan(w_gt/h_gt) - arctan(w_pred/h_pred)]Â²
    #     v = (4.0 / (math.pi ** 2)) * torch.pow(
    #         torch.atan(w2_adjusted / (h2_adjusted + self.eps)) - 
    #         torch.atan(w1_adjusted / (h1_adjusted + self.eps)), 2)
        
    #     # Facteur alpha
    #     alpha = v / ((1 - iou) + v + self.eps)
        
    #     # Calcul final du CIoU
    #     ciou = iou - distance_term - alpha * v
        
    #     return iou, torch.clamp(ciou, min=-1.0, max=1.0)
# def number_loss_fn(logits, targets, mask):
#     per_element_loss = F.l1_loss(
#         input=logits[:,:-1].squeeze(-1)*mask[:,1:],
#         target=targets[:,1:]*mask[:,1:], 
#         reduction="none"
#     )
#     mask_sum = torch.sum(mask[:,1:])
#     l1_loss = torch.sum(per_element_loss) / mask_sum
#     tmp_logits = logits[:,:-1].contiguous().view(-1,4)
#     tmp_targtes = targets[:,1:].contiguous().view(-1,4)
#     flatten_mask = mask[:,1:].flatten()
#     iou_loss = torch.mean(generalized_box_iou_loss(tmp_logits[torch.where(flatten_mask)], tmp_targtes[torch.where(flatten_mask)]))
#     if mask_sum > 0:
#         return iou_loss + l1_loss
#     else:
#         return torch.tensor(0.0, device=logits.device)  # Retourne 0 si pas d'Ã©lÃ©ments

#-------------------------------------------------------------------------------------

# class NumberLoss(nn.Module):
#     """
#     Loss combinÃ©e L1 + IoU orientÃ©e optimisÃ©e pour la dÃ©tection de boÃ®tes orientÃ©es.
#     Remplace l'IoU standard par l'IoU orientÃ©e pour une meilleure prÃ©cision.
#     """
    
#     def __init__(self, 
#                  l1_weight: float = 10.0,
#                  iou_weight: float = 2.0,
#                  iou_type: str = 'iou',  # 'iou', 'giou', 'diou'
#                  coord_format: str = 'cxcywha',  # 'xyxy', 'cxcywha'
#                  angle_normalization: str = 'symmetric_pi',
#                  eps: float = 1e-7):
#         super().__init__()
        
#         self.l1_weight = l1_weight
#         self.iou_weight = iou_weight
#         self.coord_format = coord_format
#         self.eps = eps
        
#         # Loss IoU orientÃ©e intÃ©grÃ©e
#         self.oriented_iou_loss = OrientedIoULoss(
#             loss_type=iou_type,
#             reduction='mean',
#             eps=eps
#         )
        
#         # Fonction de normalisation d'angle
#         self.angle_normalization = angle_normalization
    
#     def forward(self, logits: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
#         """
#         Args:
#             logits: (batch_size, seq_len, 4 ou 5) - prÃ©dictions des boÃ®tes
#             targets: (batch_size, seq_len, 4 ou 5) - boÃ®tes cibles  
#             mask: (batch_size, seq_len) - masque de validitÃ©
#         """
#         device = logits.device
        
#         # PrÃ©paration des donnÃ©es (dÃ©calage temporel)
#         pred_boxes = logits[:, :-1]  # (batch_size, seq_len-1, 4/5)
#         target_boxes = targets[:, 1:]  # (batch_size, seq_len-1, 4/5)
#         valid_mask = mask[:, 1:]  # (batch_size, seq_len-1)
        
#         # Normalisation des dimensions du masque
#         if valid_mask.dim() == 3:
#             valid_mask = valid_mask.squeeze(-1)
        
#         # Validation des dimensions
#         self._validate_shapes(pred_boxes, target_boxes, valid_mask)
        
#         # Calcul du nombre d'Ã©lÃ©ments valides
#         mask_sum = torch.sum(valid_mask)
#         if mask_sum == 0:
#             return torch.tensor(0.0, device=device, requires_grad=True)
        
#         # Calcul des losses
#         l1_loss = self._compute_l1_loss(pred_boxes, target_boxes, valid_mask, mask_sum)
#         iou_loss = self._compute_iou_loss(pred_boxes, target_boxes, valid_mask)
        
#         return self.l1_weight * l1_loss + self.iou_weight * iou_loss
    
#     def _validate_shapes(self, pred_boxes: torch.Tensor, target_boxes: torch.Tensor, valid_mask: torch.Tensor):
#         """Validation des dimensions des tenseurs"""
#         assert pred_boxes.shape == target_boxes.shape, \
#             f"Shape mismatch: {pred_boxes.shape} vs {target_boxes.shape}"
#         assert pred_boxes.shape[:-1] == valid_mask.shape, \
#             f"Mask shape mismatch: {pred_boxes.shape[:-1]} vs {valid_mask.shape}"
    
#     def _compute_l1_loss(self, pred_boxes: torch.Tensor, target_boxes: torch.Tensor, 
#                         valid_mask: torch.Tensor, mask_sum: torch.Tensor) -> torch.Tensor:
#         """Calcul optimisÃ© de la loss L1 avec masquage"""
#         # Masquage vectorisÃ©
#         mask_expanded = valid_mask.unsqueeze(-1)  # (batch_size, seq_len-1, 1)
        
#         # Calcul L1 seulement sur les Ã©lÃ©ments valides
#         per_element_loss = F.l1_loss(pred_boxes, target_boxes, reduction="none")  # (B, S, 4/5)
#         masked_loss = per_element_loss * mask_expanded
        
#         # Moyenne sur les Ã©lÃ©ments valides
#         num_coords = pred_boxes.shape[-1]  # 4 ou 5 coordonnÃ©es
#         return torch.sum(masked_loss) / (mask_sum * num_coords)
    
#     def _compute_iou_loss(self, pred_boxes: torch.Tensor, target_boxes: torch.Tensor, 
#                          valid_mask: torch.Tensor) -> torch.Tensor:
#         """Calcul optimisÃ© de la loss IoU orientÃ©e"""
#         device = pred_boxes.device
        
#         # Filtrage efficace des Ã©lÃ©ments valides
#         valid_indices = torch.nonzero(valid_mask.flatten(), as_tuple=False).squeeze(1)
        
#         if len(valid_indices) == 0:
#             return torch.tensor(1.0, device=device, requires_grad=True)
        
#         # Extraction des boÃ®tes valides (vectorisÃ©)
#         flat_pred = pred_boxes.reshape(-1, pred_boxes.shape[-1])
#         flat_target = target_boxes.reshape(-1, target_boxes.shape[-1])
        
#         valid_pred_boxes = flat_pred[valid_indices]
#         valid_target_boxes = flat_target[valid_indices]
        
#         # PrÃ©paration des boÃ®tes selon le format
#         valid_pred_boxes = self._prepare_boxes(valid_pred_boxes)
#         valid_target_boxes = self._prepare_boxes(valid_target_boxes)
        
#         # Calcul de la loss IoU orientÃ©e
#         try:
#             return self.oriented_iou_loss(valid_pred_boxes, valid_target_boxes)
#         except Exception as e:
#             print(f"Warning: Oriented IoU loss computation failed: {e}")
#             return torch.tensor(0.0, device=device, requires_grad=True)
    
#     def _prepare_boxes(self, boxes: torch.Tensor) -> torch.Tensor:
#         """PrÃ©paration et validation des boÃ®tes selon le format"""
#         if self.coord_format == 'xyxy':
#             # Format (xmin, ymin, xmax, ymax) ou (xmin, ymin, xmax, ymax, angle)
#             if boxes.shape[-1] == 4:
#                 # Pas d'angle fourni - on assume angle = 0
#                 angles = torch.zeros(boxes.shape[0], 1, device=boxes.device, dtype=boxes.dtype)
#                 boxes_with_angle = torch.cat([boxes, angles], dim=-1)
#             elif boxes.shape[-1] == 5:
#                 # Format (xmin, ymin, xmax, ymax, angle) - dÃ©jÃ  avec angle
#                 boxes_with_angle = boxes
#             else:
#                 raise ValueError(f"Format xyxy attendu avec 4 ou 5 dimensions, reÃ§u {boxes.shape[-1]}")
                
#             # Conversion vers format cxcywha
#             boxes_cxcywha = self._convert_xyxy_to_cxcywha(boxes_with_angle)
#             return self._ensure_valid_oriented_boxes(boxes_cxcywha)
            
#         elif self.coord_format == 'cxcywha':
#             # Format (cx, cy, w, h) ou (cx, cy, w, h, angle) - dÃ©jÃ  compatible
#             if boxes.shape[-1] == 4:
#                 # Pas d'angle fourni - on assume angle = 0
#                 angles = torch.zeros(boxes.shape[0], 1, device=boxes.device, dtype=boxes.dtype)
#                 boxes_with_angle = torch.cat([boxes, angles], dim=-1)
#             elif boxes.shape[-1] == 5:
#                 # Format (cx, cy, w, h, angle) - dÃ©jÃ  complet
#                 boxes_with_angle = boxes
#             else:
#                 raise ValueError(f"Format cxcywha attendu avec 4 ou 5 dimensions, reÃ§u {boxes.shape[-1]}")
                
#             return self._ensure_valid_oriented_boxes(boxes_with_angle)
#         else:
#             raise ValueError(f"Format {self.coord_format} non supportÃ©")

#     def _convert_xyxy_to_cxcywha(self, boxes: torch.Tensor) -> torch.Tensor:
#         """
#         Convertit les boÃ®tes du format (xmin, ymin, xmax, ymax, angle) 
#         vers le format (cx, cy, w, h, angle)
        
#         Args:
#             boxes: tensor de forme (..., 5) avec (xmin, ymin, xmax, ymax, angle)
            
#         Returns:
#             tensor de forme (..., 5) avec (cx, cy, w, h, angle)
#         """
#         # Extraction des coordonnÃ©es (compatible gradients)
#         xmin = boxes[..., 0:1]
#         ymin = boxes[..., 1:2] 
#         xmax = boxes[..., 2:3]
#         ymax = boxes[..., 3:4]
#         angle = boxes[..., 4:5]
        
#         # Calcul du centre
#         cx = (xmin + xmax) * 0.5
#         cy = (ymin + ymax) * 0.5
        
#         # Calcul des dimensions
#         width = xmax - xmin
#         height = ymax - ymin
        
#         # Assemblage du rÃ©sultat
#         result = torch.cat([cx, cy, width, height, angle], dim=-1)
        
#         return result
    
#     def _convert_xyxy_to_cxcywha(self, boxes: torch.Tensor) -> torch.Tensor:
#         """Conversion (xmin,ymin,xmax,ymax,angle) -> (cx,cy,w,h,angle)"""
#         xmin, ymin, xmax, ymax = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
#         angle = boxes[:, 4] if boxes.shape[-1] == 5 else torch.zeros_like(xmin)
        
#         cx = (xmin + xmax) / 2
#         cy = (ymin + ymax) / 2
#         w = torch.abs(xmax - xmin)
#         h = torch.abs(ymax - ymin)
        
#         return torch.stack([cx, cy, w, h, angle], dim=1)
    
#     def _ensure_valid_oriented_boxes(self, boxes: torch.Tensor) -> torch.Tensor:
#         """Validation et correction des boÃ®tes orientÃ©es - SANS opÃ©rations in-place"""
#         # Clamp des coordonnÃ©es (sauf l'angle) - crÃ©ation nouveau tenseur
#         coords_clamped = torch.clamp(boxes[:, :4], min=0.0)
        
#         # S'assurer que width et height sont positives - crÃ©ation nouveaux tenseurs
#         w_fixed = torch.maximum(coords_clamped[:, 2], torch.full_like(coords_clamped[:, 2], self.eps))
#         h_fixed = torch.maximum(coords_clamped[:, 3], torch.full_like(coords_clamped[:, 3], self.eps))
        
#         # Normalisation de l'angle - crÃ©ation nouveau tenseur
#         if self.angle_normalization == 'symmetric_pi':
#             angles_normalized = self._normalize_angle_symmetric_pi(boxes[:, 4])
#         elif self.angle_normalization == 'positive_pi':
#             angles_normalized = self._normalize_angle_positive_pi(boxes[:, 4])
#         else:
#             angles_normalized = boxes[:, 4]
        
#         # Reconstruction complÃ¨te du tenseur - AUCUNE opÃ©ration in-place
#         boxes_fixed = torch.stack([
#             coords_clamped[:, 0],  # cx
#             coords_clamped[:, 1],  # cy
#             w_fixed,               # w
#             h_fixed,               # h
#             angles_normalized      # angle
#         ], dim=1)
        
#         return boxes_fixed
    
#     def _normalize_angle_symmetric_pi(self, angles: torch.Tensor) -> torch.Tensor:
#         """Normalisation dans [-Ï€/2, Ï€/2] - SANS opÃ©rations in-place"""
#         # Normalisation de base dans [-Ï€, Ï€]
#         angles_mod = angles % (2 * math.pi)
#         angles_wrapped = torch.where(angles_mod > math.pi, angles_mod - 2 * math.pi, angles_mod)
        
#         # Normalisation dans [-Ï€/2, Ï€/2]
#         angles_sym1 = torch.where(angles_wrapped > math.pi / 2, angles_wrapped - math.pi, angles_wrapped)
#         angles_sym2 = torch.where(angles_sym1 < -math.pi / 2, angles_sym1 + math.pi, angles_sym1)
        
#         return angles_sym2
    
#     def _normalize_angle_positive_pi(self, angles: torch.Tensor) -> torch.Tensor:
#         """Normalisation dans [0, Ï€] - SANS opÃ©rations in-place"""
#         return angles % math.pi


# class OrientedIoULoss(nn.Module):
#     """Version simplifiÃ©e de votre loss IoU orientÃ©e pour l'intÃ©gration"""
    
#     def __init__(self, loss_type: str = 'iou', reduction: str = 'mean', eps: float = 1e-7):
#         super().__init__()
#         self.loss_type = loss_type
#         self.reduction = reduction
#         self.eps = eps
    
#     def forward(self, pred_boxes: torch.Tensor, target_boxes: torch.Tensor) -> torch.Tensor:
#         """
#         Args:
#             pred_boxes: (N, 5) [cx, cy, w, h, angle]
#             target_boxes: (N, 5) [cx, cy, w, h, angle]
#         """
#         iou = self.oriented_iou(pred_boxes, target_boxes)
#         loss = 1.0 - iou
        
#         if self.reduction == 'mean':
#             return loss.mean()
#         elif self.reduction == 'sum':
#             return loss.sum()
#         else:
#             return loss
    
#     def oriented_iou(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> torch.Tensor:
#         """Version optimisÃ©e du calcul IoU orientÃ©e"""
#         # Conversion en polygones
#         poly1 = self.obb_to_polygon(boxes1)
#         poly2 = self.obb_to_polygon(boxes2)
        
#         # Aires des boÃ®tes
#         area1 = boxes1[:, 2] * boxes1[:, 3]  # w * h
#         area2 = boxes2[:, 2] * boxes2[:, 3]  # w * h
        
#         # Calcul intersection (version simplifiÃ©e pour l'optimisation)
#         intersection = self.approximate_intersection_area(poly1, poly2, area1, area2)
        
#         # Union
#         union = area1 + area2 - intersection
        
#         # IoU avec stabilitÃ© numÃ©rique
#         iou = intersection / (union + self.eps)
#         return torch.clamp(iou, 0.0, 1.0)
    
#     def obb_to_polygon(self, boxes: torch.Tensor) -> torch.Tensor:
#         """Conversion OBB -> polygone optimisÃ©e - SANS opÃ©rations in-place"""
#         cx, cy, w, h, angle = boxes.unbind(-1)
        
#         # Points locaux
#         w_half, h_half = w / 2, h / 2
        
#         # Calcul trigonomÃ©trique optimisÃ©
#         cos_a, sin_a = torch.cos(angle), torch.sin(angle)
        
#         # Matrice de rotation appliquÃ©e aux 4 coins - crÃ©ation nouveaux tenseurs
#         corners_x = torch.stack([
#             -w_half * cos_a + h_half * sin_a + cx,
#             w_half * cos_a + h_half * sin_a + cx,
#             w_half * cos_a - h_half * sin_a + cx,
#             -w_half * cos_a - h_half * sin_a + cx
#         ], dim=1)
        
#         corners_y = torch.stack([
#             -w_half * sin_a - h_half * cos_a + cy,
#             w_half * sin_a - h_half * cos_a + cy,
#             w_half * sin_a + h_half * cos_a + cy,
#             -w_half * sin_a + h_half * cos_a + cy
#         ], dim=1)
        
#         return torch.stack([corners_x, corners_y], dim=-1)  # (N, 4, 2)
    
#     def approximate_intersection_area(self, poly1: torch.Tensor, poly2: torch.Tensor, 
#                                     area1: torch.Tensor, area2: torch.Tensor) -> torch.Tensor:
#         """
#         Approximation rapide de l'aire d'intersection pour l'optimisation.
#         Utilise une heuristique basÃ©e sur la distance entre centres et l'overlap des AABB.
#         SANS opÃ©rations in-place
#         """
#         # AABB de chaque polygone
#         min1 = torch.min(poly1, dim=1)[0]  # (N, 2)
#         max1 = torch.max(poly1, dim=1)[0]  # (N, 2)
#         min2 = torch.min(poly2, dim=1)[0]  # (N, 2)
#         max2 = torch.max(poly2, dim=1)[0]  # (N, 2)
        
#         # Intersection des AABB - crÃ©ation nouveaux tenseurs
#         inter_min = torch.maximum(min1, min2)
#         inter_max = torch.minimum(max1, max2)
        
#         # Aires d'intersection AABB - crÃ©ation nouveaux tenseurs
#         inter_wh = torch.clamp(inter_max - inter_min, min=0.0)
#         aabb_intersection = inter_wh[:, 0] * inter_wh[:, 1]
        
#         # Heuristique : l'intersection rÃ©elle est proportionnelle Ã  l'intersection AABB
#         # avec un facteur basÃ© sur les aires relatives - crÃ©ation nouveaux tenseurs
#         min_area = torch.minimum(area1, area2)
#         intersection_ratio = torch.clamp(aabb_intersection / (min_area + self.eps), 0.0, 1.0)
        
#         return intersection_ratio * min_area

#--------------------------------------------------------------------------------------

# def number_loss_fn(logits, targets, mask):
#     """
#     Args:
#         logits: (batch_size, seq_len, 4) - prÃ©dictions des boÃ®tes
#         targets: (batch_size, seq_len, 4) - boÃ®tes cibles  
#         mask: (batch_size, seq_len) ou (batch_size, seq_len, 1) - masque de validitÃ©
#     """
#     device = logits.device
    
#     # Supprimer la derniÃ¨re position pour logits
#     pred_boxes = logits[:, :-1]  # (batch_size, seq_len-1, 4)
#     target_boxes = targets[:, 1:]  # (batch_size, seq_len-1, 4)
#     valid_mask = mask[:, 1:]  # (batch_size, seq_len-1) ou (batch_size, seq_len-1, 1)
    
#     # GÃ©rer la dimension supplÃ©mentaire du masque si elle existe
#     if valid_mask.dim() == 3:
#         valid_mask = valid_mask.squeeze(-1)  # (batch_size, seq_len-1)
    
#     # VÃ©rifier que les dimensions sont cohÃ©rentes
#     assert pred_boxes.shape == target_boxes.shape, f"Shape mismatch: {pred_boxes.shape} vs {target_boxes.shape}"
#     assert pred_boxes.shape[:-1] == valid_mask.shape, f"Mask shape mismatch: {pred_boxes.shape[:-1]} vs {valid_mask.shape}"
    
#     # Calculer le nombre total d'Ã©lÃ©ments valides
#     mask_sum = torch.sum(valid_mask)
    
#     if mask_sum == 0:
#         return torch.tensor(0.0, device=device, requires_grad=True)
    
#     # === L1 Loss ===
#     # Appliquer le masque Ã©lÃ©ment par Ã©lÃ©ment
#     masked_pred = pred_boxes * valid_mask.unsqueeze(-1)  # (batch_size, seq_len-1, 4)
#     masked_target = target_boxes * valid_mask.unsqueeze(-1)  # (batch_size, seq_len-1, 4)
    
#     per_element_loss = F.l1_loss(masked_pred, masked_target, reduction="none")  # (batch_size, seq_len-1, 4)
    
#     # Sommer seulement sur les Ã©lÃ©ments valides
#     l1_loss = torch.sum(per_element_loss * valid_mask.unsqueeze(-1)) / (mask_sum * 4)
#     # penalty = coord_penalty(masked_pred[valid_mask.unsqueeze(-1)], max_coord=1.0)
    
#     # === IoU Loss ===
#     # Aplatir et filtrer les Ã©lÃ©ments valides
#     flat_pred = pred_boxes[:,:,:4].reshape(-1, 4)  # (batch_size * (seq_len-1), 4)
#     flat_target = target_boxes[:,:,:4].reshape(-1, 4)  # (batch_size * (seq_len-1), 4)
#     flat_mask = valid_mask.flatten()  # (batch_size * (seq_len-1),)
    
#     # Extraire seulement les boÃ®tes valides
#     valid_indices = torch.nonzero(flat_mask, as_tuple=False).squeeze(1)
    
#     if len(valid_indices) == 0:
#         iou_loss = torch.tensor(0.0, device=device, requires_grad=True)
#     else:
#         valid_pred_boxes = flat_pred[valid_indices]  # (num_valid, 4)
#         valid_target_boxes = flat_target[valid_indices]  # (num_valid, 4)
        
#         # VÃ©rifier que les boÃ®tes sont dans un format valide
#         valid_pred_boxes = ensure_valid_boxes(valid_pred_boxes)
#         valid_target_boxes = ensure_valid_boxes(valid_target_boxes)
        
#         # Calculer la perte IoU
#         try:
#             iou_loss = generalized_box_iou_loss(valid_pred_boxes, valid_target_boxes, reduction='mean')
#         except Exception as e:
#             print(f"Warning: IoU loss computation failed: {e}")
#             iou_loss = torch.tensor(0.0, device=device, requires_grad=True)
    
#     return 10*l1_loss + 2*iou_loss

# def ensure_valid_boxes(boxes):
#     boxes_clamped = torch.clamp(boxes, min=0.0)
    
#     # S'assurer que x2 > x1 et y2 > y1 sans opÃ©rations in-place
#     x1, y1, x2, y2 = boxes_clamped[:, 0], boxes_clamped[:, 1], boxes_clamped[:, 2], boxes_clamped[:, 3]
    
#     x2_fixed = torch.maximum(x2, x1 + 1e-6)
#     y2_fixed = torch.maximum(y2, y1 + 1e-6)
    
#     # Reconstruire le tenseur sans modification in-place
#     boxes_fixed = torch.stack([x1, y1, x2_fixed, y2_fixed], dim=1)
    
#     return boxes_fixed


# class OrientedIoULoss(nn.Module):
#     """
#     Loss IoU orientÃ©e optimisÃ©e pour la dÃ©tection de documents.
#     Calcul exact et diffÃ©rentiable de l'intersection de polygones convexes.
#     """
    
#     def __init__(self, 
#                  loss_type: str = 'iou',  # 'iou', 'giou', 'diou', 'ciou'
#                  reduction: str = 'mean',
#                  eps: float = 1e-7):
#         super().__init__()
#         self.loss_type = loss_type
#         self.reduction = reduction
#         self.eps = eps
    
#     def forward(self, pred_boxes: torch.Tensor, target_boxes: torch.Tensor) -> torch.Tensor:
#         """
#         Args:
#             pred_boxes: (N, 5) [cx, cy, w, h, angle] ou (N, 5) [xmin, ymin, xmax, ymax, angle]
#             target_boxes: (N, 5) [cx, cy, w, h, angle] ou (N, 5) [xmin, ymin, xmax, ymax, angle]
#         """
#         # Conversion vers format (cx, cy, w, h, angle) si nÃ©cessaire
#         pred_boxes = self.ensure_cxcywha_format(pred_boxes)
#         target_boxes = self.ensure_cxcywha_format(target_boxes)
        
#         if self.loss_type == 'iou':
#             iou = self.oriented_iou(pred_boxes, target_boxes)
#             loss = 1.0 - iou
#         elif self.loss_type == 'giou':
#             loss = self.oriented_giou_loss(pred_boxes, target_boxes)
#         elif self.loss_type in ['diou', 'ciou']:
#             loss = self.oriented_distance_iou_loss(pred_boxes, target_boxes)
#         else:
#             raise ValueError(f"Loss type {self.loss_type} not supported")
        
#         return self._reduce_loss(loss)
    
#     def ensure_cxcywha_format(self, boxes: torch.Tensor) -> torch.Tensor:
#         """Convertit (xmin,ymin,xmax,ymax,angle) vers (cx,cy,w,h,angle) si nÃ©cessaire"""
#         # DÃ©tection automatique du format basÃ©e sur les valeurs
#         # Si les 2 premiÃ¨res valeurs sont gÃ©nÃ©ralement plus petites que les 2 suivantes,
#         # on assume le format (xmin,ymin,xmax,ymax,angle)
#         if boxes.shape[-1] != 5:
#             raise ValueError("Les boÃ®tes doivent avoir 5 dimensions")
            
#         # Test simple : si x1 < x2 et y1 < y2 pour la plupart des Ã©chantillons
#         x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
#         is_xyxy_format = ((x1 < x2) & (y1 < y2)).float().mean() > 0.8
        
#         if is_xyxy_format:
#             # Conversion (xmin,ymin,xmax,ymax,angle) -> (cx,cy,w,h,angle)
#             cx = (boxes[:, 0] + boxes[:, 2]) / 2
#             cy = (boxes[:, 1] + boxes[:, 3]) / 2
#             w = torch.abs(boxes[:, 2] - boxes[:, 0])
#             h = torch.abs(boxes[:, 3] - boxes[:, 1])
#             angle = boxes[:, 4]
#             return torch.stack([cx, cy, w, h, angle], dim=1)
#         else:
#             return boxes  # DÃ©jÃ  au bon format
    
#     def oriented_iou(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> torch.Tensor:
#         """Calcul IoU orientÃ©e exact et diffÃ©rentiable"""
#         # Conversion en polygones
#         poly1 = self.obb_to_polygon(boxes1)
#         poly2 = self.obb_to_polygon(boxes2)
        
#         # Aires des boÃ®tes
#         area1 = boxes1[:, 2] * boxes1[:, 3]  # w * h
#         area2 = boxes2[:, 2] * boxes2[:, 3]  # w * h
        
#         # Calcul intersection exact
#         intersection = self.convex_polygon_intersection_area(poly1, poly2)
        
#         # Union
#         union = area1 + area2 - intersection
        
#         # IoU avec stabilitÃ© numÃ©rique
#         iou = intersection / (union + self.eps)
#         return torch.clamp(iou, 0.0, 1.0)
    
#     def obb_to_polygon(self, boxes: torch.Tensor) -> torch.Tensor:
#         """
#         Convertit OBB vers polygone (4 sommets).
#         Args:
#             boxes: (N, 5) [cx, cy, w, h, angle]
#         Returns:
#             polygons: (N, 4, 2) coordonnÃ©es des 4 sommets
#         """
#         cx, cy, w, h, angle = boxes.unbind(-1)
        
#         # Points locaux (rectangle centrÃ© Ã  l'origine)
#         w_half, h_half = w / 2, h / 2
#         corners_local = torch.stack([
#             torch.stack([-w_half, -h_half], dim=-1),  # bottom-left
#             torch.stack([w_half, -h_half], dim=-1),   # bottom-right  
#             torch.stack([w_half, h_half], dim=-1),    # top-right
#             torch.stack([-w_half, h_half], dim=-1)    # top-left
#         ], dim=1)  # (N, 4, 2)
        
#         # Rotation
#         cos_a, sin_a = torch.cos(angle), torch.sin(angle)
#         rotation_matrix = torch.stack([
#             torch.stack([cos_a, -sin_a], dim=-1),
#             torch.stack([sin_a, cos_a], dim=-1)
#         ], dim=1)  # (N, 2, 2)
        
#         # Application rotation + translation
#         corners_rotated = torch.matmul(corners_local, rotation_matrix.transpose(-2, -1))
#         center = torch.stack([cx, cy], dim=-1).unsqueeze(1)  # (N, 1, 2)
#         corners_global = corners_rotated + center
        
#         return corners_global
    
#     def convex_polygon_intersection_area(self, poly1: torch.Tensor, poly2: torch.Tensor) -> torch.Tensor:
#         """
#         Calcul exact de l'aire d'intersection entre polygones convexes.
#         Utilise l'algorithme de Sutherland-Hodgman modifiÃ© pour Ãªtre diffÃ©rentiable.
#         """
#         batch_size = poly1.shape[0]
#         intersection_areas = torch.zeros(batch_size, device=poly1.device, dtype=poly1.dtype)
        
#         for i in range(batch_size):
#             # Intersection par clipping successif
#             subject_polygon = poly1[i]  # (4, 2)
#             clip_polygon = poly2[i]     # (4, 2)
            
#             # Algorithme de Sutherland-Hodgman
#             output_list = subject_polygon
            
#             for j in range(4):  # Pour chaque arÃªte du polygone de clipping
#                 if output_list.shape[0] < 3:
#                     break
                    
#                 # ArÃªte courante du polygone de clipping
#                 clip_vertex1 = clip_polygon[j]
#                 clip_vertex2 = clip_polygon[(j + 1) % 4]
                
#                 input_list = output_list
#                 if input_list.shape[0] == 0:
#                     break
                    
#                 output_list = torch.zeros((0, 2), device=poly1.device, dtype=poly1.dtype)
                
#                 if input_list.shape[0] > 0:
#                     s = input_list[-1]  # Dernier point
                    
#                 for k in range(input_list.shape[0]):
#                     e = input_list[k]  # Point courant
                    
#                     if self._inside_edge(e, clip_vertex1, clip_vertex2):
#                         if not self._inside_edge(s, clip_vertex1, clip_vertex2):
#                             # Intersection
#                             intersection_point = self._line_intersection(
#                                 s, e, clip_vertex1, clip_vertex2
#                             )
#                             if intersection_point is not None:
#                                 output_list = torch.cat([output_list, intersection_point.unsqueeze(0)])
#                         output_list = torch.cat([output_list, e.unsqueeze(0)])
#                     elif self._inside_edge(s, clip_vertex1, clip_vertex2):
#                         # Intersection
#                         intersection_point = self._line_intersection(
#                             s, e, clip_vertex1, clip_vertex2
#                         )
#                         if intersection_point is not None:
#                             output_list = torch.cat([output_list, intersection_point.unsqueeze(0)])
                    
#                     s = e
            
#             # Calcul de l'aire du polygone rÃ©sultant
#             if output_list.shape[0] >= 3:
#                 intersection_areas[i] = self._polygon_area(output_list)
        
#         return torch.clamp(intersection_areas, 0.0)
    
#     def _inside_edge(self, point: torch.Tensor, edge_start: torch.Tensor, edge_end: torch.Tensor) -> bool:
#         """Teste si un point est Ã  l'intÃ©rieur d'une arÃªte (Ã  gauche)"""
#         return ((edge_end[0] - edge_start[0]) * (point[1] - edge_start[1]) - 
#                 (edge_end[1] - edge_start[1]) * (point[0] - edge_start[0])) >= 0
    
#     def _line_intersection(self, p1: torch.Tensor, p2: torch.Tensor, 
#                           p3: torch.Tensor, p4: torch.Tensor) -> Optional[torch.Tensor]:
#         """Calcule l'intersection entre deux segments [p1,p2] et [p3,p4]"""
#         x1, y1 = p1[0], p1[1]
#         x2, y2 = p2[0], p2[1]
#         x3, y3 = p3[0], p3[1]
#         x4, y4 = p4[0], p4[1]
        
#         denom = (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)
        
#         if torch.abs(denom) < self.eps:
#             return None  # Lignes parallÃ¨les
        
#         t = ((x1 - x3) * (y3 - y4) - (y1 - y3) * (x3 - x4)) / denom
        
#         intersection_x = x1 + t * (x2 - x1)
#         intersection_y = y1 + t * (y2 - y1)
        
#         return torch.stack([intersection_x, intersection_y])
    
#     def _polygon_area(self, vertices: torch.Tensor) -> torch.Tensor:
#         """Calcule l'aire d'un polygone avec la formule du lacet (shoelace)"""
#         if vertices.shape[0] < 3:
#             return torch.tensor(0.0, device=vertices.device, dtype=vertices.dtype)
        
#         x = vertices[:, 0]
#         y = vertices[:, 1]
        
#         # Formule du lacet
#         area = 0.5 * torch.abs(
#             torch.sum(x[:-1] * y[1:] - x[1:] * y[:-1]) + 
#             x[-1] * y[0] - x[0] * y[-1]
#         )
        
#         return area
    
#     def oriented_giou_loss(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> torch.Tensor:
#         """GIoU orientÃ©e pour une meilleure convergence"""
#         iou = self.oriented_iou(boxes1, boxes2)
        
#         # Calcul de la plus petite boÃ®te englobante
#         poly1 = self.obb_to_polygon(boxes1)
#         poly2 = self.obb_to_polygon(boxes2)
        
#         # Union des deux polygones pour calculer l'aire de la boÃ®te englobante
#         all_points = torch.cat([poly1, poly2], dim=1)  # (N, 8, 2)
        
#         # BoÃ®te englobante axis-aligned
#         min_coords = torch.min(all_points, dim=1)[0]  # (N, 2)
#         max_coords = torch.max(all_points, dim=1)[0]  # (N, 2)
        
#         enclosing_area = (max_coords[:, 0] - min_coords[:, 0]) * (max_coords[:, 1] - min_coords[:, 1])
        
#         # Aires des boÃ®tes originales
#         area1 = boxes1[:, 2] * boxes1[:, 3]
#         area2 = boxes2[:, 2] * boxes2[:, 3]
#         union = area1 + area2 - iou * area1  # approximation
        
#         giou = iou - (enclosing_area - union) / (enclosing_area + self.eps)
#         return 1.0 - giou
    
#     def oriented_distance_iou_loss(self, boxes1: torch.Tensor, boxes2: torch.Tensor) -> torch.Tensor:
#         """DIoU orientÃ©e - prend en compte la distance entre centres"""
#         iou = self.oriented_iou(boxes1, boxes2)
        
#         # Distance entre centres
#         center_distance = torch.sum((boxes1[:, :2] - boxes2[:, :2]) ** 2, dim=1)
        
#         # Diagonale de la boÃ®te englobante
#         poly1 = self.obb_to_polygon(boxes1)
#         poly2 = self.obb_to_polygon(boxes2)
#         all_points = torch.cat([poly1, poly2], dim=1)
        
#         min_coords = torch.min(all_points, dim=1)[0]
#         max_coords = torch.max(all_points, dim=1)[0]
#         diagonal_squared = torch.sum((max_coords - min_coords) ** 2, dim=1)
        
#         diou = iou - center_distance / (diagonal_squared + self.eps)
#         return 1.0 - diou
    
#     def _reduce_loss(self, loss: torch.Tensor) -> torch.Tensor:
#         """Applique la rÃ©duction spÃ©cifiÃ©e"""
#         if self.reduction == 'mean':
#             return loss.mean()
#         elif self.reduction == 'sum':
#             return loss.sum()
#         else:
#             return loss

# # Fonctions utilitaires pour la conversion de formats
# def convert_xyxy_to_cxcywha(boxes_xyxy: torch.Tensor, angles: torch.Tensor) -> torch.Tensor:
#     """Convertit (xmin,ymin,xmax,ymax) + angles vers (cx,cy,w,h,angle)"""
#     cx = (boxes_xyxy[:, 0] + boxes_xyxy[:, 2]) / 2
#     cy = (boxes_xyxy[:, 1] + boxes_xyxy[:, 3]) / 2
#     w = boxes_xyxy[:, 2] - boxes_xyxy[:, 0]
#     h = boxes_xyxy[:, 3] - boxes_xyxy[:, 1]
#     return torch.stack([cx, cy, w, h, angles], dim=1)

    
__all__ = ["LeL2_ForConditionalGeneration", "Qwen2_5_VLModel", "Qwen2_5_VLPreTrainedModel", "Qwen2_5_VLTextModel"]
